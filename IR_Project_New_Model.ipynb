{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "decb7cf229d941dfb3cdf148a4606f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3354d50dc574eb98904b7ff130ffc25",
              "IPY_MODEL_a0b9f5cfb01b4f29ab01593b3b1e1f94",
              "IPY_MODEL_f073ee0c0e35467e934206e0ce06589d"
            ],
            "layout": "IPY_MODEL_5f28aa791513468c80d2fe6968556ae7"
          }
        },
        "e3354d50dc574eb98904b7ff130ffc25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c8fbc2e3e0a4d69a4be268c960095f2",
            "placeholder": "​",
            "style": "IPY_MODEL_9ede480ca62b445781d7dc16e18bcbe6",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a0b9f5cfb01b4f29ab01593b3b1e1f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d24ed8c554a4ec2bc10096e9a8866ce",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d7238c9b8324020bdb4361c64ee938e",
            "value": 52
          }
        },
        "f073ee0c0e35467e934206e0ce06589d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71f56020557e4175aec61ce9746d08c7",
            "placeholder": "​",
            "style": "IPY_MODEL_664cdec99e6f429ca99065ee66ac43e9",
            "value": " 52.0/52.0 [00:00&lt;00:00, 6.06kB/s]"
          }
        },
        "5f28aa791513468c80d2fe6968556ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8fbc2e3e0a4d69a4be268c960095f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ede480ca62b445781d7dc16e18bcbe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d24ed8c554a4ec2bc10096e9a8866ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d7238c9b8324020bdb4361c64ee938e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71f56020557e4175aec61ce9746d08c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "664cdec99e6f429ca99065ee66ac43e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ff5fbdb150c497f81c4c4e5386ad1e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3875c7b1ff5147b88a4edcf30a8660e8",
              "IPY_MODEL_05db1b49c4ed4d65ae1ecbe20a470d5b",
              "IPY_MODEL_817371b2b8e64ff1a1f76bc7677c965b"
            ],
            "layout": "IPY_MODEL_89d6c60eef1946d7a90366b685897e3f"
          }
        },
        "3875c7b1ff5147b88a4edcf30a8660e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5c332351d9548d9a6f7343501ea4616",
            "placeholder": "​",
            "style": "IPY_MODEL_574a2953f8874b11bf93f5fa88a603aa",
            "value": "config.json: 100%"
          }
        },
        "05db1b49c4ed4d65ae1ecbe20a470d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fb1f0e374a440cb95f0ac6ae5bc26c3",
            "max": 578,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb0c778afa1040fda7b89a34e438b343",
            "value": 578
          }
        },
        "817371b2b8e64ff1a1f76bc7677c965b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abbc1c15bccc440eb9dffa3b15f06b19",
            "placeholder": "​",
            "style": "IPY_MODEL_afa4f87bff5b4b6e89c3e97da074c2f8",
            "value": " 578/578 [00:00&lt;00:00, 62.5kB/s]"
          }
        },
        "89d6c60eef1946d7a90366b685897e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c332351d9548d9a6f7343501ea4616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "574a2953f8874b11bf93f5fa88a603aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fb1f0e374a440cb95f0ac6ae5bc26c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb0c778afa1040fda7b89a34e438b343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "abbc1c15bccc440eb9dffa3b15f06b19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa4f87bff5b4b6e89c3e97da074c2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecf24f6e5c56407b86bd90d19e003073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_646d700493644248b064159e5cee4ec8",
              "IPY_MODEL_4b92aa4053d44d3192ad757497ba55b2",
              "IPY_MODEL_56d5b72fec3a479398a9f7a8ef481ef9"
            ],
            "layout": "IPY_MODEL_bba199644dd64737a831db4d304927b3"
          }
        },
        "646d700493644248b064159e5cee4ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_134d0d14f94c46ec88505c6d72050dcd",
            "placeholder": "​",
            "style": "IPY_MODEL_315b736e4d6044e78fb33f5ad2268683",
            "value": "spm.model: 100%"
          }
        },
        "4b92aa4053d44d3192ad757497ba55b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eb7353b698e4721ba7904151ea456b2",
            "max": 2464616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33a017f49f154cecbe419ee72117ac42",
            "value": 2464616
          }
        },
        "56d5b72fec3a479398a9f7a8ef481ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a95fa2a5b3a415dbfb7abf7e7eb8af4",
            "placeholder": "​",
            "style": "IPY_MODEL_53a42726015341cdb6e7528697ce3f7f",
            "value": " 2.46M/2.46M [00:01&lt;00:00, 80.3kB/s]"
          }
        },
        "bba199644dd64737a831db4d304927b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "134d0d14f94c46ec88505c6d72050dcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315b736e4d6044e78fb33f5ad2268683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2eb7353b698e4721ba7904151ea456b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33a017f49f154cecbe419ee72117ac42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a95fa2a5b3a415dbfb7abf7e7eb8af4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53a42726015341cdb6e7528697ce3f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a4666b5e9e640c299bdc55fe1706dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6b41984ebd24f568139d6b8b307ebc9",
              "IPY_MODEL_69b6a1f0fe5b4b96b806d3d02d35285c",
              "IPY_MODEL_c68dfbc8b40a4741939a913f3b0f5b94"
            ],
            "layout": "IPY_MODEL_25632a649f094fabb57a5c8f49ac5b7a"
          }
        },
        "f6b41984ebd24f568139d6b8b307ebc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca8bb40fe9ac437d95bfa7f803d12e48",
            "placeholder": "​",
            "style": "IPY_MODEL_1ce7a8fb4ed442ed85b2ce999457fcb0",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "69b6a1f0fe5b4b96b806d3d02d35285c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1bdbe43b780441294f3530052b79783",
            "max": 286059269,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_144575f2f88147f3a9e21d338c842a4f",
            "value": 286059269
          }
        },
        "c68dfbc8b40a4741939a913f3b0f5b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f8b678b2b2840cb9ce5b0cfb4502138",
            "placeholder": "​",
            "style": "IPY_MODEL_518ce6b967ad46f59f4ca4a6cbb194b4",
            "value": " 286M/286M [00:06&lt;00:00, 36.1MB/s]"
          }
        },
        "25632a649f094fabb57a5c8f49ac5b7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca8bb40fe9ac437d95bfa7f803d12e48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ce7a8fb4ed442ed85b2ce999457fcb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1bdbe43b780441294f3530052b79783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "144575f2f88147f3a9e21d338c842a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f8b678b2b2840cb9ce5b0cfb4502138": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "518ce6b967ad46f59f4ca4a6cbb194b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84ad92aa839d4b61bf40727f86e34092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36d8849a74b141db988d97c2d0539c58",
              "IPY_MODEL_cc39c59c0afc4160ae176dbfdd5e5ee2",
              "IPY_MODEL_1f57da5db2bd4e108e5669f245000a0e"
            ],
            "layout": "IPY_MODEL_84b566fa0738463cb242b718034a1db9"
          }
        },
        "36d8849a74b141db988d97c2d0539c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e4f4101a3534033a8ab23bc833824b4",
            "placeholder": "​",
            "style": "IPY_MODEL_8c15a4cda2d4460db606dbfc41b529ac",
            "value": "model.safetensors: 100%"
          }
        },
        "cc39c59c0afc4160ae176dbfdd5e5ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b1762f72fb44cc1aeb20ce509a02a99",
            "max": 286034994,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02cbfe2f4add4e0f918c4ddc9a1b343e",
            "value": 286034994
          }
        },
        "1f57da5db2bd4e108e5669f245000a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f4732884e684e0e8d4e4f851b50abf5",
            "placeholder": "​",
            "style": "IPY_MODEL_fe106a1c0adf4093b390b472c4072817",
            "value": " 286M/286M [00:02&lt;00:00, 217MB/s]"
          }
        },
        "84b566fa0738463cb242b718034a1db9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e4f4101a3534033a8ab23bc833824b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c15a4cda2d4460db606dbfc41b529ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b1762f72fb44cc1aeb20ce509a02a99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02cbfe2f4add4e0f918c4ddc9a1b343e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f4732884e684e0e8d4e4f851b50abf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe106a1c0adf4093b390b472c4072817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Laddu's doing**"
      ],
      "metadata": {
        "id": "bpTkmI6XsJXK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDAIp-C6p59k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b5ac42-8918-44bc-ad8e-e71af2f4c866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 515 qdoc entries and 9596 queries\n",
            "Train: 8637 Dev: 959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:  28%|██▊       | 201/720 [00:30<01:20,  6.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 step 200 avg_loss -2.1241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:  56%|█████▌    | 401/720 [00:59<00:46,  6.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 step 400 avg_loss -4.7169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:  83%|████████▎ | 601/720 [01:28<00:15,  7.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 step 600 avg_loss -5.7139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|██████████| 720/720 [01:45<00:00,  6.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 metrics: {'mSICR': 0, 'mWISE': 0, 'MDCR_soft': 0, 'MDCR_strict': 0}\n",
            "Saved best model: iade_best_model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3:  28%|██▊       | 201/720 [00:28<01:15,  6.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 step 200 avg_loss -7.5195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3:  56%|█████▌    | 401/720 [00:57<00:49,  6.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 step 400 avg_loss -7.4396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3:  83%|████████▎ | 601/720 [01:26<00:17,  6.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 step 600 avg_loss -7.3020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 720/720 [01:42<00:00,  7.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 metrics: {'mSICR': 0, 'mWISE': 0, 'MDCR_soft': 0, 'MDCR_strict': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3:  28%|██▊       | 201/720 [00:28<01:21,  6.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 step 200 avg_loss -7.5716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3:  56%|█████▌    | 400/720 [00:57<00:42,  7.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 step 400 avg_loss -7.1250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3:  83%|████████▎ | 601/720 [01:25<00:17,  6.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 step 600 avg_loss -7.4320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 720/720 [01:42<00:00,  7.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 metrics: {'mSICR': 0, 'mWISE': 0, 'MDCR_soft': 0, 'MDCR_strict': 0}\n",
            "Final metrics: {'mSICR': 0, 'mWISE': 0, 'MDCR_soft': 0, 'MDCR_strict': 0}\n",
            "{\n",
            "  \"model\": \"IADE_finetuned\",\n",
            "  \"mSICR\": 0,\n",
            "  \"mWISE\": 0,\n",
            "  \"MDCR_soft\": 0,\n",
            "  \"MDCR_strict\": 0\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# IADE final stable — train + eval (Colab-ready)\n",
        "# Uncomment and run once if required:\n",
        "# !pip install -q sentence-transformers faiss-cpu tqdm scikit-learn\n",
        "\n",
        "import os, json, random, math, sys\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.optim import AdamW\n",
        "from sentence_transformers import util\n",
        "\n",
        "# -------------------------- Config --------------------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "BACKBONE = \"sentence-transformers/all-MiniLM-L6-v2\"  # change if you have larger GPU\n",
        "PROJ_DIM = 384\n",
        "BATCH_SIZE = 12\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "TEMPERATURE = 0.05\n",
        "MAX_LENGTH = 256\n",
        "MAX_HARD_NEGS = 2\n",
        "DEV_FRAC = 0.1\n",
        "SAVE_PATH = \"iade_best_model.pt\"\n",
        "PRINT_EVERY = 200\n",
        "\n",
        "DATA_PATH = \"query-doc.json\"\n",
        "QUERIES_PATH = \"final_sorted.jsonl\"\n",
        "\n",
        "# -------------------------- Data helpers ---------------------------\n",
        "def load_qdoc(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    examples = []\n",
        "    for e in data:\n",
        "        qid = e.get(\"query_id\")\n",
        "        docs = e.get(\"documents\", [])\n",
        "        positives = [d for d in docs if d.get(\"type\", \"\").lower() == \"positive\"]\n",
        "        hard_negs = [d for d in docs if d.get(\"type\", \"\").lower() == \"hard_negative\"]\n",
        "        other_negs = [d for d in docs if d.get(\"type\", \"\").lower() not in (\"positive\", \"hard_negative\")]\n",
        "        if len(positives) == 0:\n",
        "            continue\n",
        "        examples.append({\"query_id\": qid, \"positives\": positives, \"hard_negs\": hard_negs, \"other_negs\": other_negs})\n",
        "    return examples\n",
        "\n",
        "def load_queries(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "def build_qdoc_map(qdoc_list):\n",
        "    d = {}\n",
        "    for e in qdoc_list:\n",
        "        qid = e.get(\"query_id\")\n",
        "        positives = [{\"text\": doc.get(\"text\",\"\"), \"doc_id\": doc.get(\"doc_id\")} for doc in e.get(\"documents\", []) if doc.get(\"type\",\"\").lower()==\"positive\"]\n",
        "        hard_negs = [{\"text\": doc.get(\"text\",\"\"), \"doc_id\": doc.get(\"doc_id\")} for doc in e.get(\"documents\", []) if doc.get(\"type\",\"\").lower()==\"hard_negative\"]\n",
        "        other_negs = [{\"text\": doc.get(\"text\",\"\"), \"doc_id\": doc.get(\"doc_id\")} for doc in e.get(\"documents\", []) if doc.get(\"type\",\"\").lower() not in (\"positive\",\"hard_negative\")]\n",
        "        d[qid] = {\"positives\": positives, \"hard_negs\": hard_negs, \"other_negs\": other_negs}\n",
        "    return d\n",
        "\n",
        "# -------------------------- Dataset --------------------------------\n",
        "class IADataset(Dataset):\n",
        "    def __init__(self, queries_records: List[Dict], qdoc_map: Dict, max_hard_negs: int = 2):\n",
        "        self.queries = queries_records\n",
        "        self.qdoc = qdoc_map\n",
        "        self.max_hard_negs = max_hard_negs\n",
        "\n",
        "    def __len__(self): return len(self.queries)\n",
        "\n",
        "    def _extract_positive_text(self, r, qid):\n",
        "        pos_field = r.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\",\"\")\n",
        "        elif isinstance(pos_field, list) and len(pos_field)>0:\n",
        "            first = pos_field[0]\n",
        "            if isinstance(first, dict):\n",
        "                pos_text = first.get(\"text\",\"\")\n",
        "            else:\n",
        "                pos_text = str(first)\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field\n",
        "        pos_text = pos_text.strip()\n",
        "        if not pos_text:\n",
        "            candidates = self.qdoc.get(qid, {}).get(\"positives\", [])\n",
        "            if candidates:\n",
        "                pos_text = candidates[0].get(\"text\",\"\").strip()\n",
        "        return pos_text\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.queries[idx]\n",
        "        qid = r[\"query_id\"]\n",
        "        q_text = r.get(\"query\",\"\")\n",
        "        q_ins = r.get(\"instructed_query\", q_text)\n",
        "        q_rev = r.get(\"reversed_query\", q_text + \" not\")\n",
        "        pos_text = self._extract_positive_text(r, qid)\n",
        "\n",
        "        # hard negatives:\n",
        "        hns = []\n",
        "        for d in self.qdoc.get(qid, {}).get(\"hard_negs\", []):\n",
        "            if isinstance(d, dict) and d.get(\"text\"):\n",
        "                hns.append(d[\"text\"])\n",
        "            elif isinstance(d, str) and d.strip():\n",
        "                hns.append(d.strip())\n",
        "        hns = hns[:self.max_hard_negs]\n",
        "\n",
        "        return {\n",
        "            \"query_id\": qid,\n",
        "            \"query\": q_text,\n",
        "            \"instructed_query\": q_ins,\n",
        "            \"reversed_query\": q_rev,\n",
        "            \"positive\": pos_text,\n",
        "            \"hard_negs\": hns,\n",
        "            \"attributes\": r.get(\"attributes\", {})\n",
        "        }\n",
        "\n",
        "# -------------------------- Model ----------------------------------\n",
        "class IADE(nn.Module):\n",
        "    def __init__(self, backbone_name=BACKBONE, proj_dim=PROJ_DIM, device=DEVICE):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.backbone = AutoModel.from_pretrained(backbone_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(backbone_name)\n",
        "        hidden = self.backbone.config.hidden_size\n",
        "        self.proj = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, proj_dim))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # expose output dim for safe empty-tensor creation:\n",
        "        self.out_dim = proj_dim\n",
        "\n",
        "    def forward_encode(self, texts: List[str], max_length=MAX_LENGTH):\n",
        "        # return tensor on self.device, shape (N, out_dim)\n",
        "        if not texts:\n",
        "            return torch.zeros((0, self.out_dim), device=self.device)\n",
        "        tok = self.tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "        tok = {k: v.to(self.device) for k,v in tok.items()}\n",
        "        out = self.backbone(**tok, return_dict=True)\n",
        "        mask = tok[\"attention_mask\"].unsqueeze(-1).float()\n",
        "        mean_pooled = (out.last_hidden_state * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
        "        proj = self.proj(self.dropout(mean_pooled))\n",
        "        return F.normalize(proj, dim=-1)\n",
        "\n",
        "    def encode(self, texts: List[str], batch_size=32):\n",
        "        # batched; returns tensor on device\n",
        "        if not texts:\n",
        "            return torch.zeros((0, self.out_dim), device=self.device)\n",
        "        parts = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            if not batch:\n",
        "                continue\n",
        "            with torch.no_grad():\n",
        "                emb = self.forward_encode(batch)\n",
        "            parts.append(emb)\n",
        "        return torch.cat(parts, dim=0).to(self.device)\n",
        "\n",
        "# -------------------------- Loss + Collate -------------------------\n",
        "def info_nce_loss(q_emb, p_emb, temperature=TEMPERATURE):\n",
        "    sims = torch.matmul(q_emb, p_emb.t()) / temperature\n",
        "    labels = torch.arange(q_emb.size(0), device=sims.device, dtype=torch.long)\n",
        "    return F.cross_entropy(sims, labels)\n",
        "\n",
        "def collate_train(samples: List[Dict]):\n",
        "    B = len(samples)\n",
        "    queries_orig = [s[\"query\"] for s in samples]\n",
        "    queries_ins = [s[\"instructed_query\"] for s in samples]\n",
        "    queries_rev = [s[\"reversed_query\"] for s in samples]\n",
        "    positives = [s[\"positive\"] if s[\"positive\"] else \" \" for s in samples]\n",
        "    hard_negs = [hn for s in samples for hn in s[\"hard_negs\"]]\n",
        "    passages = positives + hard_negs\n",
        "    return {\"B\": B, \"queries_orig\": queries_orig, \"queries_ins\": queries_ins, \"queries_rev\": queries_rev, \"passages\": passages}\n",
        "\n",
        "# -------------------------- Metrics (same formulas) ------------------------------\n",
        "def find_rank_in_ranking(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "def compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev):\n",
        "    return int((Rins_rank < Rori_rank and Sins > Sori) and (Rrev_rank > Rori_rank and Srev < Sori))\n",
        "\n",
        "def compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, sat_count, vio_count, K=10):\n",
        "    delta_ins = Rori_rank - Rins_rank\n",
        "    delta_rev = Rrev_rank - Rori_rank\n",
        "    reward = (sat_count / m) * (1 - math.sqrt(max(delta_ins, 0) / K)) * (1.0 / math.sqrt(max(Rins_rank, 1)))\n",
        "    penalty = - (vio_count / m)\n",
        "    return reward + penalty\n",
        "\n",
        "def compute_MDCR(attrs: Dict, pos_doc_text: str, model_for_mdcr: IADE):\n",
        "    if not attrs:\n",
        "        return 0.0, 0\n",
        "    pos_emb = model_for_mdcr.encode([pos_doc_text])  # on device\n",
        "    sim_scores = []\n",
        "    for attr_name, attr_value in attrs.items():\n",
        "        attr_desc = f\"The document should reflect {attr_name} = {attr_value}.\"\n",
        "        attr_emb = model_for_mdcr.encode([attr_desc])\n",
        "        sim = float(util.cos_sim(attr_emb, pos_emb)[0,0])\n",
        "        sim_scores.append(sim)\n",
        "    mdcr_soft = float(np.mean(sim_scores))\n",
        "    threshold = max(0.45, mdcr_soft - 0.05)\n",
        "    mdcr_strict = int(all(s >= threshold for s in sim_scores))\n",
        "    return mdcr_soft, mdcr_strict\n",
        "\n",
        "# -------------------------- Evaluation (robust) -------------------------------\n",
        "def evaluate_model_on_queries(model: IADE, queries_list: List[Dict], qdoc_map: Dict, top_k=10, debug_sample_n=3):\n",
        "    model.eval()\n",
        "\n",
        "    # build corpus (unique) and mapping\n",
        "    corpus_texts = []\n",
        "    doc_ids = []\n",
        "    qid_to_text2docid = {}\n",
        "    for qid, groups in qdoc_map.items():\n",
        "        qid_to_text2docid[qid] = {}\n",
        "        for doc in groups[\"positives\"] + groups[\"hard_negs\"] + groups[\"other_negs\"]:\n",
        "            text = doc.get(\"text\",\"\").strip()\n",
        "            if not text: continue\n",
        "            key = \" \".join(text.lower().split())\n",
        "            if key in qid_to_text2docid[qid]:\n",
        "                continue\n",
        "            docid = f\"{qid}_{doc.get('doc_id', len(doc_ids))}\"\n",
        "            qid_to_text2docid[qid][key] = docid\n",
        "            doc_ids.append(docid)\n",
        "            corpus_texts.append(key)\n",
        "\n",
        "    if not corpus_texts:\n",
        "        return {\"mSICR\":0,\"mWISE\":0,\"MDCR_soft\":0,\"MDCR_strict\":0}\n",
        "\n",
        "    corpus_embeddings = model.encode(corpus_texts, batch_size=64)  # on device\n",
        "    results = []\n",
        "    skipped = 0\n",
        "\n",
        "    # sample debug queries to display retrievals\n",
        "    debug_samples = random.sample(queries_list, min(debug_sample_n, len(queries_list)))\n",
        "\n",
        "    for q in tqdm(queries_list, desc=\"Evaluating\"):\n",
        "        try:\n",
        "            qid = q[\"query_id\"]\n",
        "            pos_field = q.get(\"positive_doc\", \"\")\n",
        "            if isinstance(pos_field, dict):\n",
        "                pos_text_raw = pos_field.get(\"text\",\"\")\n",
        "            elif isinstance(pos_field, list) and len(pos_field)>0:\n",
        "                first = pos_field[0]\n",
        "                pos_text_raw = first.get(\"text\",\"\") if isinstance(first, dict) else str(first)\n",
        "            else:\n",
        "                pos_text_raw = str(pos_field) if pos_field is not None else \"\"\n",
        "\n",
        "            pos_text_key = \" \".join(str(pos_text_raw).strip().lower().split())\n",
        "            if not pos_text_key:\n",
        "                # try qdoc map first positive\n",
        "                cand = qdoc_map.get(qid, {}).get(\"positives\", [])\n",
        "                if cand:\n",
        "                    pos_text_key = \" \".join(cand[0].get(\"text\",\"\").strip().lower().split())\n",
        "\n",
        "            if not pos_text_key:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            # find doc id: exact normalized match -> substring match -> semantic fallback\n",
        "            pos_doc = qid_to_text2docid.get(qid, {}).get(pos_text_key)\n",
        "            if not pos_doc:\n",
        "                # substring match\n",
        "                for k, did in qid_to_text2docid.get(qid, {}).items():\n",
        "                    if pos_text_key in k or k in pos_text_key:\n",
        "                        pos_doc = did\n",
        "                        break\n",
        "\n",
        "            if not pos_doc:\n",
        "                # semantic fallback: find best corpus doc overall\n",
        "                pos_emb = model.encode([pos_text_key])  # device\n",
        "                sims = util.cos_sim(pos_emb, corpus_embeddings)[0]\n",
        "                best_idx = int(torch.argmax(sims))\n",
        "                pos_doc = doc_ids[best_idx]\n",
        "\n",
        "            # compute embeddings for queries\n",
        "            qori_emb = model.encode([q.get(\"query\",\"\")])\n",
        "            qins_emb = model.encode([q.get(\"instructed_query\", q.get(\"query\",\"\"))])\n",
        "            qrev_emb = model.encode([q.get(\"reversed_query\", q.get(\"query\",\"\"))])\n",
        "\n",
        "            sims_ori = util.cos_sim(qori_emb, corpus_embeddings)[0]\n",
        "            sims_ins = util.cos_sim(qins_emb, corpus_embeddings)[0]\n",
        "            sims_rev = util.cos_sim(qrev_emb, corpus_embeddings)[0]\n",
        "\n",
        "            topk_ori = torch.topk(sims_ori, k=min(top_k, sims_ori.shape[0]))\n",
        "            topk_ins = torch.topk(sims_ins, k=min(top_k, sims_ins.shape[0]))\n",
        "            topk_rev = torch.topk(sims_rev, k=min(top_k, sims_rev.shape[0]))\n",
        "\n",
        "            Rori = [(doc_ids[i], float(sims_ori[i])) for i in topk_ori.indices]\n",
        "            Rins = [(doc_ids[i], float(sims_ins[i])) for i in topk_ins.indices]\n",
        "            Rrev = [(doc_ids[i], float(sims_rev[i])) for i in topk_rev.indices]\n",
        "\n",
        "            Rori_rank = find_rank_in_ranking(pos_doc, Rori)\n",
        "            Rins_rank = find_rank_in_ranking(pos_doc, Rins)\n",
        "            Rrev_rank = find_rank_in_ranking(pos_doc, Rrev)\n",
        "\n",
        "            pos_emb = model.encode([pos_text_key])\n",
        "            Sori = float(util.cos_sim(qori_emb, pos_emb)[0,0])\n",
        "            Sins = float(util.cos_sim(qins_emb, pos_emb)[0,0])\n",
        "            Srev = float(util.cos_sim(qrev_emb, pos_emb)[0,0])\n",
        "\n",
        "            attrs = q.get(\"attributes\", {})\n",
        "            m = len(attrs) if attrs else 1\n",
        "            mdcr_soft, mdcr_strict = compute_MDCR(attrs, pos_text_key, model)\n",
        "            sat_count = int(round(mdcr_soft * m))\n",
        "            vio_count = max(0, m - sat_count)\n",
        "\n",
        "            msicr = compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev)\n",
        "            mwise = compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, sat_count, vio_count)\n",
        "\n",
        "            results.append({\"mSICR\": msicr, \"mWISE\": mwise, \"MDCR_soft\": mdcr_soft, \"MDCR_strict\": mdcr_strict})\n",
        "\n",
        "        except Exception as e:\n",
        "            skipped += 1\n",
        "            # continue quietly but note\n",
        "            # print(\"Eval skip\", q.get(\"query_id\"), e)\n",
        "            continue\n",
        "\n",
        "    if not results:\n",
        "        return {\"mSICR\": 0, \"mWISE\": 0, \"MDCR_soft\": 0, \"MDCR_strict\": 0}\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    aggregated = {\"mSICR\": float(df[\"mSICR\"].mean()), \"mWISE\": float(df[\"mWISE\"].mean()),\n",
        "                  \"MDCR_soft\": float(df[\"MDCR_soft\"].mean()), \"MDCR_strict\": float(df[\"MDCR_strict\"].mean())}\n",
        "    if skipped:\n",
        "        print(f\"Evaluation: skipped {skipped} queries due to missing/invalid positives.\")\n",
        "    # show a few debug retrievals\n",
        "    print(\"Sample retrievals (debug):\")\n",
        "    for s in random.sample(queries_list, min(3, len(queries_list))):\n",
        "        qtxt = s.get(\"query\",\"\")[:120]\n",
        "        q_emb = model.encode([s.get(\"query\",\"\")])\n",
        "        sims = util.cos_sim(q_emb, corpus_embeddings)[0]\n",
        "        top = torch.topk(sims, k=3)\n",
        "        print(\" Q:\", qtxt)\n",
        "        for idx in top.indices.tolist():\n",
        "            print(\"   \", corpus_texts[idx][:140])\n",
        "    return aggregated\n",
        "\n",
        "# -------------------------- Training + run -----------------------------------------\n",
        "def run_training_and_eval():\n",
        "    qdoc = load_qdoc(DATA_PATH)\n",
        "    queries = load_queries(QUERIES_PATH)\n",
        "    print(\"Loaded:\", len(qdoc), \"qdoc entries and\", len(queries), \"queries\")\n",
        "    qdoc_map = build_qdoc_map(qdoc)\n",
        "    random.shuffle(queries)\n",
        "    n_dev = max(1, int(len(queries) * DEV_FRAC))\n",
        "    dev_qs, train_qs = queries[:n_dev], queries[n_dev:]\n",
        "    print(\"Train:\", len(train_qs), \"Dev:\", len(dev_qs))\n",
        "\n",
        "    train_ds = IADataset(train_qs, qdoc_map, MAX_HARD_NEGS)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: x)\n",
        "\n",
        "    model = IADE().to(DEVICE)\n",
        "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    best_msicr = -1.0\n",
        "    for ep in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {ep+1}/{EPOCHS}\")):\n",
        "            coll = collate_train(batch)\n",
        "            q_ori = model.forward_encode(coll[\"queries_orig\"])\n",
        "            q_ins = model.forward_encode(coll[\"queries_ins\"])\n",
        "            q_rev = model.forward_encode(coll[\"queries_rev\"])\n",
        "            p_emb = model.forward_encode(coll[\"passages\"])\n",
        "\n",
        "            # Ensure p_emb has at least B rows (if no hns, still positives >= B)\n",
        "            if p_emb.size(0) < q_ori.size(0):\n",
        "                # pad with tiny noise vectors to avoid shape mismatch (rare)\n",
        "                pad = torch.randn((q_ori.size(0)-p_emb.size(0), p_emb.size(1)), device=p_emb.device) * 1e-6\n",
        "                p_emb = torch.cat([p_emb, pad], dim=0)\n",
        "\n",
        "            loss = info_nce_loss(q_ori, p_emb) + info_nce_loss(q_ins, p_emb) - 0.3 * info_nce_loss(q_rev, p_emb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += float(loss.item())\n",
        "            if (step + 1) % PRINT_EVERY == 0:\n",
        "                print(f\"Epoch {ep+1} step {step+1} avg_loss {running_loss/(step+1):.4f}\")\n",
        "\n",
        "        metrics = evaluate_model_on_queries(model, dev_qs, qdoc_map)\n",
        "        print(f\"Epoch {ep+1} metrics:\", metrics)\n",
        "        if metrics[\"mSICR\"] > best_msicr:\n",
        "            best_msicr = metrics[\"mSICR\"]\n",
        "            torch.save(model.state_dict(), SAVE_PATH)\n",
        "            print(\"Saved best model:\", SAVE_PATH)\n",
        "\n",
        "    # final eval\n",
        "    model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))\n",
        "    final_metrics = evaluate_model_on_queries(model, queries, qdoc_map)\n",
        "    print(\"Final metrics:\", final_metrics)\n",
        "    return model, final_metrics\n",
        "\n",
        "# -------------------------- Run -----------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    model, metrics = run_training_and_eval()\n",
        "    print(json.dumps({\"model\": \"IADE_finetuned\", **metrics}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kZNBg7JitU6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RekONsrUtU3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-kZPpvhtU0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWkj7S6MtUyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Saod0phGtUv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Van doing"
      ],
      "metadata": {
        "id": "v1z6i0RltVhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers accelerate peft sentencepiece\n",
        "\n",
        "import os, json, random, math, time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from contextlib import nullcontext\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "\n",
        "try:\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    PEFT_AVAILABLE = True\n",
        "except Exception:\n",
        "    PEFT_AVAILABLE = False\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "SEED = 1337\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "FINAL_JSONL = \"final_sorted.jsonl\"\n",
        "QDOC_JSON   = \"query-doc.json\"\n",
        "\n",
        "BASE_MODEL   = \"microsoft/deberta-v3-small\"\n",
        "MAX_LEN      = 256\n",
        "USE_LORA     = True and PEFT_AVAILABLE\n",
        "LR_MAIN      = 3e-4 if USE_LORA else 2e-5\n",
        "BATCH_SIZE   = 12 if torch.cuda.is_available() else 4\n",
        "GRAD_ACCUM   = 1\n",
        "EPOCHS       = 2\n",
        "WARMUP_RATIO = 0.06\n",
        "\n",
        "MARGIN       = 0.2\n",
        "W_REVERSE    = 1.8\n",
        "LAMBDA_ATTR  = 0.35\n",
        "\n",
        "MAX_TRAIN_PAIRS = 12000\n",
        "MAX_VAL_PAIRS   = 2000\n",
        "\n",
        "OUTPUT_DIR   = \"reranker-fast-fix\"\n",
        "CKPT_EVERY   = 800\n",
        "RESUME       = True\n",
        "ATTR_KEYS    = [\"audience\",\"format\",\"language\",\"length\",\"source\",\"keyword\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------- IO helpers ----------------\n",
        "def load_jsonl(path):\n",
        "    rows=[]\n",
        "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f,1):\n",
        "            line=line.strip()\n",
        "            if not line: continue\n",
        "            try:\n",
        "                rows.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Skipping malformed line {i} in {path}\")\n",
        "    return rows\n",
        "\n",
        "with open(QDOC_JSON,\"r\",encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "entries = load_jsonl(FINAL_JSONL)\n",
        "print(f\"Loaded entries: {len(entries)} | query-doc groups: {len(qdoc)}\")\n",
        "\n",
        "qid_to_docs = {e[\"query_id\"]: [d.get(\"text\",\"\") for d in e.get(\"documents\",[])] for e in qdoc}\n",
        "\n",
        "# ---------------- Normalization ----------------\n",
        "def get_doc_text(d):\n",
        "    if isinstance(d, dict): return str(d.get(\"text\",\"\")).strip()\n",
        "    if isinstance(d, list) and len(d)>0:\n",
        "        first=d[0]\n",
        "        return (first.get(\"text\",\"\").strip() if isinstance(first,dict) else str(first).strip())\n",
        "    if isinstance(d, str): return d.strip()\n",
        "    return \"\"\n",
        "\n",
        "def weak_attr_vec(text:str, attrs:Dict[str,Any]) -> Dict[str,int]:\n",
        "    lt = (text or \"\").lower()\n",
        "    return {k: int(str(v).lower() in lt) for k,v in (attrs or {}).items()}\n",
        "\n",
        "def build_pairs(records:List[Dict[str,Any]]):\n",
        "    pairs=[]\n",
        "    for e in records:\n",
        "        orig = e.get(\"query\",\"\"); ins = e.get(\"instructed_query\",\"\"); rev = e.get(\"reversed_query\",\"\")\n",
        "        pos = get_doc_text(e.get(\"positive_doc\")); neg = get_doc_text(e.get(\"hard_negative_doc\"))\n",
        "        attrs = e.get(\"attributes\",{}) or {}\n",
        "        if not (orig and ins and rev and pos and neg): continue\n",
        "        pos_attr = weak_attr_vec(pos, attrs); neg_attr = weak_attr_vec(neg, attrs)\n",
        "        pairs.append(dict(query=ins,  pos=pos, neg=neg, mode=\"instructed\", pos_attr=pos_attr))\n",
        "        pairs.append(dict(query=orig, pos=pos, neg=neg, mode=\"original\",   pos_attr=pos_attr))\n",
        "        pairs.append(dict(query=rev,  pos=neg, neg=pos, mode=\"reverse\",    pos_attr=neg_attr))\n",
        "    random.shuffle(pairs)\n",
        "    return pairs\n",
        "\n",
        "pairs = build_pairs(entries)\n",
        "print(\"Total trainable pairs (raw):\", len(pairs))\n",
        "\n",
        "if MAX_TRAIN_PAIRS:\n",
        "    pairs = pairs[:MAX_TRAIN_PAIRS + MAX_VAL_PAIRS]\n",
        "split = int(0.85 * len(pairs))\n",
        "train_pairs, val_pairs = pairs[:split], pairs[split:]\n",
        "if MAX_VAL_PAIRS: val_pairs = val_pairs[:MAX_VAL_PAIRS]\n",
        "print(f\"Using: train={len(train_pairs)} | val={len(val_pairs)}\")\n",
        "\n",
        "# ---------------- Dataset & Collator ----------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "\n",
        "@dataclass\n",
        "class Item:\n",
        "    query:str; pos:str; neg:str; mode:str; pos_attr:Dict[str,int]\n",
        "\n",
        "class PairwiseDataset(Dataset):\n",
        "    def __init__(self, items, max_len=256):\n",
        "        self.items=[Item(**x) for x in items]; self.max_len=max_len\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, i): return self.items[i]\n",
        "\n",
        "def pack(query, doc, max_len=256):\n",
        "    text = f\"[QUERY]\\n{query}\\n[DOCUMENT]\\n{doc}\"\n",
        "    return tokenizer(text, max_length=max_len, truncation=True, padding=False, return_tensors=\"pt\")\n",
        "\n",
        "class Collator:\n",
        "    def __init__(self, max_len=256): self.max_len=max_len\n",
        "    def __call__(self, batch:List[Item]):\n",
        "        qpos = [pack(b.query, b.pos, self.max_len) for b in batch]\n",
        "        qneg = [pack(b.query, b.neg, self.max_len) for b in batch]\n",
        "        def pad(key, seqs):\n",
        "            return torch.nn.utils.rnn.pad_sequence(\n",
        "                [s[key].squeeze(0) for s in seqs],\n",
        "                batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "            )\n",
        "        pos = {\"input_ids\": pad(\"input_ids\", qpos), \"attention_mask\": pad(\"attention_mask\", qpos)}\n",
        "        neg = {\"input_ids\": pad(\"input_ids\", qneg), \"attention_mask\": pad(\"attention_mask\", qneg)}\n",
        "        modes=[b.mode for b in batch]\n",
        "        attr_t = torch.zeros(len(batch), len(ATTR_KEYS), dtype=torch.float32)\n",
        "        attr_m = torch.zeros_like(attr_t)\n",
        "        for i,b in enumerate(batch):\n",
        "            for j,k in enumerate(ATTR_KEYS):\n",
        "                if k in b.pos_attr:\n",
        "                    attr_m[i,j]=1.0\n",
        "                    attr_t[i,j]=float(b.pos_attr[k])\n",
        "        return pos, neg, modes, attr_t, attr_m\n",
        "\n",
        "train_ds = PairwiseDataset(train_pairs, MAX_LEN)\n",
        "val_ds   = PairwiseDataset(val_pairs,   MAX_LEN)\n",
        "collate  = Collator(MAX_LEN)\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "class CrossEncoderWithAttr(nn.Module):\n",
        "    def __init__(self, base=BASE_MODEL, dropout=0.1, grad_ckpt=True):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(base)\n",
        "        if grad_ckpt and hasattr(self.encoder, \"gradient_checkpointing_enable\"):\n",
        "            self.encoder.gradient_checkpointing_enable()\n",
        "        # VERY IMPORTANT: allow gradient flow from inputs for checkpointing\n",
        "        if hasattr(self.encoder, \"enable_input_require_grads\"):\n",
        "            self.encoder.enable_input_require_grads()\n",
        "        self.config  = self.encoder.config\n",
        "        h = self.encoder.config.hidden_size\n",
        "        self.dp = nn.Dropout(dropout)\n",
        "        self.rank = nn.Linear(h,1)\n",
        "        self.attr = nn.Linear(h,len(ATTR_KEYS))\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
        "        if \"labels\" in kwargs: kwargs.pop(\"labels\")\n",
        "        out = self.encoder(\n",
        "            input_ids=input_ids, attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids, **kwargs\n",
        "        )\n",
        "        cls = self.dp(out.last_hidden_state[:,0,:])\n",
        "        return self.rank(cls).squeeze(-1), self.attr(cls)\n",
        "\n",
        "model = CrossEncoderWithAttr(BASE_MODEL, grad_ckpt=True)\n",
        "\n",
        "# Attach LoRA; ensure inputs require grads on wrapper too\n",
        "if USE_LORA:\n",
        "    try:\n",
        "        lconf = LoraConfig(\n",
        "            task_type=TaskType.FEATURE_EXTRACTION,\n",
        "            r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "            target_modules=[\"query_proj\",\"key_proj\",\"value_proj\",\"dense\",\"out_proj\"]\n",
        "        )\n",
        "        model = get_peft_model(model, lconf)\n",
        "        # For PEFT, also enable input grads on the wrapper (important with ckpt)\n",
        "        if hasattr(model, \"enable_input_require_grads\"):\n",
        "            model.enable_input_require_grads()\n",
        "        print(\"LoRA attached (FEATURE_EXTRACTION).\")\n",
        "    except Exception as e:\n",
        "        print(f\"LoRA attach failed; continuing without LoRA: {e}\")\n",
        "        USE_LORA=False\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# ---- Freeze everything except LoRA adapters + heads ----\n",
        "def mark_trainable_modules(m):\n",
        "    trainable, total = 0, 0\n",
        "    for n,p in m.named_parameters():\n",
        "        total += p.numel()\n",
        "        # Heads must be trainable; LoRA adapters have \"lora_\" in their param names\n",
        "        if (\"rank\" in n) or (\"attr\" in n) or (\"lora_\" in n):\n",
        "            p.requires_grad_(True); trainable += p.numel()\n",
        "        else:\n",
        "            p.requires_grad_(False)\n",
        "    return trainable, total\n",
        "\n",
        "trainable, total = mark_trainable_modules(model)\n",
        "print(f\"Trainable params: {trainable/1e6:.2f}M / {total/1e6:.2f}M\")\n",
        "\n",
        "# ---------------- Loss, Optim, Sched ----------------\n",
        "def bce_with_mask(logits, targets, mask):\n",
        "    bce = nn.BCEWithLogitsLoss(reduction=\"none\")(logits, targets)\n",
        "    denom = torch.clamp(mask.sum(), min=1.0)\n",
        "    return (bce * mask).sum() / denom\n",
        "\n",
        "def batch_loss(batch):\n",
        "    pos,neg,modes,attr_t,attr_m = batch\n",
        "    pos = {k:v.to(device) for k,v in pos.items()}\n",
        "    neg = {k:v.to(device) for k,v in neg.items()}\n",
        "    attr_t = attr_t.to(device); attr_m = attr_m.to(device)\n",
        "    s_pos, a_pos = model(**pos)\n",
        "    s_neg, _     = model(**neg)\n",
        "    diff = s_pos - s_neg\n",
        "    hinge = torch.clamp(MARGIN - diff, min=0.0)\n",
        "    w = torch.tensor([W_REVERSE if m==\"reverse\" else 1.0 for m in modes], device=device)\n",
        "    rank_loss = (hinge * w).mean()\n",
        "    attr_loss = bce_with_mask(a_pos, attr_t, attr_m)\n",
        "    loss = rank_loss + LAMBDA_ATTR * attr_loss\n",
        "    return loss, {\"rank_loss\": rank_loss.item(), \"attr_loss\": attr_loss.item()}\n",
        "\n",
        "# Optim only over trainable params\n",
        "opt = AdamW([p for p in model.parameters() if p.requires_grad], lr=LR_MAIN, weight_decay=0.01)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate,\n",
        "    pin_memory=True, num_workers=2, persistent_workers=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate,\n",
        "    pin_memory=True, num_workers=1, persistent_workers=True\n",
        ")\n",
        "\n",
        "total_steps = max(1, (len(train_loader) * EPOCHS) // GRAD_ACCUM)\n",
        "warm_steps  = int(total_steps * WARMUP_RATIO)\n",
        "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=warm_steps, num_training_steps=total_steps)\n",
        "scaler = torch.amp.GradScaler(device=\"cuda\") if torch.cuda.is_available() else None\n",
        "\n",
        "# ---------------- Checkpoint utils ----------------\n",
        "def save_ckpt(step):\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f\"model_step{step}.bin\"))\n",
        "    torch.save({\"step\": step, \"opt\": opt.state_dict(), \"sched\": sched.state_dict()},\n",
        "               os.path.join(OUTPUT_DIR, f\"state_step{step}.pt\"))\n",
        "\n",
        "def latest_ckpt():\n",
        "    if not os.path.isdir(OUTPUT_DIR): return None\n",
        "    cands = [f for f in os.listdir(OUTPUT_DIR) if f.startswith(\"state_step\")]\n",
        "    if not cands: return None\n",
        "    return max(int(f.split(\"state_step\")[1].split(\".\")[0]) for f in cands)\n",
        "\n",
        "def load_ckpt(step):\n",
        "    mpath = os.path.join(OUTPUT_DIR, f\"model_step{step}.bin\")\n",
        "    spath = os.path.join(OUTPUT_DIR, f\"state_step{step}.pt\")\n",
        "    if os.path.exists(mpath):\n",
        "        sd = torch.load(mpath, map_location=\"cpu\")\n",
        "        model.load_state_dict(sd, strict=False)\n",
        "        model.to(device)\n",
        "    if os.path.exists(spath):\n",
        "        s = torch.load(spath, map_location=\"cpu\")\n",
        "        opt.load_state_dict(s[\"opt\"])\n",
        "        sched.load_state_dict(s[\"sched\"])\n",
        "    print(f\"Resumed from step {step}\")\n",
        "\n",
        "# ---------------- Eval ----------------\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    losses=[]\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            ctx = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16) if torch.cuda.is_available() else nullcontext()\n",
        "            if torch.cuda.is_available():\n",
        "                with ctx:\n",
        "                    loss, _ = batch_loss(batch)\n",
        "            else:\n",
        "                loss,_ = batch_loss(batch)\n",
        "            losses.append(loss.item())\n",
        "    return float(np.mean(losses)) if losses else 0.0\n",
        "\n",
        "# ---------------- Train (resume-safe) ----------------\n",
        "best_val = float(\"inf\")\n",
        "global_step = 0\n",
        "if RESUME:\n",
        "    ck = latest_ckpt()\n",
        "    if ck is not None:\n",
        "        load_ckpt(ck)\n",
        "        global_step = ck\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    epoch_rank, epoch_attr, nb = 0.0, 0.0, 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
        "    for batch in pbar:\n",
        "        ctx = torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16) if torch.cuda.is_available() else nullcontext()\n",
        "        if torch.cuda.is_available():\n",
        "            with ctx:\n",
        "                loss, logs = batch_loss(batch)\n",
        "            (scaler.scale(loss/GRAD_ACCUM) if scaler else (loss/GRAD_ACCUM)).backward()\n",
        "        else:\n",
        "            loss, logs = batch_loss(batch)\n",
        "            (loss/GRAD_ACCUM).backward()\n",
        "\n",
        "        epoch_rank += logs[\"rank_loss\"]; epoch_attr += logs[\"attr_loss\"]; nb += 1\n",
        "\n",
        "        if (nb % GRAD_ACCUM) == 0:\n",
        "            if scaler: scaler.step(opt); scaler.update()\n",
        "            else: opt.step()\n",
        "            sched.step()\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            global_step += 1\n",
        "            pbar.set_postfix(loss=float(loss.item()), rank=epoch_rank/nb, attr=epoch_attr/nb)\n",
        "\n",
        "            if global_step % CKPT_EVERY == 0:\n",
        "                save_ckpt(global_step)\n",
        "\n",
        "    val_loss = evaluate()\n",
        "    print(f\"\\n>> Val loss: {val_loss:.4f}\")\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"pytorch_model.bin\"))\n",
        "        from transformers import AutoTokenizer as _AT\n",
        "        _AT.from_pretrained(BASE_MODEL, use_fast=True).save_pretrained(OUTPUT_DIR)\n",
        "        print(f\"Saved BEST to {OUTPUT_DIR} (val={best_val:.4f})\")\n",
        "\n",
        "# ---------------- Inference helper ----------------\n",
        "@torch.no_grad()\n",
        "def rerank_scores(query_text: str, docs: List[str], batch_size=32, max_len=256):\n",
        "    model.eval()\n",
        "    scores=[]\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        chunk = docs[i:i+batch_size]\n",
        "        toks = tokenizer(\n",
        "            [f\"[QUERY]\\n{query_text}\\n[DOCUMENT]\\n{d}\" for d in chunk],\n",
        "            max_length=max_len, truncation=True, padding=True, return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "        s,_ = model(**toks)\n",
        "        scores.extend(s.detach().float().cpu().tolist())\n",
        "    order = np.argsort(-np.array(scores))\n",
        "    return [(docs[i], float(scores[i])) for i in order]\n",
        "\n",
        "# ---------------- Smoke test ----------------\n",
        "if len(entries) > 0:\n",
        "    sample = entries[0]\n",
        "    q = sample.get(\"instructed_query\") or sample.get(\"query\") or \"\"\n",
        "    cand = []\n",
        "    cand.append(get_doc_text(sample.get(\"positive_doc\")))\n",
        "    cand.append(get_doc_text(sample.get(\"hard_negative_doc\")))\n",
        "    pool = qid_to_docs.get(sample.get(\"query_id\"), [])[:6]\n",
        "    for d in pool:\n",
        "        if d and d not in cand: cand.append(d)\n",
        "    cand = [d for d in cand if d]\n",
        "    ranked = rerank_scores(q, cand, batch_size=16, max_len=MAX_LEN)[:5]\n",
        "    print(\"\\nTop-5 (smoke test):\")\n",
        "    for i,(d,s) in enumerate(ranked,1):\n",
        "        print(f\"{i:>2}. score={s:.3f} | {d[:90].replace('\\\\n',' ')}...\")\n",
        "else:\n",
        "    print(\"No entries to test.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700,
          "referenced_widgets": [
            "decb7cf229d941dfb3cdf148a4606f05",
            "e3354d50dc574eb98904b7ff130ffc25",
            "a0b9f5cfb01b4f29ab01593b3b1e1f94",
            "f073ee0c0e35467e934206e0ce06589d",
            "5f28aa791513468c80d2fe6968556ae7",
            "0c8fbc2e3e0a4d69a4be268c960095f2",
            "9ede480ca62b445781d7dc16e18bcbe6",
            "3d24ed8c554a4ec2bc10096e9a8866ce",
            "3d7238c9b8324020bdb4361c64ee938e",
            "71f56020557e4175aec61ce9746d08c7",
            "664cdec99e6f429ca99065ee66ac43e9",
            "7ff5fbdb150c497f81c4c4e5386ad1e7",
            "3875c7b1ff5147b88a4edcf30a8660e8",
            "05db1b49c4ed4d65ae1ecbe20a470d5b",
            "817371b2b8e64ff1a1f76bc7677c965b",
            "89d6c60eef1946d7a90366b685897e3f",
            "a5c332351d9548d9a6f7343501ea4616",
            "574a2953f8874b11bf93f5fa88a603aa",
            "1fb1f0e374a440cb95f0ac6ae5bc26c3",
            "bb0c778afa1040fda7b89a34e438b343",
            "abbc1c15bccc440eb9dffa3b15f06b19",
            "afa4f87bff5b4b6e89c3e97da074c2f8",
            "ecf24f6e5c56407b86bd90d19e003073",
            "646d700493644248b064159e5cee4ec8",
            "4b92aa4053d44d3192ad757497ba55b2",
            "56d5b72fec3a479398a9f7a8ef481ef9",
            "bba199644dd64737a831db4d304927b3",
            "134d0d14f94c46ec88505c6d72050dcd",
            "315b736e4d6044e78fb33f5ad2268683",
            "2eb7353b698e4721ba7904151ea456b2",
            "33a017f49f154cecbe419ee72117ac42",
            "6a95fa2a5b3a415dbfb7abf7e7eb8af4",
            "53a42726015341cdb6e7528697ce3f7f",
            "0a4666b5e9e640c299bdc55fe1706dd2",
            "f6b41984ebd24f568139d6b8b307ebc9",
            "69b6a1f0fe5b4b96b806d3d02d35285c",
            "c68dfbc8b40a4741939a913f3b0f5b94",
            "25632a649f094fabb57a5c8f49ac5b7a",
            "ca8bb40fe9ac437d95bfa7f803d12e48",
            "1ce7a8fb4ed442ed85b2ce999457fcb0",
            "e1bdbe43b780441294f3530052b79783",
            "144575f2f88147f3a9e21d338c842a4f",
            "1f8b678b2b2840cb9ce5b0cfb4502138",
            "518ce6b967ad46f59f4ca4a6cbb194b4",
            "84ad92aa839d4b61bf40727f86e34092",
            "36d8849a74b141db988d97c2d0539c58",
            "cc39c59c0afc4160ae176dbfdd5e5ee2",
            "1f57da5db2bd4e108e5669f245000a0e",
            "84b566fa0738463cb242b718034a1db9",
            "7e4f4101a3534033a8ab23bc833824b4",
            "8c15a4cda2d4460db606dbfc41b529ac",
            "1b1762f72fb44cc1aeb20ce509a02a99",
            "02cbfe2f4add4e0f918c4ddc9a1b343e",
            "3f4732884e684e0e8d4e4f851b50abf5",
            "fe106a1c0adf4093b390b472c4072817"
          ]
        },
        "id": "28GzS4W6TZ23",
        "outputId": "d6daf167-66b0-499c-d888-061f7a775980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded entries: 9596 | query-doc groups: 515\n",
            "Total trainable pairs (raw): 28632\n",
            "Using: train=11900 | val=2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "decb7cf229d941dfb3cdf148a4606f05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ff5fbdb150c497f81c4c4e5386ad1e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecf24f6e5c56407b86bd90d19e003073"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a4666b5e9e640c299bdc55fe1706dd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA attached (FEATURE_EXTRACTION).\n",
            "Trainable params: 1.33M / 142.64M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/2:   0%|          | 0/992 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84ad92aa839d4b61bf40727f86e34092"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 992/992 [17:06<00:00,  1.03s/it, attr=0.351, loss=0.113, rank=0.144]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">> Val loss: 0.1220\n",
            "Saved BEST to reranker-fast-fix (val=0.1220)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 992/992 [17:09<00:00,  1.04s/it, attr=0.318, loss=0.0899, rank=0.0112]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">> Val loss: 0.1113\n",
            "Saved BEST to reranker-fast-fix (val=0.1113)\n",
            "\n",
            "Top-5 (smoke test):\n",
            " 1. score=2.329 | A good coding standard should encompass several key components to ensure code quality and ...\n",
            " 2. score=2.162 | A comprehensive coding standard is essential for maintaining high-quality code. It should ...\n",
            " 3. score=2.000 | A comprehensive coding standard should encompass several key elements to ensure that the c...\n",
            " 4. score=0.172 | When creating a coding standard, it's important to consider various aspects. It should lis...\n",
            " 5. score=0.044 | A coding standard is a set of guidelines that can include various aspects of coding. It mi...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Evaluate trained reranker on your dataset pools (mSICR/mWISE/MDCR)\n",
        "# Uses: reranker-fast-fix/, /mnt/data/final_sorted.jsonl, /mnt/data/query-doc.json\n",
        "# ================================================================\n",
        "!pip -q install transformers peft sentencepiece\n",
        "\n",
        "import json, numpy as np, torch, torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "from contextlib import nullcontext\n",
        "\n",
        "MODEL_DIR     = \"reranker-fast-fix\"\n",
        "BASE_MODEL    = \"microsoft/deberta-v3-small\"\n",
        "FINAL_JSONL   = \"final_sorted.jsonl\"\n",
        "QDOC_JSON     = \"query-doc.json\"\n",
        "ATTR_KEYS     = [\"audience\",\"format\",\"language\",\"length\",\"source\",\"keyword\"]\n",
        "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MAX_LEN       = 256\n",
        "BATCH_SIZE    = 32\n",
        "MAX_EVAL      = 1000    # set to None for full set\n",
        "\n",
        "# ---------- IO ----------\n",
        "def load_jsonl(path):\n",
        "    rows=[]\n",
        "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
        "        for i,l in enumerate(f,1):\n",
        "            l=l.strip()\n",
        "            if not l: continue\n",
        "            try: rows.append(json.loads(l))\n",
        "            except: pass\n",
        "    return rows\n",
        "\n",
        "with open(QDOC_JSON,\"r\",encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "entries = load_jsonl(FINAL_JSONL)\n",
        "\n",
        "# ---------- normalize ----------\n",
        "def get_doc_text(d):\n",
        "    if isinstance(d, dict): return str(d.get(\"text\",\"\")).strip()\n",
        "    if isinstance(d, list) and len(d)>0:\n",
        "        first=d[0]\n",
        "        return (first.get(\"text\",\"\").strip() if isinstance(first,dict) else str(first).strip())\n",
        "    if isinstance(d, str): return d.strip()\n",
        "    return \"\"\n",
        "\n",
        "# Build pools: qid -> list of (doc_id, text)\n",
        "qid_pools = {}\n",
        "for group in qdoc:\n",
        "    qid = group[\"query_id\"]\n",
        "    pool=[]\n",
        "    for d in group.get(\"documents\", []):\n",
        "        pool.append((d.get(\"doc_id\", f\"{qid}_unk\"), d.get(\"text\",\"\").strip()))\n",
        "    qid_pools[qid] = pool\n",
        "\n",
        "# ---------- model (same head as training) ----------\n",
        "class CrossEncoderWithAttr(nn.Module):\n",
        "    def __init__(self, base=BASE_MODEL):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(base)\n",
        "        self.config  = self.encoder.config\n",
        "        h = self.encoder.config.hidden_size\n",
        "        self.dp = nn.Dropout(0.1)\n",
        "        self.rank = nn.Linear(h,1)\n",
        "        self.attr = nn.Linear(h, len(ATTR_KEYS))\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
        "        if \"labels\" in kwargs: kwargs.pop(\"labels\", None)\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n",
        "        cls = self.dp(out.last_hidden_state[:,0,:])\n",
        "        return self.rank(cls).squeeze(-1), self.attr(cls)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
        "model = CrossEncoderWithAttr(BASE_MODEL)\n",
        "state = torch.load(f\"{MODEL_DIR}/pytorch_model.bin\", map_location=\"cpu\")\n",
        "model.load_state_dict(state, strict=False)\n",
        "model.to(DEVICE); model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_batch(query, docs):\n",
        "    toks = tokenizer(\n",
        "        [f\"[QUERY]\\n{query}\\n[DOCUMENT]\\n{d}\" for d in docs],\n",
        "        max_length=MAX_LEN, truncation=True, padding=True, return_tensors=\"pt\"\n",
        "    ).to(DEVICE)\n",
        "    s,_ = model(**toks)\n",
        "    return s.detach().float().cpu().numpy()\n",
        "\n",
        "def rank_within_pool(query, pool_texts):\n",
        "    scores=[]\n",
        "    for i in range(0, len(pool_texts), BATCH_SIZE):\n",
        "        chunk = pool_texts[i:i+BATCH_SIZE]\n",
        "        scores.extend(score_batch(query, chunk))\n",
        "    order = np.argsort(-np.array(scores))\n",
        "    return order, np.array(scores)\n",
        "\n",
        "def find_pos_index_in_pool(pos_text, pool_texts):\n",
        "    # exact match first\n",
        "    try:\n",
        "        return pool_texts.index(pos_text)\n",
        "    except ValueError:\n",
        "        # fallback: highest token-overlap\n",
        "        def tokset(t): return set(t.lower().split())\n",
        "        pt = tokset(pos_text)\n",
        "        best_i, best_j = -1, -1.0\n",
        "        for i, d in enumerate(pool_texts):\n",
        "            s = len(pt & tokset(d)) / (len(pt | tokset(d)) + 1e-9)\n",
        "            if s > best_j:\n",
        "                best_j, best_i = s, i\n",
        "        return best_i if best_i >= 0 else None\n",
        "\n",
        "# ---------- evaluation ----------\n",
        "def evaluate(entries, limit=None):\n",
        "    msicr, mwise, mdcr_s, mdcr_soft = [], [], [], []\n",
        "    n = len(entries) if (limit is None) else min(limit, len(entries))\n",
        "    for e in tqdm(entries[:n], desc=\"Evaluating reranker\"):\n",
        "        qid = e.get(\"query_id\")\n",
        "        pool = qid_pools.get(qid, [])\n",
        "        if not pool: continue\n",
        "        doc_ids, doc_texts = zip(*pool)\n",
        "        doc_texts = list(doc_texts)\n",
        "\n",
        "        pos_text = get_doc_text(e.get(\"positive_doc\"))\n",
        "        if not pos_text: continue\n",
        "        pos_idx  = find_pos_index_in_pool(pos_text, doc_texts)\n",
        "        if pos_idx is None: continue\n",
        "        pos_docid = doc_ids[pos_idx]\n",
        "\n",
        "        q_ori = e.get(\"query\",\"\")\n",
        "        q_ins = e.get(\"instructed_query\",\"\")\n",
        "        q_rev = e.get(\"reversed_query\",\"\")\n",
        "        if not (q_ori and q_ins and q_rev): continue\n",
        "\n",
        "        # Rankings within the pool\n",
        "        ord_ori, _ = rank_within_pool(q_ori, doc_texts)\n",
        "        ord_ins, _ = rank_within_pool(q_ins, doc_texts)\n",
        "        ord_rev, _ = rank_within_pool(q_rev, doc_texts)\n",
        "\n",
        "        def rank_of(idx, order):\n",
        "            # order holds indices into doc_texts in ranked order\n",
        "            return int(np.where(order == idx)[0][0]) + 1 if idx in order else len(order) + 1\n",
        "\n",
        "        r_ori = rank_of(pos_idx, ord_ori)\n",
        "        r_ins = rank_of(pos_idx, ord_ins)\n",
        "        r_rev = rank_of(pos_idx, ord_rev)\n",
        "\n",
        "        # mSICR\n",
        "        msicr.append(int((r_ins < r_ori) and (r_rev > r_ori)))\n",
        "\n",
        "        # mWISE\n",
        "        m = max(1, len((e.get(\"attributes\") or {}).keys()))\n",
        "        delta_ins = r_ori - r_ins\n",
        "        delta_rev = r_rev - r_ori\n",
        "        mwise.append((delta_ins - delta_rev)/m)\n",
        "\n",
        "        # MDCR (soft/strict): simple attribute presence in pos_text\n",
        "        attrs = e.get(\"attributes\",{}) or {}\n",
        "        if attrs:\n",
        "            lt = pos_text.lower()\n",
        "            scores = []\n",
        "            for k,v in attrs.items():\n",
        "                scores.append(1.0 if str(v).lower() in lt else 0.0)\n",
        "            soft = float(np.mean(scores))\n",
        "            thr = max(0.45, soft - 0.05)  # same heuristic you used\n",
        "            mdcr_soft.append(soft)\n",
        "            mdcr_s.append(int(all(s >= thr for s in scores)))\n",
        "        else:\n",
        "            mdcr_soft.append(0.0); mdcr_s.append(0)\n",
        "\n",
        "    def avg(x): return float(np.mean(x)) if x else 0.0\n",
        "    return {\n",
        "        \"count\": len(msicr),\n",
        "        \"mSICR\": avg(msicr),\n",
        "        \"mWISE\": avg(mwise),\n",
        "        \"MDCR_strict\": avg(mdcr_s),\n",
        "        \"MDCR_soft\": avg(mdcr_soft),\n",
        "    }\n",
        "\n",
        "metrics = evaluate(entries, limit=MAX_EVAL)\n",
        "print(\"\\n📊 Reranker metrics (subset):\")\n",
        "for k,v in metrics.items(): print(f\"{k}: {v:.4f}\" if isinstance(v,float) else f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTS20FOJeFm1",
        "outputId": "87a85837-b651-404c-ab83-13b311374317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Evaluating reranker: 100%|██████████| 1000/1000 [09:13<00:00,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Reranker metrics (subset):\n",
            "count: 1000\n",
            "mSICR: 0.0400\n",
            "mWISE: -0.3048\n",
            "MDCR_strict: 0.0120\n",
            "MDCR_soft: 0.1610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New one"
      ],
      "metadata": {
        "id": "jxZJcmQ7lEmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Fast Beyond-Content Reranker++ (T4-safe)\n",
        "# - List-wise (instructed), pos-only margins (original/reversed)\n",
        "# - LoRA adapters, small DeBERTa, AMP FP16, no torch.compile\n",
        "# - Works well on Tesla T4 / Colab\n",
        "#\n",
        "# Required files in CWD:\n",
        "#   - final_sorted.jsonl\n",
        "#   - query-doc.json\n",
        "# ================================================================\n",
        "!pip -q install transformers peft accelerate sentencepiece rank-bm25\n",
        "\n",
        "import os, json, random, numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from tqdm import tqdm\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# -------- Optional: LoRA (speeds training by tuning fewer params)\n",
        "try:\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    PEFT_AVAILABLE = True\n",
        "except Exception:\n",
        "    PEFT_AVAILABLE = False\n",
        "\n",
        "# ---------------- CONFIG (T4-safe defaults) ----------------\n",
        "SEED = 1337\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "FINAL_JSONL = \"final_sorted.jsonl\"\n",
        "QDOC_JSON   = \"query-doc.json\"\n",
        "\n",
        "BASE_MODEL   = \"microsoft/deberta-v3-xsmall\"   # quick backbone; upgrade later to -small/-base once stable\n",
        "USE_LORA     = True and PEFT_AVAILABLE\n",
        "MAX_LEN      = 192\n",
        "BATCH_SIZE   = 12\n",
        "GRAD_ACCUM   = 1\n",
        "EPOCHS       = 1\n",
        "LR_MAIN      = 4e-4 if USE_LORA else 3e-5\n",
        "WARMUP_RATIO = 0.06\n",
        "NUM_WORKERS  = 0                   # 0 is simplest/most stable on Colab\n",
        "\n",
        "K_CAND       = 6                   # pos + hard_neg + (K-2) others\n",
        "BM25_HARDK   = 4\n",
        "MAX_TRAIN    = 10000               # cap for speed; set None for full\n",
        "MAX_VAL      = 1200\n",
        "\n",
        "# Margins / weights\n",
        "MARGIN_IO    = 0.6                 # instructed > original\n",
        "MARGIN_OR    = 0.6                 # original   > reversed\n",
        "MARGIN_INS   = 0.25                # instructed pos > each neg (intra-list)\n",
        "W_LIST       = 1.0\n",
        "W_PAIR       = 1.0\n",
        "LAMBDA_ATTR  = 0.25                # small aux weight\n",
        "W_REVERSE_BOOST = 2.0\n",
        "\n",
        "OUTPUT_DIR   = \"reranker_beyond_listwise_fast_t4\"\n",
        "RESUME       = False\n",
        "ATTR_KEYS    = [\"audience\",\"format\",\"language\",\"length\",\"source\",\"keyword\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "GPU_NAME = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
        "AMP_DTYPE = torch.float16 if torch.cuda.is_available() else None\n",
        "\n",
        "# Speed/stability knobs (good for T4)\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "# ---------------- IO ----------------\n",
        "def load_jsonl(path):\n",
        "    rows=[]\n",
        "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
        "        for i,l in enumerate(f,1):\n",
        "            l=l.strip()\n",
        "            if not l: continue\n",
        "            try: rows.append(json.loads(l))\n",
        "            except: pass\n",
        "    return rows\n",
        "\n",
        "assert os.path.exists(FINAL_JSONL), f\"Missing {FINAL_JSONL} in current directory.\"\n",
        "assert os.path.exists(QDOC_JSON),   f\"Missing {QDOC_JSON} in current directory.\"\n",
        "\n",
        "with open(QDOC_JSON,\"r\",encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "entries = load_jsonl(FINAL_JSONL)\n",
        "print(f\"Loaded entries: {len(entries)} | query-doc groups: {len(qdoc)} | GPU: {GPU_NAME}\")\n",
        "\n",
        "# ---------------- Pools & per-q BM25 ----------------\n",
        "def norm_text(d):\n",
        "    if isinstance(d, dict): return str(d.get(\"text\",\"\")).strip()\n",
        "    if isinstance(d, list) and len(d)>0:\n",
        "        first=d[0]\n",
        "        return (first.get(\"text\",\"\").strip() if isinstance(first,dict) else str(first).strip())\n",
        "    if isinstance(d, str): return d.strip()\n",
        "    return \"\"\n",
        "\n",
        "qid_to_pool = {}\n",
        "for group in qdoc:\n",
        "    qid = group[\"query_id\"]\n",
        "    docs = []\n",
        "    for d in group.get(\"documents\", []):\n",
        "        t = d.get(\"text\",\"\").strip()\n",
        "        if t: docs.append(t)\n",
        "    ded, seen = [], set()\n",
        "    for t in docs:\n",
        "        key = t[:200]\n",
        "        if key in seen: continue\n",
        "        ded.append(t); seen.add(key)\n",
        "    qid_to_pool[qid] = ded\n",
        "\n",
        "def tokenize(s): return s.lower().split()\n",
        "bm25_by_qid = {}\n",
        "for qid, pool in qid_to_pool.items():\n",
        "    if len(pool) < 2: continue\n",
        "    tokenized = [tokenize(t) for t in pool]\n",
        "    bm25_by_qid[qid] = (BM25Okapi(tokenized), pool, tokenized)\n",
        "\n",
        "# ---------------- Weak attribute labels (string-match heuristic) ----------------\n",
        "def weak_attr_vec(text:str, attrs:Dict[str,Any]) -> np.ndarray:\n",
        "    lt = (text or \"\").lower()\n",
        "    return np.array([\n",
        "        1.0 if (k in (attrs or {}) and str(attrs[k]).lower() in lt) else 0.0\n",
        "        for k in ATTR_KEYS\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "# ---------------- Items ----------------\n",
        "@dataclass\n",
        "class TrainingItem:\n",
        "    qid:str; q_ori:str; q_ins:str; q_rev:str\n",
        "    pos_text:str; neg_text:str; attrs:Dict[str,Any]\n",
        "\n",
        "def build_items(records:List[Dict[str,Any]])->List[TrainingItem]:\n",
        "    items=[]\n",
        "    for e in records:\n",
        "        qid = e.get(\"query_id\")\n",
        "        q_ori = e.get(\"query\",\"\"); q_ins = e.get(\"instructed_query\",\"\"); q_rev = e.get(\"reversed_query\",\"\")\n",
        "        pos  = norm_text(e.get(\"positive_doc\")); neg  = norm_text(e.get(\"hard_negative_doc\"))\n",
        "        if qid in qid_to_pool and q_ori and q_ins and q_rev and pos and neg:\n",
        "            items.append(TrainingItem(qid,q_ori,q_ins,q_rev,pos,neg,e.get(\"attributes\",{}) or {}))\n",
        "    random.shuffle(items)\n",
        "    return items\n",
        "\n",
        "items_all = build_items(entries)\n",
        "if MAX_TRAIN: items_all = items_all[:MAX_TRAIN + (MAX_VAL or 0)]\n",
        "split = int(0.85 * len(items_all))\n",
        "train_items, val_items = items_all[:split], items_all[split:]\n",
        "if MAX_VAL: val_items = val_items[:MAX_VAL]\n",
        "print(f\"Using items: train={len(train_items)} | val={len(val_items)}\")\n",
        "\n",
        "# ---------------- Tokenizer ----------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "\n",
        "def pack_pairs(queries:List[str], docs:List[str]):\n",
        "    texts = [f\"[QUERY]\\n{q}\\n[DOCUMENT]\\n{d}\" for q,d in zip(queries, docs)]\n",
        "    return tokenizer(texts, max_length=MAX_LEN, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# ---------------- Dataset & Collator ----------------\n",
        "class ListwiseDataset(Dataset):\n",
        "    def __init__(self, items:List[TrainingItem], k:int=6):\n",
        "        self.items = items; self.k = k\n",
        "    def __len__(self): return len(self.items)\n",
        "    def _bm25_negs(self, qid:str, query:str, avoid:set, want:int)->List[str]:\n",
        "        pack = bm25_by_qid.get(qid)\n",
        "        if not pack: return []\n",
        "        bm25, pool, _ = pack\n",
        "        scores = bm25.get_scores(tokenize(query))\n",
        "        order = np.argsort(scores)[::-1]\n",
        "        out=[]\n",
        "        for idx in order:\n",
        "            doc = pool[idx]\n",
        "            if doc in avoid: continue\n",
        "            out.append(doc)\n",
        "            if len(out) >= want: break\n",
        "        return out\n",
        "    def __getitem__(self, i):\n",
        "        x = self.items[i]\n",
        "        pool = qid_to_pool.get(x.qid, [])\n",
        "        cand = [x.pos_text]; avoid = set([x.pos_text])\n",
        "        if x.neg_text and x.neg_text not in avoid:\n",
        "            cand.append(x.neg_text); avoid.add(x.neg_text)\n",
        "        need = max(0, K_CAND - len(cand))\n",
        "        bmnegs = self._bm25_negs(x.qid, x.q_ins, avoid, want=min(need, BM25_HARDK))\n",
        "        for d in bmnegs:\n",
        "            if d not in avoid: cand.append(d); avoid.add(d)\n",
        "        if len(cand) < K_CAND:\n",
        "            extras = [d for d in pool if d not in avoid]\n",
        "            random.shuffle(extras); cand.extend(extras[:K_CAND - len(cand)])\n",
        "        cand = cand[:K_CAND]\n",
        "        return {\n",
        "            \"qid\": x.qid, \"q_ori\": x.q_ori, \"q_ins\": x.q_ins, \"q_rev\": x.q_rev,\n",
        "            \"cands\": cand, \"pos_index\": 0, \"pos_attr\": weak_attr_vec(x.pos_text, x.attrs)\n",
        "        }\n",
        "\n",
        "class FastCollator:\n",
        "    \"\"\"INS: full K; ORI/REV: positive-only (for speed).\"\"\"\n",
        "    def __init__(self, k:int=6): self.k=k\n",
        "    def __call__(self, batch):\n",
        "        B = len(batch); K = self.k\n",
        "        docs_ins, queries_ins = [], []\n",
        "        pos_docs, queries_ori_pos, queries_rev_pos = [], [], []\n",
        "        pos_indices = []; pos_attr = []\n",
        "        for b in batch:\n",
        "            cands = b[\"cands\"]\n",
        "            if len(cands) < K: cands = cands + [cands[-1]]*(K-len(cands))\n",
        "            cands = cands[:K]\n",
        "            docs_ins.extend(cands); queries_ins.extend([b[\"q_ins\"]]*K)\n",
        "            pos_doc = cands[0]\n",
        "            pos_docs.append(pos_doc); queries_ori_pos.append(b[\"q_ori\"]); queries_rev_pos.append(b[\"q_rev\"])\n",
        "            pos_indices.append(0); pos_attr.append(b[\"pos_attr\"])\n",
        "        tok_ins     = pack_pairs(queries_ins,     docs_ins)\n",
        "        tok_pos_ori = pack_pairs(queries_ori_pos, pos_docs)\n",
        "        tok_pos_rev = pack_pairs(queries_rev_pos, pos_docs)\n",
        "        return {\n",
        "            \"tok_ins\": {k:v for k,v in tok_ins.items()},\n",
        "            \"tok_pos_ori\": {k:v for k,v in tok_pos_ori.items()},\n",
        "            \"tok_pos_rev\": {k:v for k,v in tok_pos_rev.items()},\n",
        "            \"pos_indices\": torch.tensor(pos_indices, dtype=torch.long),\n",
        "            \"pos_attr\": torch.tensor(np.stack(pos_attr), dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "class CrossEncoderWithAttr(nn.Module):\n",
        "    def __init__(self, base=BASE_MODEL, dropout=0.1, grad_ckpt=True):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(base)\n",
        "        if grad_ckpt and hasattr(self.encoder, \"gradient_checkpointing_enable\"):\n",
        "            self.encoder.gradient_checkpointing_enable()\n",
        "        if hasattr(self.encoder, \"enable_input_require_grads\"):\n",
        "            self.encoder.enable_input_require_grads()\n",
        "        h = self.encoder.config.hidden_size\n",
        "        self.dp = nn.Dropout(dropout)\n",
        "        self.rank = nn.Linear(h,1)\n",
        "        self.attr = nn.Linear(h,len(ATTR_KEYS))\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, **kwargs):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n",
        "        cls = self.dp(out.last_hidden_state[:,0,:])\n",
        "        return self.rank(cls).squeeze(-1), self.attr(cls)\n",
        "\n",
        "model = CrossEncoderWithAttr(BASE_MODEL, grad_ckpt=True)\n",
        "if USE_LORA:\n",
        "    try:\n",
        "        lconf = LoraConfig(\n",
        "            task_type=TaskType.FEATURE_EXTRACTION,\n",
        "            r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "            target_modules=[\"query_proj\",\"key_proj\",\"value_proj\",\"dense\",\"out_proj\"]\n",
        "        )\n",
        "        model = get_peft_model(model, lconf)\n",
        "        if hasattr(model, \"enable_input_require_grads\"):\n",
        "            model.enable_input_require_grads()\n",
        "        print(\"LoRA attached (FEATURE_EXTRACTION).\")\n",
        "    except Exception as e:\n",
        "        print(f\"LoRA attach failed; continuing without LoRA: {e}\")\n",
        "        USE_LORA=False\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Freeze backbone; train only LoRA + heads\n",
        "def mark_trainable(m):\n",
        "    trainable, total = 0, 0\n",
        "    for n,p in m.named_parameters():\n",
        "        total += p.numel()\n",
        "        if (\"lora_\" in n) or (\"rank\" in n) or (\"attr\" in n):\n",
        "            p.requires_grad_(True); trainable += p.numel()\n",
        "        else:\n",
        "            p.requires_grad_(False)\n",
        "    print(f\"Trainable params: {trainable/1e6:.2f}M / {total/1e6:.2f}M\")\n",
        "mark_trainable(model)\n",
        "\n",
        "# ---------------- Losses ----------------\n",
        "def listwise_ce(scores_bk:torch.Tensor, pos_idx:torch.Tensor)->torch.Tensor:\n",
        "    return nn.CrossEntropyLoss()(scores_bk, pos_idx)\n",
        "\n",
        "def bce_simple(logits, targets):  # average BCE\n",
        "    return nn.BCEWithLogitsLoss()(logits, targets)\n",
        "\n",
        "# ---------------- Dataloaders ----------------\n",
        "train_ds = ListwiseDataset(train_items, k=K_CAND)\n",
        "val_ds   = ListwiseDataset(val_items,   k=K_CAND)\n",
        "collate  = FastCollator(k=K_CAND)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate, num_workers=NUM_WORKERS)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate, num_workers=NUM_WORKERS)\n",
        "\n",
        "# ---------------- Optim & Sched ----------------\n",
        "opt = AdamW([p for p in model.parameters() if p.requires_grad], lr=LR_MAIN, weight_decay=0.01)\n",
        "total_steps = max(1, (len(train_loader) * EPOCHS) // GRAD_ACCUM)\n",
        "warm_steps  = int(total_steps * WARMUP_RATIO)\n",
        "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=warm_steps, num_training_steps=total_steps)\n",
        "\n",
        "# ---------------- Helpers ----------------\n",
        "def forward_scores(tok_dict):\n",
        "    tok = {k: v.to(device) for k,v in tok_dict.items()}\n",
        "    s, a = model(**tok)\n",
        "    return s, a\n",
        "\n",
        "def compute_losses(batch, K:int)->Tuple[torch.Tensor, dict]:\n",
        "    B = batch[\"pos_indices\"].shape[0]\n",
        "    pos_idx = batch[\"pos_indices\"].to(device)\n",
        "\n",
        "    # 1) INS list-wise over K\n",
        "    s_ins_flat, a_ins_flat = forward_scores(batch[\"tok_ins\"])       # [B*K], [B*K, A]\n",
        "    s_ins = s_ins_flat.view(B, K)\n",
        "    L_list = listwise_ce(s_ins, pos_idx)\n",
        "\n",
        "    # Attribute aux on positive\n",
        "    A = len(ATTR_KEYS)\n",
        "    a_ins = a_ins_flat.view(B, K, A)\n",
        "    a_ins_pos = a_ins[torch.arange(B), pos_idx, :]\n",
        "    attr_t = batch[\"pos_attr\"].to(device)\n",
        "    L_attr = bce_simple(a_ins_pos, attr_t)\n",
        "\n",
        "    # 2) POS-only margins for ORI/REV\n",
        "    s_ori_pos, _ = forward_scores(batch[\"tok_pos_ori\"])             # [B]\n",
        "    s_rev_pos, _ = forward_scores(batch[\"tok_pos_rev\"])             # [B]\n",
        "    s_ins_pos = s_ins[torch.arange(B), pos_idx]                     # [B]\n",
        "\n",
        "    L_io = torch.clamp(MARGIN_IO - (s_ins_pos - s_ori_pos), min=0).mean()\n",
        "    L_or = torch.clamp(MARGIN_OR - (s_ori_pos - s_rev_pos), min=0).mean() * W_REVERSE_BOOST\n",
        "\n",
        "    # Intra-instructed margin (pos > each neg)\n",
        "    neg_mask = torch.ones_like(s_ins, device=device); neg_mask[torch.arange(B), pos_idx] = 0.0\n",
        "    L_intra = torch.clamp(MARGIN_INS - (s_ins_pos.unsqueeze(1) - s_ins), min=0) * neg_mask\n",
        "    denom = neg_mask.sum(dim=1).clamp(min=1.0)\n",
        "    L_intra = (L_intra.sum(dim=1) / denom).mean()\n",
        "\n",
        "    loss = W_LIST*L_list + W_PAIR*(L_io + L_or + L_intra) + LAMBDA_ATTR*L_attr\n",
        "    logs = dict(L_list=L_list.item(), L_io=L_io.item(), L_or=L_or.item(), L_intra=L_intra.item(), L_attr=L_attr.item())\n",
        "    return loss, logs\n",
        "\n",
        "def evaluate(val_loader, K:int)->float:\n",
        "    model.eval(); losses=[]\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                with torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE):\n",
        "                    l,_ = compute_losses(batch, K)\n",
        "            else:\n",
        "                l,_ = compute_losses(batch, K)\n",
        "            losses.append(l.item())\n",
        "    return float(np.mean(losses)) if losses else 0.0\n",
        "\n",
        "# ---------------- Train ----------------\n",
        "best_val = float(\"inf\")\n",
        "print(\"Starting training…\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    pbar=tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
        "    meter={\"L_list\":0,\"L_io\":0,\"L_or\":0,\"L_intra\":0,\"L_attr\":0,\"n\":0}\n",
        "    for batch in pbar:\n",
        "        if torch.cuda.is_available():\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE):\n",
        "                loss, logs = compute_losses(batch, K_CAND)\n",
        "        else:\n",
        "            loss, logs = compute_losses(batch, K_CAND)\n",
        "\n",
        "        (loss/GRAD_ACCUM).backward()\n",
        "        torch.nn.utils.clip_grad_norm_([p for p in model.parameters() if p.requires_grad], 1.0)\n",
        "\n",
        "        opt.step(); sched.step(); opt.zero_grad(set_to_none=True)\n",
        "\n",
        "        for k in [\"L_list\",\"L_io\",\"L_or\",\"L_intra\",\"L_attr\"]: meter[k]+=logs[k]\n",
        "        meter[\"n\"]+=1\n",
        "        show = {k: round(meter[k]/meter[\"n\"],4) for k in [\"L_list\",\"L_io\",\"L_or\",\"L_intra\",\"L_attr\"]}\n",
        "        show[\"loss\"]=round(loss.item(),4); pbar.set_postfix(show)\n",
        "\n",
        "    v = evaluate(val_loader, K_CAND)\n",
        "    print(f\">> Val (combined loss): {v:.4f}\")\n",
        "    if v < best_val:\n",
        "        best_val = v\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"pytorch_model.bin\"))\n",
        "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "        print(f\"Saved BEST to {OUTPUT_DIR} (val={best_val:.4f})\")\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuO89_ncI9uf",
        "outputId": "fa7c873b-ab21-4490-f6bd-ada4abcc7e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded entries: 9596 | query-doc groups: 515 | GPU: Tesla T4\n",
            "Using items: train=8112 | val=1200\n",
            "LoRA attached (FEATURE_EXTRACTION).\n",
            "Trainable params: 1.33M / 72.01M\n",
            "Starting training…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  25%|██▌       | 169/676 [03:38<09:52,  1.17s/it, L_list=1.51, L_io=0.251, L_or=0.366, L_intra=0.236, L_attr=0.342, loss=1.61]"
          ]
        }
      ]
    }
  ]
}