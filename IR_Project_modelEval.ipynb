{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Azure Open AI Endpoint and Key:  GPT - 4o - Mini Model**"
      ],
      "metadata": {
        "id": "cyrWGRG2EbNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "endpoint = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "model_name = \"gpt-4o-mini\"\n",
        "deployment = \"gpt-4o-mini\"\n",
        "\n",
        "subscription_key = \"#hidden\"\n",
        "api_version = \"2024-12-01-preview\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"I am going to Paris, what should I see?\",\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=4096,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    model=deployment\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3Eb040YEYlM",
        "outputId": "83401475-e289-485c-a627-3c82df237057",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris is a city rich in history, art, culture, and beauty. Here’s a list of must-see attractions and experiences to consider during your visit:\n",
            "\n",
            "1. **Eiffel Tower** - A symbol of Paris, you can either admire it from the ground or take an elevator to the top for stunning views of the city.\n",
            "\n",
            "2. **Louvre Museum** - Home to thousands of works of art, including the Mona Lisa and the Venus de Milo. It's advisable to plan your visit, as it can be overwhelming due to its size.\n",
            "\n",
            "3. **Notre-Dame Cathedral** - Although it is undergoing restoration, the exterior remains impressive. Explore the Île de la Cité while you’re there.\n",
            "\n",
            "4. **Sacré-Cœur Basilica** - Located on Montmartre hill, this basilica offers beautiful views of Paris and features stunning mosaics inside.\n",
            "\n",
            "5. **Champs-Élysées and Arc de Triomphe** - Stroll down this famous avenue and visit the iconic arch at the western end.\n",
            "\n",
            "6. **Palace of Versailles** - A day trip from Paris, this opulent palace and its gardens are a perfect example of royal grandeur.\n",
            "\n",
            "7. **Musée d'Orsay** - Housed in a former railway station, this museum features an extensive collection of Impressionist and Post-Impressionist masterpieces.\n",
            "\n",
            "8. **Seine River Cruise** - A boat cruise on the Seine offers a great perspective of many iconic landmarks, especially at sunset.\n",
            "\n",
            "9. **Montmartre** - Explore this artistic neighborhood, visit the Place du Tertre, and enjoy the bohemian atmosphere.\n",
            "\n",
            "10. **Le Marais** - An historic district known for its narrow streets, boutique shops, and vibrant atmosphere. Don’t miss the Place des Vosges.\n",
            "\n",
            "11. **Sainte-Chapelle** - Famous for its breathtaking stained-glass windows, this Gothic chapel is a hidden gem located near Notre-Dame.\n",
            "\n",
            "12. **Luxembourg Gardens** - A beautiful park where you can relax, enjoy the scenery, and even watch local Parisians play pétanque.\n",
            "\n",
            "13. **Catacombs of Paris** - For something different, explore the underground ossuaries that hold the remains of millions of Parisians.\n",
            "\n",
            "14. **Père Lachaise Cemetery** - Visit the graves of famous figures such as Jim Morrison, Oscar Wilde, and Édith Piaf.\n",
            "\n",
            "15. **Shopping** - Consider visiting the Galeries Lafayette for luxury shopping or the charming boutiques in the Le Marais or Saint-Germain-des-Prés.\n",
            "\n",
            "Make sure to also indulge in the local cuisine. Try classic French dishes, pastries from bakeries, and enjoy a coffee at one of Paris’s many cafés. Enjoy your trip!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-1OjTk5DOZr",
        "outputId": "d9cbd982-b48e-4f06-976a-9745deece30e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ir_datasets\n",
            "  Downloading ir_datasets-0.5.11-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (4.13.5)\n",
            "Collecting inscriptis>=2.2.0 (from ir_datasets)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (2.32.4)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir_datasets)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting lz4>=3.1.10 (from ir_datasets)\n",
            "  Downloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting warc3-wet>=0.2.3 (from ir_datasets)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir_datasets)\n",
            "  Downloading zlib_state-0.1.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting ijson>=3.1.3 (from ir_datasets)\n",
            "  Downloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir_datasets)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (18.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (2025.8.3)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading ir_datasets-0.5.11-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=62531b6d80204e358011c2fc0682dc666d9453e5afc11447dfd2e11055201010\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/85/c2/9f0f621def52a1d5db7d29984f81e45f9fb6dfeb1a4eb6e31c\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp312-cp312-linux_x86_64.whl size=55023 sha256=6f625c716e1c35c035c31a63a991c0c8ec5f311fd16ad39ed876c694d38e6773\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/3e/21/a739cbcc331a1ab45c326d6edbdac6118de4402f6076e30ff1\n",
            "Successfully built warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, cbor, zlib-state, unlzw3, trec-car-tools, lz4, ijson, inscriptis, ir_datasets\n",
            "Successfully installed cbor-1.0.0 ijson-3.4.0.post0 inscriptis-2.6.0 ir_datasets-0.5.11 lz4-4.4.4 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.10\n"
          ]
        }
      ],
      "source": [
        "pip install ir_datasets tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collecting Base Query Documents from msmarco and beir**"
      ],
      "metadata": {
        "id": "YrKWvOClDtHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FINAL Step A — Build base_dataset_questq.jsonl\n",
        "Compatible with your available BEIR datasets.\n",
        "Collects query–document pairs for multi-attribute dataset building.\n",
        "\"\"\"\n",
        "\n",
        "import ir_datasets, random, json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ✅ Choose only available dataset identifiers\n",
        "SAMPLES = {\n",
        "    \"msmarco-passage/train\": 400,            # open-domain search\n",
        "    \"beir/cqadupstack/programmers\": 100,     # StackOverflow-style Q&A\n",
        "    \"beir/fever\": 100,                       # fact verification\n",
        "    \"beir/scidocs\": 50,                      # academic domain\n",
        "    \"beir/quora\": 50,                        # question paraphrasing\n",
        "}\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def sample_dataset(name, sample_size):\n",
        "    print(f\"\\n📘 Processing {name}\")\n",
        "    ds = ir_datasets.load(name)\n",
        "\n",
        "    # Ensure dataset has the necessary parts\n",
        "    for needed in [\"queries_iter\", \"docs_iter\", \"qrels_iter\"]:\n",
        "        if not hasattr(ds, needed):\n",
        "            raise AttributeError(f\"Dataset {name} missing: {needed}\")\n",
        "\n",
        "    queries = {q.query_id: q.text for q in ds.queries_iter()}\n",
        "    docs = {d.doc_id: d.text for d in ds.docs_iter()}\n",
        "    qrels = list(ds.qrels_iter())\n",
        "\n",
        "    # Build mapping query → list of relevant doc IDs\n",
        "    pos_map = {}\n",
        "    for q in qrels:\n",
        "        if q.relevance > 0:\n",
        "            pos_map.setdefault(q.query_id, []).append(q.doc_id)\n",
        "\n",
        "    valid_qids = list(pos_map.keys())\n",
        "    if not valid_qids:\n",
        "        print(f\"⚠️ No valid qrels for {name}\")\n",
        "        return []\n",
        "\n",
        "    chosen_qids = random.sample(valid_qids, min(sample_size, len(valid_qids)))\n",
        "    results = []\n",
        "\n",
        "    for qid in tqdm(chosen_qids):\n",
        "        query = queries.get(qid, \"\")\n",
        "        pos_docs = pos_map.get(qid, [])\n",
        "        for did in pos_docs[:3]:  # take up to 3 positive docs\n",
        "            if did in docs:\n",
        "                results.append({\n",
        "                    \"dataset\": name,\n",
        "                    \"query_id\": qid,\n",
        "                    \"query\": query,\n",
        "                    \"document\": docs[did],\n",
        "                    \"relevance\": 1\n",
        "                })\n",
        "\n",
        "    print(f\"✅ Collected {len(results)} pairs from {name}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# ------------------ MAIN EXECUTION ------------------\n",
        "all_data = []\n",
        "for name, n in SAMPLES.items():\n",
        "    try:\n",
        "        all_data.extend(sample_dataset(name, n))\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Skipping {name}: {e}\")\n",
        "\n",
        "# Save the combined dataset\n",
        "with open(\"1_base_dataset_questq.jsonl\", \"w\", encoding=\"utf8\") as f:\n",
        "    for item in all_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"\\n🎯 Done! Collected {len(all_data)} total query–doc pairs.\")\n",
        "print(\"📂 Output saved to base_dataset_questq.jsonl\")\n"
      ],
      "metadata": {
        "id": "8r32D9TZDpoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Attributes to Base Dataset**"
      ],
      "metadata": {
        "id": "RNb3qPqOD7yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time, random\n",
        "from tqdm import tqdm\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# 🔹 Azure setup\n",
        "endpoint = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "api_version = \"2024-12-01-preview\"\n",
        "deployment = \"gpt-4o-mini\"\n",
        "api_key = \"#hidden\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# 🔹 Load base dataset\n",
        "with open(\"1_base_dataset_questq.jsonl\") as f:\n",
        "    base_data = [json.loads(l) for l in f]\n",
        "\n",
        "# 🔹 Few-shot examples to guide GPT\n",
        "FEWSHOT = \"\"\"\n",
        "Examples:\n",
        "1️⃣ Query: \"what is the purpose of DNA replication\"\n",
        "Document: \"DNA replication ensures each cell gets an exact copy of the DNA during cell division.\"\n",
        "Attributes:\n",
        "{\n",
        " \"audience\": \"Student\",\n",
        " \"keyword\": [\"Biology\", \"Genetics\"],\n",
        " \"format\": \"Academic Paper\",\n",
        " \"language\": \"English\",\n",
        " \"length\": \"Short\",\n",
        " \"source\": \"Wikipedia\"\n",
        "}\n",
        "\n",
        "2️⃣ Query: \"price of a bushel of wheat\"\n",
        "Document: \"Interactive chart of historical daily wheat prices...\"\n",
        "Attributes:\n",
        "{\n",
        " \"audience\": \"Researcher\",\n",
        " \"keyword\": [\"Economics\", \"Agriculture\"],\n",
        " \"format\": \"Report\",\n",
        " \"language\": \"English\",\n",
        " \"length\": \"Short\",\n",
        " \"source\": \"NewsSite\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 🔹 Attribute generation function\n",
        "def get_attributes(query, document, retries=2):\n",
        "    prompt = f\"\"\"\n",
        "You are labeling information retrieval data using InfoSearch-style attributes.\n",
        "\n",
        "{FEWSHOT}\n",
        "\n",
        "Now label this new pair.\n",
        "Return ONLY a valid JSON dictionary (no explanations, no markdown).\n",
        "\n",
        "Query: {query}\n",
        "Document: {document[:800]}\n",
        "JSON:\n",
        "\"\"\"\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=deployment,\n",
        "                temperature=0.4,\n",
        "                max_tokens=250,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "            # Clean and try parsing JSON\n",
        "            start = content.find(\"{\")\n",
        "            end = content.rfind(\"}\") + 1\n",
        "            json_part = content[start:end]\n",
        "            attrs = json.loads(json_part)\n",
        "            # Ensure all expected keys exist\n",
        "            required = {\"audience\", \"keyword\", \"format\", \"language\", \"length\", \"source\"}\n",
        "            if required.issubset(attrs.keys()):\n",
        "                return attrs\n",
        "        except Exception as e:\n",
        "            time.sleep(1)\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "# 🔹 Process all samples\n",
        "enriched = []\n",
        "for i, item in enumerate(tqdm(base_data, desc=\"Annotating\")):\n",
        "    attrs = get_attributes(item[\"query\"], item[\"document\"])\n",
        "    if attrs:\n",
        "        item.update(attrs)\n",
        "        enriched.append(item)\n",
        "\n",
        "# 🔹 Save\n",
        "with open(\"2_base_with_multi_attri.jsonl\", \"w\", encoding=\"utf8\") as f:\n",
        "    for e in enriched:\n",
        "        f.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Created multi_attr_dataset.jsonl with {len(enriched)} labeled pairs.\")\n"
      ],
      "metadata": {
        "id": "GjsZ5oJ8D64C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Attribute Instructed + Reversed Query Generation**"
      ],
      "metadata": {
        "id": "8ZYDYTL51L1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Install required packages ---\n",
        "!pip install openai aiohttp nest_asyncio -q\n",
        "\n",
        "#2. Imports and setup ---\n",
        "import json, re, time, asyncio, nest_asyncio\n",
        "from openai import AsyncAzureOpenAI\n",
        "nest_asyncio.apply()\n",
        "\n",
        "#3. Azure OpenAI configuration ---\n",
        "API_KEY = \"#hidden\"\n",
        "ENDPOINT = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "API_VERSION = \"2024-12-01-preview\"\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "client = AsyncAzureOpenAI(\n",
        "    api_key=API_KEY,\n",
        "    azure_endpoint=ENDPOINT,\n",
        "    api_version=API_VERSION\n",
        ")\n",
        "\n",
        "#4. Rate-limit and save settings ---\n",
        "TOKENS_PER_MINUTE = 100_000\n",
        "REQUESTS_PER_MINUTE = 1000\n",
        "MAX_CONCURRENT_REQUESTS = 8\n",
        "SLEEP_BETWEEN_REQUESTS = 60 / REQUESTS_PER_MINUTE\n",
        "SAVE_INTERVAL = 50   # Auto-save every 50 queries\n",
        "\n",
        "def build_prompt(query, attributes):\n",
        "    \"\"\"Prompt asking GPT to return instructed and reversed versions as strict JSON.\"\"\"\n",
        "    attr_text = \", \".join([f\"{k}: {v}\" for k, v in attributes.items() if v])\n",
        "    return f\"\"\"\n",
        "You are generating search queries with multiple document-level attributes.\n",
        "\n",
        "Given a base query and its attributes, produce:\n",
        "1. An instructed version that naturally includes 2–3 attributes.\n",
        "2. A reversed instructed version that logically negates those same attributes.\n",
        "\n",
        "Return output ONLY as a JSON object with two keys:\n",
        "\"instructed_query\" and \"reversed_query\".\n",
        "\n",
        "Example:\n",
        "Base query: \"best travel destinations in Europe\"\n",
        "Attributes: format=blog, language=English, audience=layman\n",
        "\n",
        "Output:\n",
        "{{\n",
        "  \"instructed_query\": \"List the best travel destinations in Europe. Please provide a blog in English for laymen.\",\n",
        "  \"reversed_query\": \"List the best travel destinations in Europe. Please do not provide a blog in English for laymen.\"\n",
        "}}\n",
        "\n",
        "Now for this:\n",
        "Base query: \"{query}\"\n",
        "Attributes: {attr_text}\n",
        "\"\"\"\n",
        "\n",
        "def safe_json_parse(text):\n",
        "    \"\"\"Try robust JSON parsing; fallback to regex extraction if needed.\"\"\"\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        instructed = re.search(r'\"?instructed[_ ]?query\"?[:\\-]?\\s*[\"“](.*?)[\"”]', text, re.I | re.S)\n",
        "        reversed_q = re.search(r'\"?reversed[_ ]?query\"?[:\\-]?\\s*[\"“](.*?)[\"”]', text, re.I | re.S)\n",
        "        return {\n",
        "            \"instructed_query\": instructed.group(1).strip() if instructed else \"\",\n",
        "            \"reversed_query\": reversed_q.group(1).strip() if reversed_q else \"\"\n",
        "        }\n",
        "\n",
        "# Async GPT-4 call for one entry\n",
        "\n",
        "async def process_entry(entry):\n",
        "    query = entry[\"query\"]\n",
        "    attributes = {\n",
        "        \"audience\": entry.get(\"audience\", \"\"),\n",
        "        \"format\": entry.get(\"format\", \"\"),\n",
        "        \"language\": entry.get(\"language\", \"\"),\n",
        "        \"length\": entry.get(\"length\", \"\"),\n",
        "        \"source\": entry.get(\"source\", \"\")\n",
        "    }\n",
        "\n",
        "    prompt = build_prompt(query, attributes)\n",
        "\n",
        "    try:\n",
        "        response = await client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            temperature=0.4,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            response_format={\"type\": \"json_object\"}  # force JSON output\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        parsed = safe_json_parse(content)\n",
        "\n",
        "        entry[\"instructed_query\"] = parsed.get(\"instructed_query\", \"\")\n",
        "        entry[\"reversed_query\"] = parsed.get(\"reversed_query\", \"\")\n",
        "        return entry\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error on {entry.get('query_id','?')}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Async batch generator\n",
        "\n",
        "async def generate_instructed_queries():\n",
        "    input_file = \"2_base_with_multi_attri.jsonl\"\n",
        "    output_file = \"multi_attr_instructed.jsonl\"\n",
        "\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "\n",
        "    print(f\"📘 Loaded {len(data)} entries from {input_file}\")\n",
        "\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "    processed = []\n",
        "\n",
        "    async def limited_process(entry):\n",
        "        async with semaphore:\n",
        "            result = await process_entry(entry)\n",
        "            await asyncio.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "            return result\n",
        "\n",
        "    tasks = [limited_process(e) for e in data]\n",
        "\n",
        "    for i, coro in enumerate(asyncio.as_completed(tasks), 1):\n",
        "        result = await coro\n",
        "        if result:\n",
        "            processed.append(result)\n",
        "\n",
        "        # Auto-save every N records\n",
        "        if i % SAVE_INTERVAL == 0:\n",
        "            with open(output_file, \"a\") as f:\n",
        "                for p in processed[-SAVE_INTERVAL:]:\n",
        "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "            print(f\"💾 Auto-saved {i} entries...\")\n",
        "\n",
        "    # Final save\n",
        "    with open(output_file, \"a\") as f:\n",
        "        for p in processed:\n",
        "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"✅ Finished generating {len(processed)} instructed queries!\")\n",
        "    print(f\"📁 Output: {output_file}\")\n",
        "\n",
        "\n",
        "await generate_instructed_queries()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrZNUCkXM6AJ",
        "outputId": "68226c6d-1b5d-4268-f2e5-4a2b2ea3cb2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Loaded 715 entries from multi_attr_dataset.jsonl\n",
            "💾 Auto-saved 50 entries...\n",
            "💾 Auto-saved 100 entries...\n",
            "💾 Auto-saved 150 entries...\n",
            "💾 Auto-saved 200 entries...\n",
            "💾 Auto-saved 250 entries...\n",
            "💾 Auto-saved 300 entries...\n",
            "💾 Auto-saved 350 entries...\n",
            "💾 Auto-saved 400 entries...\n",
            "💾 Auto-saved 450 entries...\n",
            "💾 Auto-saved 500 entries...\n",
            "💾 Auto-saved 550 entries...\n",
            "💾 Auto-saved 600 entries...\n",
            "💾 Auto-saved 650 entries...\n",
            "💾 Auto-saved 700 entries...\n",
            "✅ Finished generating 715 instructed queries!\n",
            "📁 Output: multi_attr_instructed.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document Rewriting + Hard Negative Generation**"
      ],
      "metadata": {
        "id": "WdaSfQcstGTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 🚀 Step 4 — Document Rewriting + Hard Negative Generation\n",
        "# ====================================================\n",
        "\n",
        "!pip install openai aiohttp nest_asyncio -q\n",
        "\n",
        "import json, re, time, asyncio, nest_asyncio\n",
        "from openai import AsyncAzureOpenAI\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Azure config (same as Step 3) ---\n",
        "API_KEY = \"#hidden\"\n",
        "ENDPOINT = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "API_VERSION = \"2024-12-01-preview\"\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "client = AsyncAzureOpenAI(\n",
        "    api_key=API_KEY,\n",
        "    azure_endpoint=ENDPOINT,\n",
        "    api_version=API_VERSION\n",
        ")\n",
        "\n",
        "# --- Rate control ---\n",
        "MAX_CONCURRENT_REQUESTS = 8\n",
        "REQUESTS_PER_MINUTE = 1000\n",
        "SLEEP_BETWEEN_REQUESTS = 60 / REQUESTS_PER_MINUTE\n",
        "SAVE_INTERVAL = 50\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 🧠 Prompt builder\n",
        "# ====================================================\n",
        "\n",
        "def build_doc_prompt(document, attributes):\n",
        "    attr_text = \", \".join([f\"{k}: {v}\" for k, v in attributes.items() if v])\n",
        "    return f\"\"\"\n",
        "You are refining a document for an information-retrieval benchmark.\n",
        "\n",
        "Task 1 – Rewrite the base document so that it **fully satisfies all given attributes**.\n",
        "Task 2 – Create **one hard negative document** that is **topically similar** but **violates at least one attribute**\n",
        "(e.g., wrong format, language, or audience).\n",
        "\n",
        "Return **only** a JSON object:\n",
        "{{\n",
        "  \"positive_doc\": \"... rewritten version matching attributes ...\",\n",
        "  \"hard_negative_doc\": \"... realistic but violating version ...\"\n",
        "}}\n",
        "\n",
        "Base document:\n",
        "\\\"\\\"\\\"{document}\\\"\\\"\\\"\n",
        "Attributes: {attr_text}\n",
        "\"\"\"\n",
        "\n",
        "# ====================================================\n",
        "# ⚙️ Async single entry processor\n",
        "# ====================================================\n",
        "\n",
        "def safe_json_parse(text):\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        pos = re.search(r'\"?positive[_ ]?doc\"?[:\\-]?\\s*[\"“](.*?)[\"”]', text, re.I | re.S)\n",
        "        neg = re.search(r'\"?hard[_ ]?negative[_ ]?doc\"?[:\\-]?\\s*[\"“](.*?)[\"”]', text, re.I | re.S)\n",
        "        return {\n",
        "            \"positive_doc\": pos.group(1).strip() if pos else \"\",\n",
        "            \"hard_negative_doc\": neg.group(1).strip() if neg else \"\"\n",
        "        }\n",
        "\n",
        "async def process_doc(entry):\n",
        "    document = entry.get(\"document\", \"\")\n",
        "    attributes = {\n",
        "        \"audience\": entry.get(\"audience\", \"\"),\n",
        "        \"format\": entry.get(\"format\", \"\"),\n",
        "        \"language\": entry.get(\"language\", \"\"),\n",
        "        \"length\": entry.get(\"length\", \"\"),\n",
        "        \"source\": entry.get(\"source\", \"\")\n",
        "    }\n",
        "\n",
        "    prompt = build_doc_prompt(document, attributes)\n",
        "    try:\n",
        "        response = await client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            temperature=0.5,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        parsed = safe_json_parse(content)\n",
        "        entry[\"positive_doc\"] = parsed.get(\"positive_doc\", \"\")\n",
        "        entry[\"hard_negative_doc\"] = parsed.get(\"hard_negative_doc\", \"\")\n",
        "        return entry\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error on {entry.get('query_id','?')}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 🚀 Async batch processor\n",
        "# ====================================================\n",
        "\n",
        "async def rewrite_documents():\n",
        "    input_file = \"multi_attr_instructed.jsonl\"\n",
        "    output_file = \"multi_attr_docs.jsonl\"\n",
        "\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "\n",
        "    print(f\"📘 Loaded {len(data)} entries from {input_file}\")\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "    processed = []\n",
        "\n",
        "    async def limited(entry):\n",
        "        async with semaphore:\n",
        "            result = await process_doc(entry)\n",
        "            await asyncio.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "            return result\n",
        "\n",
        "    tasks = [limited(e) for e in data]\n",
        "\n",
        "    for i, coro in enumerate(asyncio.as_completed(tasks), 1):\n",
        "        res = await coro\n",
        "        if res:\n",
        "            processed.append(res)\n",
        "        if i % SAVE_INTERVAL == 0:\n",
        "            with open(output_file, \"a\") as f:\n",
        "                for p in processed[-SAVE_INTERVAL:]:\n",
        "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "            print(f\"💾 Saved {i} entries...\")\n",
        "\n",
        "    # final save\n",
        "    with open(output_file, \"a\") as f:\n",
        "        for p in processed:\n",
        "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"✅ Finished rewriting {len(processed)} documents!\")\n",
        "    print(f\"📁 Output: {output_file}\")\n",
        "\n",
        "# ====================================================\n",
        "# ▶️ Run rewriting inside Colab/Jupyter\n",
        "# ====================================================\n",
        "await rewrite_documents()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNXoFTSH68Pf",
        "outputId": "ac549d3d-8831-46bc-ab21-f9fb774bd536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Loaded 2818 entries from multi_attr_instructed.jsonl\n",
            "💾 Saved 50 entries...\n",
            "❌ Error on 919029: 'NoneType' object has no attribute 'strip'\n",
            "💾 Saved 100 entries...\n",
            "💾 Saved 150 entries...\n",
            "💾 Saved 200 entries...\n",
            "💾 Saved 250 entries...\n",
            "💾 Saved 300 entries...\n",
            "💾 Saved 350 entries...\n",
            "💾 Saved 400 entries...\n",
            "💾 Saved 450 entries...\n",
            "💾 Saved 500 entries...\n",
            "💾 Saved 550 entries...\n",
            "💾 Saved 600 entries...\n",
            "💾 Saved 650 entries...\n",
            "💾 Saved 700 entries...\n",
            "💾 Saved 750 entries...\n",
            "💾 Saved 800 entries...\n",
            "💾 Saved 850 entries...\n",
            "💾 Saved 900 entries...\n",
            "💾 Saved 950 entries...\n",
            "💾 Saved 1000 entries...\n",
            "💾 Saved 1050 entries...\n",
            "💾 Saved 1100 entries...\n",
            "💾 Saved 1150 entries...\n",
            "💾 Saved 1200 entries...\n",
            "💾 Saved 1250 entries...\n",
            "💾 Saved 1300 entries...\n",
            "❌ Error on 737282: 'NoneType' object has no attribute 'strip'\n",
            "💾 Saved 1350 entries...\n",
            "💾 Saved 1400 entries...\n",
            "💾 Saved 1450 entries...\n",
            "💾 Saved 1500 entries...\n",
            "💾 Saved 1550 entries...\n",
            "💾 Saved 1600 entries...\n",
            "💾 Saved 1650 entries...\n",
            "💾 Saved 1700 entries...\n",
            "💾 Saved 1750 entries...\n",
            "💾 Saved 1800 entries...\n",
            "💾 Saved 1850 entries...\n",
            "💾 Saved 1900 entries...\n",
            "💾 Saved 1950 entries...\n",
            "💾 Saved 2000 entries...\n",
            "💾 Saved 2050 entries...\n",
            "💾 Saved 2100 entries...\n",
            "❌ Error on 737282: 'NoneType' object has no attribute 'strip'\n",
            "💾 Saved 2150 entries...\n",
            "💾 Saved 2200 entries...\n",
            "❌ Error on 737282: 'NoneType' object has no attribute 'strip'\n",
            "💾 Saved 2250 entries...\n",
            "💾 Saved 2300 entries...\n",
            "💾 Saved 2350 entries...\n",
            "💾 Saved 2400 entries...\n",
            "💾 Saved 2450 entries...\n",
            "💾 Saved 2500 entries...\n",
            "💾 Saved 2550 entries...\n",
            "💾 Saved 2600 entries...\n",
            "💾 Saved 2650 entries...\n",
            "💾 Saved 2700 entries...\n",
            "💾 Saved 2750 entries...\n",
            "💾 Saved 2800 entries...\n",
            "✅ Finished rewriting 2814 documents!\n",
            "📁 Output: multi_attr_docs.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UPDATED WITH ATTRIBUTES MENTIONED**"
      ],
      "metadata": {
        "id": "lk9Qvmndq_Do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Attribute Instructed + Reversed Query Generation + Document Rewriting + Hard Negative Generation** (Updated)\n"
      ],
      "metadata": {
        "id": "rBGWsMbvtTbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# STEP 1: Install & Imports\n",
        "# =========================================================\n",
        "!pip install openai aiohttp nest_asyncio -q\n",
        "\n",
        "import json, re, time, asyncio, nest_asyncio, random, copy\n",
        "from openai import AsyncAzureOpenAI\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# =========================================================\n",
        "# STEP 2: Azure OpenAI Configuration\n",
        "# =========================================================\n",
        "API_KEY = \"#hidden\"\n",
        "ENDPOINT = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "API_VERSION = \"2024-12-01-preview\"\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "client = AsyncAzureOpenAI(\n",
        "    api_key=API_KEY,\n",
        "    azure_endpoint=ENDPOINT,\n",
        "    api_version=API_VERSION\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# STEP 3: Rate Control\n",
        "# =========================================================\n",
        "MAX_CONCURRENT_REQUESTS = 6\n",
        "REQUESTS_PER_MINUTE = 1000\n",
        "SLEEP_BETWEEN_REQUESTS = 60 / REQUESTS_PER_MINUTE\n",
        "SAVE_INTERVAL = 50\n",
        "\n",
        "# =========================================================\n",
        "# STEP 4: Helper Functions\n",
        "# =========================================================\n",
        "\n",
        "def safe_json_parse(text):\n",
        "    \"\"\"Try to parse model output as JSON; fallback to regex.\"\"\"\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception:\n",
        "        instructed = re.search(r'\"?instructed[_ ]?query\"?[:\\-]?\\s*[\"“](.*?)[\"”]', text, re.I | re.S)\n",
        "        reversed_q = re.search(r'\"?reversed[_ ]?query\"?[:\\-]?\\s*[\"“](.*?)[\"”]', text, re.I | re.S)\n",
        "        pos = re.search(r'\"?positive[_ ]?doc\"?[:\\-]?\\s*[\"“](.*?)[\"”]', text, re.I | re.S)\n",
        "        neg = re.search(r'\"?hard[_ ]?negative[_ ]?doc\"?[:\\-]?\\s*[\"“](.*?)[\"”]', text, re.I | re.S)\n",
        "        return {\n",
        "            \"instructed_query\": instructed.group(1).strip() if instructed else \"\",\n",
        "            \"reversed_query\": reversed_q.group(1).strip() if reversed_q else \"\",\n",
        "            \"positive_doc\": pos.group(1).strip() if pos else \"\",\n",
        "            \"hard_negative_doc\": neg.group(1).strip() if neg else \"\"\n",
        "        }\n",
        "\n",
        "def build_instruct_prompt(query, attributes):\n",
        "    \"\"\"Prompt for Instructed + Reversed query generation.\"\"\"\n",
        "    attr_text = \", \".join([f\"{k}: {v}\" for k, v in attributes.items() if v])\n",
        "    return f\"\"\"\n",
        "You are generating search queries with multiple document-level attributes.\n",
        "\n",
        "Given a base query and its attributes, produce:\n",
        "1. An instructed version that naturally includes all given attributes.\n",
        "2. A reversed instructed version that logically negates them.\n",
        "\n",
        "Return a JSON object:\n",
        "{{\n",
        "  \"instructed_query\": \"...\",\n",
        "  \"reversed_query\": \"...\"\n",
        "}}\n",
        "\n",
        "Base query: \"{query}\"\n",
        "Attributes: {attr_text}\n",
        "\"\"\"\n",
        "\n",
        "def build_doc_prompt(document, attributes):\n",
        "    \"\"\"Prompt for rewriting + hard negative generation.\"\"\"\n",
        "    attr_text = \", \".join([f\"{k}: {v}\" for k, v in attributes.items() if v])\n",
        "    return f\"\"\"\n",
        "You are refining a document for an information-retrieval benchmark.\n",
        "\n",
        "Task 1 – Rewrite the base document so that it **fully satisfies all given attributes**.\n",
        "Task 2 – Create one **hard negative document** that is **topically similar** but violates one or two attributes.\n",
        "\n",
        "Return only JSON:\n",
        "{{\n",
        "  \"positive_doc\": \"... rewritten version ...\",\n",
        "  \"hard_negative_doc\": \"... violating version ...\"\n",
        "}}\n",
        "\n",
        "Base document:\n",
        "\\\"\\\"\\\"{document}\\\"\\\"\\\"\n",
        "Attributes: {attr_text}\n",
        "\"\"\"\n",
        "\n",
        "def sample_attr_combinations(entry, num_combinations=5):\n",
        "    \"\"\"Generate 3–5 random 2–3 attribute combinations for a single query.\"\"\"\n",
        "    all_attrs = {\n",
        "        \"audience\": entry.get(\"audience\", \"\"),\n",
        "        \"format\": entry.get(\"format\", \"\"),\n",
        "        \"language\": entry.get(\"language\", \"\"),\n",
        "        \"length\": entry.get(\"length\", \"\"),\n",
        "        \"source\": entry.get(\"source\", \"\")\n",
        "    }\n",
        "    non_empty = {k: v for k, v in all_attrs.items() if v}\n",
        "    num_combinations = random.randint(3, 5)\n",
        "\n",
        "    combos = []\n",
        "    for _ in range(num_combinations):\n",
        "        chosen = random.sample(list(non_empty.keys()), k=min(random.randint(2, 3), len(non_empty)))\n",
        "        combos.append({k: non_empty[k] for k in chosen})\n",
        "    return combos\n",
        "\n",
        "# =========================================================\n",
        "# STEP 5: Async Generation\n",
        "# =========================================================\n",
        "\n",
        "async def generate_instructed_queries():\n",
        "    input_file = \"2_base_with_multi_attri.jsonl\"\n",
        "    output_file = \"3_instructed_reverse_queries_with_attri.jsonl\"\n",
        "\n",
        "    with open(input_file, \"r\") as f:\n",
        "        base_data = [json.loads(line) for line in f]\n",
        "\n",
        "    print(f\"📘 Loaded {len(base_data)} core queries\")\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "    processed = []\n",
        "\n",
        "    async def process_combination(base_entry, combo, combo_id):\n",
        "        \"\"\"Generate instructed and reversed queries for one combination.\"\"\"\n",
        "        new_entry = copy.deepcopy(base_entry)\n",
        "        new_entry[\"attributes\"] = combo\n",
        "        new_entry[\"combo_id\"] = combo_id\n",
        "\n",
        "        prompt = build_instruct_prompt(base_entry[\"query\"], combo)\n",
        "        try:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                temperature=0.4,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "            parsed = safe_json_parse(response.choices[0].message.content)\n",
        "            new_entry[\"instructed_query\"] = parsed.get(\"instructed_query\", \"\")\n",
        "            new_entry[\"reversed_query\"] = parsed.get(\"reversed_query\", \"\")\n",
        "            new_entry[\"query_type\"] = \"expanded\"\n",
        "            return new_entry\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error on {base_entry.get('query_id')}: {e}\")\n",
        "            return None\n",
        "\n",
        "    tasks = []\n",
        "    for entry in base_data:\n",
        "        combos = sample_attr_combinations(entry)\n",
        "        for i, combo in enumerate(combos):\n",
        "            tasks.append(process_combination(entry, combo, i+1))\n",
        "\n",
        "    async def limited_task(task):\n",
        "        async with semaphore:\n",
        "            res = await task\n",
        "            await asyncio.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "            return res\n",
        "\n",
        "    wrapped_tasks = [limited_task(t) for t in tasks]\n",
        "    for i, coro in enumerate(asyncio.as_completed(wrapped_tasks), 1):\n",
        "        res = await coro\n",
        "        if res:\n",
        "            processed.append(res)\n",
        "        if i % SAVE_INTERVAL == 0:\n",
        "            with open(output_file, \"a\") as f:\n",
        "                for p in processed[-SAVE_INTERVAL:]:\n",
        "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "            print(f\"💾 Auto-saved {i} combinations...\")\n",
        "\n",
        "    with open(output_file, \"a\") as f:\n",
        "        for p in processed:\n",
        "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"✅ Generated {len(processed)} instructed + reversed combinations!\")\n",
        "\n",
        "# =========================================================\n",
        "# STEP 6: Document Rewriting\n",
        "# =========================================================\n",
        "\n",
        "async def rewrite_documents():\n",
        "    input_file = \"3_instructed_reverse_queries_with_attri.jsonl\"\n",
        "    output_file = \"4_rewritten_docs_hard_negatives_with_attri.jsonl\"\n",
        "\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "    processed = []\n",
        "\n",
        "    async def process_doc(entry):\n",
        "        attributes = entry.get(\"attributes\", {})\n",
        "        prompt = build_doc_prompt(entry.get(\"document\", \"\"), attributes)\n",
        "        try:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                temperature=0.5,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "            parsed = safe_json_parse(response.choices[0].message.content)\n",
        "            violated = random.sample(\n",
        "                list(attributes.keys()), k=min(random.randint(1, 2), len(attributes))\n",
        "            ) if attributes else []\n",
        "            entry[\"positive_doc\"] = parsed.get(\"positive_doc\", \"\")\n",
        "            entry[\"hard_negative_doc\"] = parsed.get(\"hard_negative_doc\", \"\")\n",
        "            entry[\"violated_attributes\"] = violated\n",
        "            return entry\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error on query_id={entry.get('query_id')}: {e}\")\n",
        "            return None\n",
        "\n",
        "    tasks = [process_doc(e) for e in data]\n",
        "    async def limited(entry_task):\n",
        "        async with semaphore:\n",
        "            res = await entry_task\n",
        "            await asyncio.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "            return res\n",
        "\n",
        "    wrapped_tasks = [limited(t) for t in tasks]\n",
        "    for i, coro in enumerate(asyncio.as_completed(wrapped_tasks), 1):\n",
        "        res = await coro\n",
        "        if res:\n",
        "            processed.append(res)\n",
        "        if i % SAVE_INTERVAL == 0:\n",
        "            with open(output_file, \"a\") as f:\n",
        "                for p in processed[-SAVE_INTERVAL:]:\n",
        "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "            print(f\"💾 Saved {i} rewritten docs...\")\n",
        "\n",
        "    with open(output_file, \"a\") as f:\n",
        "        for p in processed:\n",
        "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"✅ Finished rewriting {len(processed)} documents!\")\n",
        "\n",
        "# =========================================================\n",
        "# STEP 7: Run Sequentially (Colab-safe)\n",
        "# =========================================================\n",
        "await generate_instructed_queries()\n",
        "await rewrite_documents()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaXoOlf-Zfvh",
        "outputId": "caad315e-8a14-47d0-9729-9bd0b8f3d529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Loaded 715 core queries\n",
            "💾 Auto-saved 50 combinations...\n",
            "💾 Auto-saved 100 combinations...\n",
            "💾 Auto-saved 150 combinations...\n",
            "💾 Auto-saved 200 combinations...\n",
            "💾 Auto-saved 250 combinations...\n",
            "💾 Auto-saved 300 combinations...\n",
            "💾 Auto-saved 350 combinations...\n",
            "💾 Auto-saved 400 combinations...\n",
            "💾 Auto-saved 450 combinations...\n",
            "💾 Auto-saved 500 combinations...\n",
            "💾 Auto-saved 550 combinations...\n",
            "💾 Auto-saved 600 combinations...\n",
            "💾 Auto-saved 650 combinations...\n",
            "💾 Auto-saved 700 combinations...\n",
            "💾 Auto-saved 750 combinations...\n",
            "💾 Auto-saved 800 combinations...\n",
            "💾 Auto-saved 850 combinations...\n",
            "💾 Auto-saved 900 combinations...\n",
            "💾 Auto-saved 950 combinations...\n",
            "💾 Auto-saved 1000 combinations...\n",
            "💾 Auto-saved 1050 combinations...\n",
            "💾 Auto-saved 1100 combinations...\n",
            "💾 Auto-saved 1150 combinations...\n",
            "💾 Auto-saved 1200 combinations...\n",
            "💾 Auto-saved 1250 combinations...\n",
            "💾 Auto-saved 1300 combinations...\n",
            "💾 Auto-saved 1350 combinations...\n",
            "💾 Auto-saved 1400 combinations...\n",
            "💾 Auto-saved 1450 combinations...\n",
            "💾 Auto-saved 1500 combinations...\n",
            "💾 Auto-saved 1550 combinations...\n",
            "💾 Auto-saved 1600 combinations...\n",
            "💾 Auto-saved 1650 combinations...\n",
            "💾 Auto-saved 1700 combinations...\n",
            "💾 Auto-saved 1750 combinations...\n",
            "💾 Auto-saved 1800 combinations...\n",
            "💾 Auto-saved 1850 combinations...\n",
            "💾 Auto-saved 1900 combinations...\n",
            "💾 Auto-saved 1950 combinations...\n",
            "💾 Auto-saved 2000 combinations...\n",
            "💾 Auto-saved 2050 combinations...\n",
            "❌ Error on 362662: expected string or bytes-like object, got 'NoneType'\n",
            "💾 Auto-saved 2100 combinations...\n",
            "💾 Auto-saved 2150 combinations...\n",
            "💾 Auto-saved 2200 combinations...\n",
            "💾 Auto-saved 2250 combinations...\n",
            "💾 Auto-saved 2300 combinations...\n",
            "💾 Auto-saved 2350 combinations...\n",
            "💾 Auto-saved 2400 combinations...\n",
            "💾 Auto-saved 2450 combinations...\n",
            "💾 Auto-saved 2500 combinations...\n",
            "💾 Auto-saved 2550 combinations...\n",
            "💾 Auto-saved 2600 combinations...\n",
            "💾 Auto-saved 2650 combinations...\n",
            "💾 Auto-saved 2700 combinations...\n",
            "💾 Auto-saved 2750 combinations...\n",
            "💾 Auto-saved 2800 combinations...\n",
            "✅ Generated 2819 instructed + reversed combinations!\n",
            "💾 Saved 50 rewritten docs...\n",
            "💾 Saved 100 rewritten docs...\n",
            "💾 Saved 150 rewritten docs...\n",
            "💾 Saved 200 rewritten docs...\n",
            "💾 Saved 250 rewritten docs...\n",
            "💾 Saved 300 rewritten docs...\n",
            "💾 Saved 350 rewritten docs...\n",
            "💾 Saved 400 rewritten docs...\n",
            "💾 Saved 450 rewritten docs...\n",
            "💾 Saved 500 rewritten docs...\n",
            "💾 Saved 550 rewritten docs...\n",
            "💾 Saved 600 rewritten docs...\n",
            "💾 Saved 650 rewritten docs...\n",
            "💾 Saved 700 rewritten docs...\n",
            "💾 Saved 750 rewritten docs...\n",
            "💾 Saved 800 rewritten docs...\n",
            "💾 Saved 850 rewritten docs...\n",
            "💾 Saved 900 rewritten docs...\n",
            "💾 Saved 950 rewritten docs...\n",
            "💾 Saved 1000 rewritten docs...\n",
            "💾 Saved 1050 rewritten docs...\n",
            "💾 Saved 1100 rewritten docs...\n",
            "💾 Saved 1150 rewritten docs...\n",
            "💾 Saved 1200 rewritten docs...\n",
            "💾 Saved 1250 rewritten docs...\n",
            "💾 Saved 1300 rewritten docs...\n",
            "💾 Saved 1350 rewritten docs...\n",
            "💾 Saved 1400 rewritten docs...\n",
            "💾 Saved 1450 rewritten docs...\n",
            "💾 Saved 1500 rewritten docs...\n",
            "💾 Saved 1550 rewritten docs...\n",
            "💾 Saved 1600 rewritten docs...\n",
            "💾 Saved 1650 rewritten docs...\n",
            "💾 Saved 1700 rewritten docs...\n",
            "💾 Saved 1750 rewritten docs...\n",
            "💾 Saved 1800 rewritten docs...\n",
            "💾 Saved 1850 rewritten docs...\n",
            "💾 Saved 1900 rewritten docs...\n",
            "💾 Saved 1950 rewritten docs...\n",
            "💾 Saved 2000 rewritten docs...\n",
            "💾 Saved 2050 rewritten docs...\n",
            "💾 Saved 2100 rewritten docs...\n",
            "💾 Saved 2150 rewritten docs...\n",
            "💾 Saved 2200 rewritten docs...\n",
            "💾 Saved 2250 rewritten docs...\n",
            "❌ Error on query_id=362662: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
            "❌ Error on query_id=362662: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
            "💾 Saved 2300 rewritten docs...\n",
            "💾 Saved 2350 rewritten docs...\n",
            "💾 Saved 2400 rewritten docs...\n",
            "💾 Saved 2450 rewritten docs...\n",
            "💾 Saved 2500 rewritten docs...\n",
            "💾 Saved 2550 rewritten docs...\n",
            "💾 Saved 2600 rewritten docs...\n",
            "💾 Saved 2650 rewritten docs...\n",
            "💾 Saved 2700 rewritten docs...\n",
            "💾 Saved 2750 rewritten docs...\n",
            "💾 Saved 2800 rewritten docs...\n",
            "💾 Saved 2850 rewritten docs...\n",
            "💾 Saved 2900 rewritten docs...\n",
            "💾 Saved 2950 rewritten docs...\n",
            "💾 Saved 3000 rewritten docs...\n",
            "💾 Saved 3050 rewritten docs...\n",
            "💾 Saved 3100 rewritten docs...\n",
            "💾 Saved 3150 rewritten docs...\n",
            "💾 Saved 3200 rewritten docs...\n",
            "💾 Saved 3250 rewritten docs...\n",
            "💾 Saved 3300 rewritten docs...\n",
            "💾 Saved 3350 rewritten docs...\n",
            "💾 Saved 3400 rewritten docs...\n",
            "💾 Saved 3450 rewritten docs...\n",
            "💾 Saved 3500 rewritten docs...\n",
            "💾 Saved 3550 rewritten docs...\n",
            "💾 Saved 3600 rewritten docs...\n",
            "💾 Saved 3650 rewritten docs...\n",
            "💾 Saved 3700 rewritten docs...\n",
            "💾 Saved 3750 rewritten docs...\n",
            "💾 Saved 3800 rewritten docs...\n",
            "💾 Saved 3850 rewritten docs...\n",
            "💾 Saved 3900 rewritten docs...\n",
            "💾 Saved 3950 rewritten docs...\n",
            "❌ Error on query_id=362662: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
            "💾 Saved 4000 rewritten docs...\n",
            "💾 Saved 4050 rewritten docs...\n",
            "💾 Saved 4100 rewritten docs...\n",
            "💾 Saved 4150 rewritten docs...\n",
            "💾 Saved 4200 rewritten docs...\n",
            "💾 Saved 4250 rewritten docs...\n",
            "💾 Saved 4300 rewritten docs...\n",
            "💾 Saved 4350 rewritten docs...\n",
            "💾 Saved 4400 rewritten docs...\n",
            "💾 Saved 4450 rewritten docs...\n",
            "💾 Saved 4500 rewritten docs...\n",
            "💾 Saved 4550 rewritten docs...\n",
            "💾 Saved 4600 rewritten docs...\n",
            "💾 Saved 4650 rewritten docs...\n",
            "💾 Saved 4700 rewritten docs...\n",
            "💾 Saved 4750 rewritten docs...\n",
            "💾 Saved 4800 rewritten docs...\n",
            "💾 Saved 4850 rewritten docs...\n",
            "💾 Saved 4900 rewritten docs...\n",
            "💾 Saved 4950 rewritten docs...\n",
            "💾 Saved 5000 rewritten docs...\n",
            "💾 Saved 5050 rewritten docs...\n",
            "💾 Saved 5100 rewritten docs...\n",
            "💾 Saved 5150 rewritten docs...\n",
            "💾 Saved 5200 rewritten docs...\n",
            "💾 Saved 5250 rewritten docs...\n",
            "💾 Saved 5300 rewritten docs...\n",
            "💾 Saved 5350 rewritten docs...\n",
            "💾 Saved 5400 rewritten docs...\n",
            "💾 Saved 5450 rewritten docs...\n",
            "💾 Saved 5500 rewritten docs...\n",
            "💾 Saved 5550 rewritten docs...\n",
            "💾 Saved 5600 rewritten docs...\n",
            "✅ Finished rewriting 5616 documents!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "input_file = \"4_rewritten_docs_hard_negatives_with_attri.jsonl\"          # your input file\n",
        "output_file = \"final_sorted.jsonl\"  # sorted output file\n",
        "\n",
        "with open(input_file, \"r\") as f:\n",
        "    data = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "def sort_key(entry):\n",
        "    qid = str(entry.get(\"query_id\", \"\")).strip()\n",
        "    # Try to convert to int if purely numeric, else fallback to string\n",
        "    return (0, int(qid)) if qid.isdigit() else (1, qid.lower())\n",
        "\n",
        "data.sort(key=sort_key)\n",
        "\n",
        "with open(output_file, \"w\") as f:\n",
        "    for entry in data:\n",
        "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Sorted {len(data)} records by query_id.\")\n",
        "print(f\"📁 Output saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "XltoyGbDqlGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Process - J**"
      ],
      "metadata": {
        "id": "uJF-HTsDyGwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyserini tqdm numpy pandas -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pViowtWMyOMr",
        "outputId": "904ab40f-d448-43f9-897e-f648bcfdc2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.5/178.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyserini (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dense Retrieval Models**"
      ],
      "metadata": {
        "id": "omEeO3VwQP96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*BM25*"
      ],
      "metadata": {
        "id": "pfMXdozXI9CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25 tqdm numpy pandas -q\n"
      ],
      "metadata": {
        "id": "a5PotpR8zjQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from rank_bm25 import BM25Okapi\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---------------- SAFE LOAD ----------------\n",
        "def safe_load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"⚠ Skipping invalid JSON line {i}\")\n",
        "    return data\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "queries = safe_load_jsonl(\"final_sorted.jsonl\")\n",
        "\n",
        "# Build corpus and mappings\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "# Tokenize corpus\n",
        "tokenized_corpus = [c.lower().split() for c in corpus]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "print(f\"✅ Loaded {len(queries)} queries, {len(corpus)} documents.\")\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    tokens = query.lower().split()\n",
        "    scores = bm25.get_scores(tokens)\n",
        "    ranked_idx = np.argsort(scores)[::-1][:k]\n",
        "    return [(doc_ids[i], float(scores[i])) for i in ranked_idx]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K used in metrics\n",
        "\n",
        "for q in tqdm(queries, desc=\"Evaluating BM25\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "        Rori = retrieve_topk(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # ranks\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        # fraction of attributes satisfied in gold doc\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-correct non-linear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(\"\\n📊 BM25 Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It1AFBOSydtY",
        "outputId": "26bc0566-1103-4f13-b7b4-1c06b32d8749"
      },
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 9596 queries, 9576 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating BM25: 100%|██████████| 9596/9596 [20:12<00:00,  7.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 BM25 Evaluation Metrics (SOP-aligned):\n",
            "{\n",
            "  \"mSICR\": 0.03511879949979158,\n",
            "  \"mWISE\": -1.0332243928067535,\n",
            "  \"MDCR_strict\": 0.01979991663192997,\n",
            "  \"MDCR_soft\": 0.22087328053355565\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Bge-Large-v1.5*"
      ],
      "metadata": {
        "id": "VqsyNB-FJFXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"BAAI/bge-large-en-v1.5\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "Ut03NYZRloc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*E5-Large-v2*"
      ],
      "metadata": {
        "id": "q8XSbMmiM7tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"intfloat/e5-large-v2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "RHm9m2hvsA0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Instructor-XL*"
      ],
      "metadata": {
        "id": "ftNHNxpvNEiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"hkunlp/instructor-large\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "2JfN3dhosOdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*GTE-Qwen2*"
      ],
      "metadata": {
        "id": "QO3G7JTUNv1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"thenlper/gte-small\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "ds878A5LsOZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*E5-Mistral-ins*"
      ],
      "metadata": {
        "id": "4EI7_XSdN9Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"intfloat/e5-mistral-7b-instruct\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "GjWAtHs5ODDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*GritLM*"
      ],
      "metadata": {
        "id": "21OBFtv0OK4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"GritLM/GritLM-7B\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "_5AkTYZ8OLWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*SFR-Embedding-2-R*"
      ],
      "metadata": {
        "id": "6alPV_eMOX5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"Salesforce/SFR-Embedding-2-R\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "V2umgYZMObEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*NV-Embed-v2*"
      ],
      "metadata": {
        "id": "-EcLx4D2OhJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"nvidia/NV-Embed-v2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "UsdnqZsqOkGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Point-wise Re-ranking Models**"
      ],
      "metadata": {
        "id": "VSWqtGKSO4Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Mistral-ins-v0.2*"
      ],
      "metadata": {
        "id": "EKslfmfQO8Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "Lge9r5MYPA3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Llama-3.1*"
      ],
      "metadata": {
        "id": "WhWxLuO4POJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "d2Ipv_hLPRdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List-wise Re-ranking**"
      ],
      "metadata": {
        "id": "TsIqlSDZPryl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Zephyr-beta*"
      ],
      "metadata": {
        "id": "Vp-mm07CPYGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "he4kJuAkJJxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*RankVicuna-v1*"
      ],
      "metadata": {
        "id": "qpR98e9rPwT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"microsoft/RankVicuna-v1\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "x6W3-nIePzc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*RankZephyr-v1*"
      ],
      "metadata": {
        "id": "S6ZZieaBP4F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------- MODEL SETUP ----------------\n",
        "model_name = \"HuggingFaceH4/RankZephyr-7b-v1\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "# ---------------- LOAD FILES ----------------\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "with open(\"final_sorted.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [json.loads(line) for line in f]\n",
        "\n",
        "# ---------------- CORPUS PREPARATION ----------------\n",
        "corpus, doc_ids, qid_map = [], [], {}\n",
        "for entry in qdoc:\n",
        "    qid = entry.get(\"query_id\")\n",
        "    qid_map[qid] = []\n",
        "    for doc in entry.get(\"documents\", []):\n",
        "        text = doc.get(\"text\", \"\").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        corpus.append(text)\n",
        "        did = f\"{qid}_{doc.get('doc_id', 'unk')}\"\n",
        "        doc_ids.append(did)\n",
        "        qid_map[qid].append(did)\n",
        "\n",
        "print(f\"Encoding {len(corpus)} documents using {model_name}...\")\n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
        "\n",
        "# ---------------- RETRIEVAL HELPERS ----------------\n",
        "def retrieve_topk_dense(query, k=10):\n",
        "    if not query:\n",
        "        return []\n",
        "    query_emb = model.encode(query, convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k)\n",
        "    return [(doc_ids[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "def find_rank(docid, ranking):\n",
        "    for i, (d, _) in enumerate(ranking):\n",
        "        if d == docid:\n",
        "            return i + 1\n",
        "    return len(ranking) + 1\n",
        "\n",
        "# ---------------- EVALUATION ----------------\n",
        "results = []\n",
        "K = 10  # top-K cutoff\n",
        "\n",
        "for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "    try:\n",
        "        qid = q.get(\"query_id\")\n",
        "        if qid not in qid_map:\n",
        "            continue\n",
        "\n",
        "        pos_doc = f\"{qid}_doc_1\"\n",
        "\n",
        "        # Retrieve results for each query type\n",
        "        Rori = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "        Rins = retrieve_topk_dense(q.get(\"instructed_query\", \"\"), k=K)\n",
        "        Rrev = retrieve_topk_dense(q.get(\"reversed_query\", \"\"), k=K)\n",
        "\n",
        "        # Rank positions\n",
        "        Rori_rank = find_rank(pos_doc, Rori)\n",
        "        Rins_rank = find_rank(pos_doc, Rins)\n",
        "        Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "        # ---------- mSICR ----------\n",
        "        Iq = int((Rins_rank < Rori_rank) and (Rrev_rank > Rori_rank))\n",
        "\n",
        "        # ---------- mWISE ----------\n",
        "        m = len(q.get(\"attributes\", {}))\n",
        "        pos_field = q.get(\"positive_doc\", \"\")\n",
        "        pos_text = \"\"\n",
        "        if isinstance(pos_field, dict):\n",
        "            pos_text = pos_field.get(\"text\", \"\").lower()\n",
        "        elif isinstance(pos_field, str):\n",
        "            pos_text = pos_field.lower()\n",
        "\n",
        "        sat = sum(1 for v in q.get(\"attributes\", {}).values() if str(v).lower() in pos_text)\n",
        "        frac_satisfied = sat / max(1, m)\n",
        "        frac_violated = 1 - frac_satisfied\n",
        "\n",
        "        delta_ins = Rori_rank - Rins_rank\n",
        "        delta_rev = Rrev_rank - Rori_rank\n",
        "\n",
        "        # SOP-aligned nonlinear formulation\n",
        "        reward = frac_satisfied * (1 - np.sqrt(abs(delta_ins) / K)) * (1 / np.sqrt(max(1, Rins_rank)))\n",
        "        penalty = -frac_violated * (1 + np.sqrt(abs(delta_rev) / K))  # heavier penalty for reversed\n",
        "        mwise = reward + penalty\n",
        "\n",
        "        # ---------- MDCR ----------\n",
        "        dims = list(q.get(\"attributes\", {}).keys())\n",
        "        mq = len(dims)\n",
        "        top_k_docs = retrieve_topk_dense(q.get(\"query\", \"\"), k=K)\n",
        "\n",
        "        def satisfies(doc_text, attr_dict):\n",
        "            doc_text = doc_text.lower()\n",
        "            return {d: int(str(v).lower() in doc_text) for d, v in attr_dict.items()}\n",
        "\n",
        "        strict_scores, soft_scores = [], []\n",
        "        for docid, _ in top_k_docs:\n",
        "            try:\n",
        "                idx = doc_ids.index(docid)\n",
        "                text = corpus[idx].lower()\n",
        "                s = satisfies(text, q.get(\"attributes\", {}))\n",
        "                strict_scores.append(np.prod(list(s.values())) if mq > 0 else 0)\n",
        "                soft_scores.append(sum(s.values()) / mq if mq > 0 else 0)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        MDCR_strict = max(strict_scores) if strict_scores else 0\n",
        "        MDCR_soft = max(soft_scores) if soft_scores else 0\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": Iq,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_strict\": MDCR_strict,\n",
        "            \"MDCR_soft\": MDCR_soft\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Skipping query {q.get('query_id', '?')} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "# ---------------- AGGREGATION ----------------\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean()\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 {model_name} Evaluation Metrics (SOP-aligned):\")\n",
        "print(json.dumps(metrics, indent=2))"
      ],
      "metadata": {
        "id": "THnx8qAwP6sN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}