{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Azure Open AI Endpoint and Key:  GPT - 4o - Mini Model**"
      ],
      "metadata": {
        "id": "cyrWGRG2EbNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "endpoint = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "model_name = \"gpt-4o-mini\"\n",
        "deployment = \"gpt-4o-mini\"\n",
        "\n",
        "subscription_key = \"#hidden\"\n",
        "api_version = \"2024-12-01-preview\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"I am going to Paris, what should I see?\",\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=4096,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    model=deployment\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3Eb040YEYlM",
        "outputId": "83401475-e289-485c-a627-3c82df237057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris is a city rich in history, art, culture, and beauty. Here’s a list of must-see attractions and experiences to consider during your visit:\n",
            "\n",
            "1. **Eiffel Tower** - A symbol of Paris, you can either admire it from the ground or take an elevator to the top for stunning views of the city.\n",
            "\n",
            "2. **Louvre Museum** - Home to thousands of works of art, including the Mona Lisa and the Venus de Milo. It's advisable to plan your visit, as it can be overwhelming due to its size.\n",
            "\n",
            "3. **Notre-Dame Cathedral** - Although it is undergoing restoration, the exterior remains impressive. Explore the Île de la Cité while you’re there.\n",
            "\n",
            "4. **Sacré-Cœur Basilica** - Located on Montmartre hill, this basilica offers beautiful views of Paris and features stunning mosaics inside.\n",
            "\n",
            "5. **Champs-Élysées and Arc de Triomphe** - Stroll down this famous avenue and visit the iconic arch at the western end.\n",
            "\n",
            "6. **Palace of Versailles** - A day trip from Paris, this opulent palace and its gardens are a perfect example of royal grandeur.\n",
            "\n",
            "7. **Musée d'Orsay** - Housed in a former railway station, this museum features an extensive collection of Impressionist and Post-Impressionist masterpieces.\n",
            "\n",
            "8. **Seine River Cruise** - A boat cruise on the Seine offers a great perspective of many iconic landmarks, especially at sunset.\n",
            "\n",
            "9. **Montmartre** - Explore this artistic neighborhood, visit the Place du Tertre, and enjoy the bohemian atmosphere.\n",
            "\n",
            "10. **Le Marais** - An historic district known for its narrow streets, boutique shops, and vibrant atmosphere. Don’t miss the Place des Vosges.\n",
            "\n",
            "11. **Sainte-Chapelle** - Famous for its breathtaking stained-glass windows, this Gothic chapel is a hidden gem located near Notre-Dame.\n",
            "\n",
            "12. **Luxembourg Gardens** - A beautiful park where you can relax, enjoy the scenery, and even watch local Parisians play pétanque.\n",
            "\n",
            "13. **Catacombs of Paris** - For something different, explore the underground ossuaries that hold the remains of millions of Parisians.\n",
            "\n",
            "14. **Père Lachaise Cemetery** - Visit the graves of famous figures such as Jim Morrison, Oscar Wilde, and Édith Piaf.\n",
            "\n",
            "15. **Shopping** - Consider visiting the Galeries Lafayette for luxury shopping or the charming boutiques in the Le Marais or Saint-Germain-des-Prés.\n",
            "\n",
            "Make sure to also indulge in the local cuisine. Try classic French dishes, pastries from bakeries, and enjoy a coffee at one of Paris’s many cafés. Enjoy your trip!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-1OjTk5DOZr",
        "outputId": "d9cbd982-b48e-4f06-976a-9745deece30e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ir_datasets\n",
            "  Downloading ir_datasets-0.5.11-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (4.13.5)\n",
            "Collecting inscriptis>=2.2.0 (from ir_datasets)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (2.32.4)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir_datasets)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting lz4>=3.1.10 (from ir_datasets)\n",
            "  Downloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting warc3-wet>=0.2.3 (from ir_datasets)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir_datasets)\n",
            "  Downloading zlib_state-0.1.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting ijson>=3.1.3 (from ir_datasets)\n",
            "  Downloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir_datasets)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (18.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (2025.8.3)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading ir_datasets-0.5.11-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=62531b6d80204e358011c2fc0682dc666d9453e5afc11447dfd2e11055201010\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/85/c2/9f0f621def52a1d5db7d29984f81e45f9fb6dfeb1a4eb6e31c\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp312-cp312-linux_x86_64.whl size=55023 sha256=6f625c716e1c35c035c31a63a991c0c8ec5f311fd16ad39ed876c694d38e6773\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/3e/21/a739cbcc331a1ab45c326d6edbdac6118de4402f6076e30ff1\n",
            "Successfully built warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, cbor, zlib-state, unlzw3, trec-car-tools, lz4, ijson, inscriptis, ir_datasets\n",
            "Successfully installed cbor-1.0.0 ijson-3.4.0.post0 inscriptis-2.6.0 ir_datasets-0.5.11 lz4-4.4.4 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.10\n"
          ]
        }
      ],
      "source": [
        "pip install ir_datasets tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collecting Base Query Documents from msmarco and beir**"
      ],
      "metadata": {
        "id": "YrKWvOClDtHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FINAL Step A — Build base_dataset_questq.jsonl\n",
        "Compatible with your available BEIR datasets.\n",
        "Collects query–document pairs for multi-attribute dataset building.\n",
        "\"\"\"\n",
        "\n",
        "import ir_datasets, random, json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ✅ Choose only available dataset identifiers\n",
        "SAMPLES = {\n",
        "    \"msmarco-passage/train\": 400,            # open-domain search\n",
        "    \"beir/cqadupstack/programmers\": 100,     # StackOverflow-style Q&A\n",
        "    \"beir/fever\": 100,                       # fact verification\n",
        "    \"beir/scidocs\": 50,                      # academic domain\n",
        "    \"beir/quora\": 50,                        # question paraphrasing\n",
        "}\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def sample_dataset(name, sample_size):\n",
        "    print(f\"\\n📘 Processing {name}\")\n",
        "    ds = ir_datasets.load(name)\n",
        "\n",
        "    # Ensure dataset has the necessary parts\n",
        "    for needed in [\"queries_iter\", \"docs_iter\", \"qrels_iter\"]:\n",
        "        if not hasattr(ds, needed):\n",
        "            raise AttributeError(f\"Dataset {name} missing: {needed}\")\n",
        "\n",
        "    queries = {q.query_id: q.text for q in ds.queries_iter()}\n",
        "    docs = {d.doc_id: d.text for d in ds.docs_iter()}\n",
        "    qrels = list(ds.qrels_iter())\n",
        "\n",
        "    # Build mapping query → list of relevant doc IDs\n",
        "    pos_map = {}\n",
        "    for q in qrels:\n",
        "        if q.relevance > 0:\n",
        "            pos_map.setdefault(q.query_id, []).append(q.doc_id)\n",
        "\n",
        "    valid_qids = list(pos_map.keys())\n",
        "    if not valid_qids:\n",
        "        print(f\"⚠️ No valid qrels for {name}\")\n",
        "        return []\n",
        "\n",
        "    chosen_qids = random.sample(valid_qids, min(sample_size, len(valid_qids)))\n",
        "    results = []\n",
        "\n",
        "    for qid in tqdm(chosen_qids):\n",
        "        query = queries.get(qid, \"\")\n",
        "        pos_docs = pos_map.get(qid, [])\n",
        "        for did in pos_docs[:3]:  # take up to 3 positive docs\n",
        "            if did in docs:\n",
        "                results.append({\n",
        "                    \"dataset\": name,\n",
        "                    \"query_id\": qid,\n",
        "                    \"query\": query,\n",
        "                    \"document\": docs[did],\n",
        "                    \"relevance\": 1\n",
        "                })\n",
        "\n",
        "    print(f\"✅ Collected {len(results)} pairs from {name}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# ------------------ MAIN EXECUTION ------------------\n",
        "all_data = []\n",
        "for name, n in SAMPLES.items():\n",
        "    try:\n",
        "        all_data.extend(sample_dataset(name, n))\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Skipping {name}: {e}\")\n",
        "\n",
        "# Save the combined dataset\n",
        "with open(\"base_dataset_questq.jsonl\", \"w\", encoding=\"utf8\") as f:\n",
        "    for item in all_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"\\n🎯 Done! Collected {len(all_data)} total query–doc pairs.\")\n",
        "print(\"📂 Output saved to base_dataset_questq.jsonl\")\n"
      ],
      "metadata": {
        "id": "8r32D9TZDpoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Attributes to Base Dataset**"
      ],
      "metadata": {
        "id": "RNb3qPqOD7yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time, random\n",
        "from tqdm import tqdm\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# 🔹 Azure setup\n",
        "endpoint = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "api_version = \"2024-12-01-preview\"\n",
        "deployment = \"gpt-4o-mini\"\n",
        "api_key = \"#hidden\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# 🔹 Load base dataset\n",
        "with open(\"base_dataset_questq.jsonl\") as f:\n",
        "    base_data = [json.loads(l) for l in f]\n",
        "\n",
        "# 🔹 Few-shot examples to guide GPT\n",
        "FEWSHOT = \"\"\"\n",
        "Examples:\n",
        "1️⃣ Query: \"what is the purpose of DNA replication\"\n",
        "Document: \"DNA replication ensures each cell gets an exact copy of the DNA during cell division.\"\n",
        "Attributes:\n",
        "{\n",
        " \"audience\": \"Student\",\n",
        " \"keyword\": [\"Biology\", \"Genetics\"],\n",
        " \"format\": \"Academic Paper\",\n",
        " \"language\": \"English\",\n",
        " \"length\": \"Short\",\n",
        " \"source\": \"Wikipedia\"\n",
        "}\n",
        "\n",
        "2️⃣ Query: \"price of a bushel of wheat\"\n",
        "Document: \"Interactive chart of historical daily wheat prices...\"\n",
        "Attributes:\n",
        "{\n",
        " \"audience\": \"Researcher\",\n",
        " \"keyword\": [\"Economics\", \"Agriculture\"],\n",
        " \"format\": \"Report\",\n",
        " \"language\": \"English\",\n",
        " \"length\": \"Short\",\n",
        " \"source\": \"NewsSite\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# 🔹 Attribute generation function\n",
        "def get_attributes(query, document, retries=2):\n",
        "    prompt = f\"\"\"\n",
        "You are labeling information retrieval data using InfoSearch-style attributes.\n",
        "\n",
        "{FEWSHOT}\n",
        "\n",
        "Now label this new pair.\n",
        "Return ONLY a valid JSON dictionary (no explanations, no markdown).\n",
        "\n",
        "Query: {query}\n",
        "Document: {document[:800]}\n",
        "JSON:\n",
        "\"\"\"\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=deployment,\n",
        "                temperature=0.4,\n",
        "                max_tokens=250,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "            # Clean and try parsing JSON\n",
        "            start = content.find(\"{\")\n",
        "            end = content.rfind(\"}\") + 1\n",
        "            json_part = content[start:end]\n",
        "            attrs = json.loads(json_part)\n",
        "            # Ensure all expected keys exist\n",
        "            required = {\"audience\", \"keyword\", \"format\", \"language\", \"length\", \"source\"}\n",
        "            if required.issubset(attrs.keys()):\n",
        "                return attrs\n",
        "        except Exception as e:\n",
        "            time.sleep(1)\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "# 🔹 Process all samples\n",
        "enriched = []\n",
        "for i, item in enumerate(tqdm(base_data, desc=\"Annotating\")):\n",
        "    attrs = get_attributes(item[\"query\"], item[\"document\"])\n",
        "    if attrs:\n",
        "        item.update(attrs)\n",
        "        enriched.append(item)\n",
        "\n",
        "# 🔹 Save\n",
        "with open(\"multi_attr_dataset.jsonl\", \"w\", encoding=\"utf8\") as f:\n",
        "    for e in enriched:\n",
        "        f.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"✅ Created multi_attr_dataset.jsonl with {len(enriched)} labeled pairs.\")\n"
      ],
      "metadata": {
        "id": "GjsZ5oJ8D64C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Attribute Instructed + Reversed Query Generation**"
      ],
      "metadata": {
        "id": "8ZYDYTL51L1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Install required packages ---\n",
        "!pip install openai aiohttp nest_asyncio -q\n",
        "\n",
        "#2. Imports and setup ---\n",
        "import json, re, time, asyncio, nest_asyncio\n",
        "from openai import AsyncAzureOpenAI\n",
        "nest_asyncio.apply()\n",
        "\n",
        "#3. Azure OpenAI configuration ---\n",
        "API_KEY = \"#hidden\"\n",
        "ENDPOINT = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "API_VERSION = \"2024-12-01-preview\"\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "client = AsyncAzureOpenAI(\n",
        "    api_key=API_KEY,\n",
        "    azure_endpoint=ENDPOINT,\n",
        "    api_version=API_VERSION\n",
        ")\n",
        "\n",
        "#4. Rate-limit and save settings ---\n",
        "TOKENS_PER_MINUTE = 100_000\n",
        "REQUESTS_PER_MINUTE = 1000\n",
        "MAX_CONCURRENT_REQUESTS = 8\n",
        "SLEEP_BETWEEN_REQUESTS = 60 / REQUESTS_PER_MINUTE\n",
        "SAVE_INTERVAL = 50   # Auto-save every 50 queries\n",
        "\n",
        "def build_prompt(query, attributes):\n",
        "    \"\"\"Prompt asking GPT to return instructed and reversed versions as strict JSON.\"\"\"\n",
        "    attr_text = \", \".join([f\"{k}: {v}\" for k, v in attributes.items() if v])\n",
        "    return f\"\"\"\n",
        "You are generating search queries with multiple document-level attributes.\n",
        "\n",
        "Given a base query and its attributes, produce:\n",
        "1. An instructed version that naturally includes 2–3 attributes.\n",
        "2. A reversed instructed version that logically negates those same attributes.\n",
        "\n",
        "Return output ONLY as a JSON object with two keys:\n",
        "\"instructed_query\" and \"reversed_query\".\n",
        "\n",
        "Example:\n",
        "Base query: \"best travel destinations in Europe\"\n",
        "Attributes: format=blog, language=English, audience=layman\n",
        "\n",
        "Output:\n",
        "{{\n",
        "  \"instructed_query\": \"List the best travel destinations in Europe. Please provide a blog in English for laymen.\",\n",
        "  \"reversed_query\": \"List the best travel destinations in Europe. Please do not provide a blog in English for laymen.\"\n",
        "}}\n",
        "\n",
        "Now for this:\n",
        "Base query: \"{query}\"\n",
        "Attributes: {attr_text}\n",
        "\"\"\"\n",
        "\n",
        "def safe_json_parse(text):\n",
        "    \"\"\"Try robust JSON parsing; fallback to regex extraction if needed.\"\"\"\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        instructed = re.search(r'\"?instructed[_ ]?query\"?[:\\-]?\\s*[\"“](.*?)[\"”]', text, re.I | re.S)\n",
        "        reversed_q = re.search(r'\"?reversed[_ ]?query\"?[:\\-]?\\s*[\"“](.*?)[\"”]', text, re.I | re.S)\n",
        "        return {\n",
        "            \"instructed_query\": instructed.group(1).strip() if instructed else \"\",\n",
        "            \"reversed_query\": reversed_q.group(1).strip() if reversed_q else \"\"\n",
        "        }\n",
        "\n",
        "# Async GPT-4 call for one entry\n",
        "\n",
        "async def process_entry(entry):\n",
        "    query = entry[\"query\"]\n",
        "    attributes = {\n",
        "        \"audience\": entry.get(\"audience\", \"\"),\n",
        "        \"format\": entry.get(\"format\", \"\"),\n",
        "        \"language\": entry.get(\"language\", \"\"),\n",
        "        \"length\": entry.get(\"length\", \"\"),\n",
        "        \"source\": entry.get(\"source\", \"\")\n",
        "    }\n",
        "\n",
        "    prompt = build_prompt(query, attributes)\n",
        "\n",
        "    try:\n",
        "        response = await client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            temperature=0.4,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            response_format={\"type\": \"json_object\"}  # force JSON output\n",
        "        )\n",
        "\n",
        "        content = response.choices[0].message.content.strip()\n",
        "        parsed = safe_json_parse(content)\n",
        "\n",
        "        entry[\"instructed_query\"] = parsed.get(\"instructed_query\", \"\")\n",
        "        entry[\"reversed_query\"] = parsed.get(\"reversed_query\", \"\")\n",
        "        return entry\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error on {entry.get('query_id','?')}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Async batch generator\n",
        "\n",
        "async def generate_instructed_queries():\n",
        "    input_file = \"multi_attr_dataset.jsonl\"\n",
        "    output_file = \"multi_attr_instructed.jsonl\"\n",
        "\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "\n",
        "    print(f\"📘 Loaded {len(data)} entries from {input_file}\")\n",
        "\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "    processed = []\n",
        "\n",
        "    async def limited_process(entry):\n",
        "        async with semaphore:\n",
        "            result = await process_entry(entry)\n",
        "            await asyncio.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "            return result\n",
        "\n",
        "    tasks = [limited_process(e) for e in data]\n",
        "\n",
        "    for i, coro in enumerate(asyncio.as_completed(tasks), 1):\n",
        "        result = await coro\n",
        "        if result:\n",
        "            processed.append(result)\n",
        "\n",
        "        # Auto-save every N records\n",
        "        if i % SAVE_INTERVAL == 0:\n",
        "            with open(output_file, \"a\") as f:\n",
        "                for p in processed[-SAVE_INTERVAL:]:\n",
        "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "            print(f\"💾 Auto-saved {i} entries...\")\n",
        "\n",
        "    # Final save\n",
        "    with open(output_file, \"a\") as f:\n",
        "        for p in processed:\n",
        "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"✅ Finished generating {len(processed)} instructed queries!\")\n",
        "    print(f\"📁 Output: {output_file}\")\n",
        "\n",
        "\n",
        "await generate_instructed_queries()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrZNUCkXM6AJ",
        "outputId": "68226c6d-1b5d-4268-f2e5-4a2b2ea3cb2f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Loaded 715 entries from multi_attr_dataset.jsonl\n",
            "💾 Auto-saved 50 entries...\n",
            "💾 Auto-saved 100 entries...\n",
            "💾 Auto-saved 150 entries...\n",
            "💾 Auto-saved 200 entries...\n",
            "💾 Auto-saved 250 entries...\n",
            "💾 Auto-saved 300 entries...\n",
            "💾 Auto-saved 350 entries...\n",
            "💾 Auto-saved 400 entries...\n",
            "💾 Auto-saved 450 entries...\n",
            "💾 Auto-saved 500 entries...\n",
            "💾 Auto-saved 550 entries...\n",
            "💾 Auto-saved 600 entries...\n",
            "💾 Auto-saved 650 entries...\n",
            "💾 Auto-saved 700 entries...\n",
            "✅ Finished generating 715 instructed queries!\n",
            "📁 Output: multi_attr_instructed.jsonl\n"
          ]
        }
      ]
    }
  ]
}