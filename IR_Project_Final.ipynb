{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyrWGRG2EbNM"
      },
      "source": [
        "# **0. Azure Open AI Endpoint and Key:  GPT - 4o - Mini Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c3Eb040YEYlM",
        "outputId": "3edefec9-e240-49b0-e511-19fe9bf567e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paris is a vibrant city filled with iconic landmarks, cultural experiences, and picturesque neighborhoods. Hereâ€™s a list of must-see attractions and activities during your visit:\n",
            "\n",
            "### Iconic Landmarks\n",
            "1. **Eiffel Tower**: Visit this iconic symbol of Paris, and donâ€™t forget to go up for stunning views of the city.\n",
            "2. **Louvre Museum**: Home to thousands of artworks, including the Mona Lisa and the Venus de Milo.\n",
            "3. **Notre-Dame Cathedral**: Although under restoration, the facade is still impressive; you can also explore the Ãle de la CitÃ©.\n",
            "4. **SacrÃ©-CÅ“ur Basilica**: Located on Montmartre Hill, enjoy the beautiful architecture and panoramic views. \n",
            "5. **Arc de Triomphe**: Climb to the top for a great view of the Champs-Ã‰lysÃ©es.\n",
            "\n",
            "### Museums and Galleries\n",
            "6. **MusÃ©e d'Orsay**: Famous for its collection of Impressionist and Post-Impressionist masterpieces.\n",
            "7. **Centre Pompidou**: A modern art museum with a striking architectural design.\n",
            "8. **MusÃ©e de l'Orangerie**: Known for Monet's Water Lilies and other Impressionist works.\n",
            "\n",
            "### Historic Neighborhoods\n",
            "9. **Montmartre**: Wander the charming streets, visit artistsâ€™ studios, and enjoy the bohemian atmosphere.\n",
            "10. **Le Marais**: A lively district full of boutiques, cafes, and historic sites like the Place des Vosges.\n",
            "11. **Latin Quarter**: Known for its lively atmosphere, great dining, and the historic Sorbonne University.\n",
            "\n",
            "### Parks and Gardens\n",
            "12. **Luxembourg Gardens**: A beautiful park perfect for a leisurely stroll or a picnic.\n",
            "13. **Tuileries Garden**: Located between the Louvre and Place de la Concorde, great for a relaxing walk.\n",
            "\n",
            "### Culinary Experiences\n",
            "14. **Try French cuisine**: Enjoy traditional dishes in bistros, cafÃ©s, and patisseries. Donâ€™t miss pastries such as croissants and macarons.\n",
            "15. **Visit local markets**: Check out markets like MarchÃ© Bastille or MarchÃ© des Enfants Rouges for local food.\n",
            "\n",
            "### Unique Experiences\n",
            "16. **Seine River Cruise**: Take a boat cruise for a unique perspective of Parisâ€™ landmarks.\n",
            "17. **Explore the Catacombs**: Visit the underground ossuary filled with the bones of millions.\n",
            "18. **Attend a cabaret show**: Experience the vibrant nightlife at places like Moulin Rouge.\n",
            "\n",
            "### Day Trips\n",
            "19. **Versailles**: Visit the opulent palace and its vast gardens, just a short train ride from Paris.\n",
            "20. **Giverny**: Explore Monetâ€™s garden and home, a beautiful spot for art enthusiasts.\n",
            "\n",
            "### Tips\n",
            "- **Enjoy Parisian cafÃ©s**: Relax at a sidewalk cafÃ© and soak in the ambiance.\n",
            "- **Use public transport**: The Metro is an efficient way to get around.\n",
            "- **Consider purchasing a Paris Pass**: If you plan to visit multiple attractions, it may save you money.\n",
            "\n",
            "Remember to take your time and enjoy the city at your own pace, as Paris has something beautiful and unique around every corner!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "endpoint = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "model_name = \"gpt-4o-mini\"\n",
        "deployment = \"gpt-4o-mini\"\n",
        "\n",
        "subscription_key = \"#hidden\"\n",
        "api_version = \"2024-12-01-preview\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"I am going to Paris, what should I see?\",\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=4096,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    model=deployment\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h-1OjTk5DOZr",
        "outputId": "919f0fbc-68b9-4be3-ede4-3ad75cb7457b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ir_datasets\n",
            "  Downloading ir_datasets-0.5.11-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (4.13.5)\n",
            "Collecting inscriptis>=2.2.0 (from ir_datasets)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (2.32.4)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir_datasets)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting lz4>=3.1.10 (from ir_datasets)\n",
            "  Downloading lz4-4.4.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting warc3-wet>=0.2.3 (from ir_datasets)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir_datasets)\n",
            "  Downloading zlib_state-0.1.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting ijson>=3.1.3 (from ir_datasets)\n",
            "  Downloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir_datasets)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.12/dist-packages (from ir_datasets) (18.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir_datasets) (2025.10.5)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading ir_datasets-0.5.11-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m866.1/866.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (149 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.10-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=a094d29e64712eb3131aa5c158a23bc05281cf8f424e5361e012616ad917ffe5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/85/c2/9f0f621def52a1d5db7d29984f81e45f9fb6dfeb1a4eb6e31c\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp312-cp312-linux_x86_64.whl size=55019 sha256=3cf082d14422820030b7e4f7ca77f1f72b16594b64ab19b5bfe6135223d4e916\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/3e/21/a739cbcc331a1ab45c326d6edbdac6118de4402f6076e30ff1\n",
            "Successfully built warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, cbor, zlib-state, unlzw3, trec-car-tools, lz4, ijson, inscriptis, ir_datasets\n",
            "Successfully installed cbor-1.0.0 ijson-3.4.0.post0 inscriptis-2.6.0 ir_datasets-0.5.11 lz4-4.4.5 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.10\n"
          ]
        }
      ],
      "source": [
        "pip install ir_datasets tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKWvOClDtHl"
      },
      "source": [
        "# **1. Collecting Base Query Documents from msmarco and beir**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r32D9TZDpoW"
      },
      "outputs": [],
      "source": [
        "\n",
        "import ir_datasets, random, json\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "SAMPLES = {\n",
        "    \"msmarco-passage/train\": 400,            # open-domain search\n",
        "    \"beir/cqadupstack/programmers\": 100,     # StackOverflow-style Q&A\n",
        "    \"beir/fever\": 100,                       # fact verification\n",
        "    \"beir/scidocs\": 50,                      # academic domain\n",
        "    \"beir/quora\": 50,                        # question paraphrasing\n",
        "}\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def sample_dataset(name, sample_size):\n",
        "    print(f\"\\nğŸ“˜ Processing {name}\")\n",
        "    ds = ir_datasets.load(name)\n",
        "\n",
        "    for needed in [\"queries_iter\", \"docs_iter\", \"qrels_iter\"]:\n",
        "        if not hasattr(ds, needed):\n",
        "            raise AttributeError(f\"Dataset {name} missing: {needed}\")\n",
        "\n",
        "    queries = {q.query_id: q.text for q in ds.queries_iter()}\n",
        "    docs = {d.doc_id: d.text for d in ds.docs_iter()}\n",
        "    qrels = list(ds.qrels_iter())\n",
        "\n",
        "    # mapping query â†’ list of relevant doc IDs\n",
        "    pos_map = {}\n",
        "    for q in qrels:\n",
        "        if q.relevance > 0:\n",
        "            pos_map.setdefault(q.query_id, []).append(q.doc_id)\n",
        "\n",
        "    valid_qids = list(pos_map.keys())\n",
        "    if not valid_qids:\n",
        "        print(f\"âš ï¸ No valid qrels for {name}\")\n",
        "        return []\n",
        "\n",
        "    chosen_qids = random.sample(valid_qids, min(sample_size, len(valid_qids)))\n",
        "    results = []\n",
        "\n",
        "    for qid in tqdm(chosen_qids):\n",
        "        query = queries.get(qid, \"\")\n",
        "        pos_docs = pos_map.get(qid, [])\n",
        "        for did in pos_docs[:3]:\n",
        "            if did in docs:\n",
        "                results.append({\n",
        "                    \"dataset\": name,\n",
        "                    \"query_id\": qid,\n",
        "                    \"query\": query,\n",
        "                    \"document\": docs[did],\n",
        "                    \"relevance\": 1\n",
        "                })\n",
        "\n",
        "    print(f\"âœ… Collected {len(results)} pairs from {name}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "all_data = []\n",
        "for name, n in SAMPLES.items():\n",
        "    try:\n",
        "        all_data.extend(sample_dataset(name, n))\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Skipping {name}: {e}\")\n",
        "\n",
        "\n",
        "with open(\"1_base_dataset_questq.jsonl\", \"w\", encoding=\"utf8\") as f:\n",
        "    for item in all_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"\\n Done! Collected {len(all_data)} total queryâ€“doc pairs.\")\n",
        "print(\"Output saved to base_dataset_questq.jsonl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNb3qPqOD7yc"
      },
      "source": [
        "#**2. Adding Attributes to Base Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjsZ5oJ8D64C"
      },
      "outputs": [],
      "source": [
        "import json, time, random\n",
        "from tqdm import tqdm\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "endpoint = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "api_version = \"2024-12-01-preview\"\n",
        "deployment = \"gpt-4o-mini\"\n",
        "api_key = \"#hidden\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "\n",
        "with open(\"1_base_dataset_questq.jsonl\") as f:\n",
        "    base_data = [json.loads(l) for l in f]\n",
        "\n",
        "FEWSHOT = \"\"\"\n",
        "Examples:\n",
        "1ï¸âƒ£ Query: \"what is the purpose of DNA replication\"\n",
        "Document: \"DNA replication ensures each cell gets an exact copy of the DNA during cell division.\"\n",
        "Attributes:\n",
        "{\n",
        " \"audience\": \"Student\",\n",
        " \"keyword\": [\"Biology\", \"Genetics\"],\n",
        " \"format\": \"Academic Paper\",\n",
        " \"language\": \"English\",\n",
        " \"length\": \"Short\",\n",
        " \"source\": \"Wikipedia\"\n",
        "}\n",
        "\n",
        "2ï¸âƒ£ Query: \"price of a bushel of wheat\"\n",
        "Document: \"Interactive chart of historical daily wheat prices...\"\n",
        "Attributes:\n",
        "{\n",
        " \"audience\": \"Researcher\",\n",
        " \"keyword\": [\"Economics\", \"Agriculture\"],\n",
        " \"format\": \"Report\",\n",
        " \"language\": \"English\",\n",
        " \"length\": \"Short\",\n",
        " \"source\": \"NewsSite\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Attribute generation\n",
        "def get_attributes(query, document, retries=2):\n",
        "    prompt = f\"\"\"\n",
        "You are labeling information retrieval data using InfoSearch-style attributes.\n",
        "\n",
        "{FEWSHOT}\n",
        "\n",
        "Now label this new pair.\n",
        "Return ONLY a valid JSON dictionary (no explanations, no markdown).\n",
        "\n",
        "Query: {query}\n",
        "Document: {document[:800]}\n",
        "JSON:\n",
        "\"\"\"\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=deployment,\n",
        "                temperature=0.4,\n",
        "                max_tokens=250,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "            )\n",
        "            content = resp.choices[0].message.content.strip()\n",
        "\n",
        "            start = content.find(\"{\")\n",
        "            end = content.rfind(\"}\") + 1\n",
        "            json_part = content[start:end]\n",
        "            attrs = json.loads(json_part)\n",
        "\n",
        "            required = {\"audience\", \"keyword\", \"format\", \"language\", \"length\", \"source\"}\n",
        "            if required.issubset(attrs.keys()):\n",
        "                return attrs\n",
        "        except Exception as e:\n",
        "            time.sleep(1)\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "\n",
        "enriched = []\n",
        "for i, item in enumerate(tqdm(base_data, desc=\"Annotating\")):\n",
        "    attrs = get_attributes(item[\"query\"], item[\"document\"])\n",
        "    if attrs:\n",
        "        item.update(attrs)\n",
        "        enriched.append(item)\n",
        "\n",
        "with open(\"2_base_with_multi_attri.jsonl\", \"w\", encoding=\"utf8\") as f:\n",
        "    for e in enriched:\n",
        "        f.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\" Created multi_attr_dataset.jsonl with {len(enriched)} labeled pairs.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBGWsMbvtTbm"
      },
      "source": [
        "# **3. Multi-Attribute Instructed + Reversed Query Generation + Document Rewriting + Hard Negative Generation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaXoOlf-Zfvh",
        "outputId": "caad315e-8a14-47d0-9729-9bd0b8f3d529",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“˜ Loaded 715 core queries\n",
            "ğŸ’¾ Auto-saved 50 combinations...\n",
            "ğŸ’¾ Auto-saved 100 combinations...\n",
            "ğŸ’¾ Auto-saved 150 combinations...\n",
            "ğŸ’¾ Auto-saved 200 combinations...\n",
            "ğŸ’¾ Auto-saved 250 combinations...\n",
            "ğŸ’¾ Auto-saved 300 combinations...\n",
            "ğŸ’¾ Auto-saved 350 combinations...\n",
            "ğŸ’¾ Auto-saved 400 combinations...\n",
            "ğŸ’¾ Auto-saved 450 combinations...\n",
            "ğŸ’¾ Auto-saved 500 combinations...\n",
            "ğŸ’¾ Auto-saved 550 combinations...\n",
            "ğŸ’¾ Auto-saved 600 combinations...\n",
            "ğŸ’¾ Auto-saved 650 combinations...\n",
            "ğŸ’¾ Auto-saved 700 combinations...\n",
            "ğŸ’¾ Auto-saved 750 combinations...\n",
            "ğŸ’¾ Auto-saved 800 combinations...\n",
            "ğŸ’¾ Auto-saved 850 combinations...\n",
            "ğŸ’¾ Auto-saved 900 combinations...\n",
            "ğŸ’¾ Auto-saved 950 combinations...\n",
            "ğŸ’¾ Auto-saved 1000 combinations...\n",
            "ğŸ’¾ Auto-saved 1050 combinations...\n",
            "ğŸ’¾ Auto-saved 1100 combinations...\n",
            "ğŸ’¾ Auto-saved 1150 combinations...\n",
            "ğŸ’¾ Auto-saved 1200 combinations...\n",
            "ğŸ’¾ Auto-saved 1250 combinations...\n",
            "ğŸ’¾ Auto-saved 1300 combinations...\n",
            "ğŸ’¾ Auto-saved 1350 combinations...\n",
            "ğŸ’¾ Auto-saved 1400 combinations...\n",
            "ğŸ’¾ Auto-saved 1450 combinations...\n",
            "ğŸ’¾ Auto-saved 1500 combinations...\n",
            "ğŸ’¾ Auto-saved 1550 combinations...\n",
            "ğŸ’¾ Auto-saved 1600 combinations...\n",
            "ğŸ’¾ Auto-saved 1650 combinations...\n",
            "ğŸ’¾ Auto-saved 1700 combinations...\n",
            "ğŸ’¾ Auto-saved 1750 combinations...\n",
            "ğŸ’¾ Auto-saved 1800 combinations...\n",
            "ğŸ’¾ Auto-saved 1850 combinations...\n",
            "ğŸ’¾ Auto-saved 1900 combinations...\n",
            "ğŸ’¾ Auto-saved 1950 combinations...\n",
            "ğŸ’¾ Auto-saved 2000 combinations...\n",
            "ğŸ’¾ Auto-saved 2050 combinations...\n",
            "âŒ Error on 362662: expected string or bytes-like object, got 'NoneType'\n",
            "ğŸ’¾ Auto-saved 2100 combinations...\n",
            "ğŸ’¾ Auto-saved 2150 combinations...\n",
            "ğŸ’¾ Auto-saved 2200 combinations...\n",
            "ğŸ’¾ Auto-saved 2250 combinations...\n",
            "ğŸ’¾ Auto-saved 2300 combinations...\n",
            "ğŸ’¾ Auto-saved 2350 combinations...\n",
            "ğŸ’¾ Auto-saved 2400 combinations...\n",
            "ğŸ’¾ Auto-saved 2450 combinations...\n",
            "ğŸ’¾ Auto-saved 2500 combinations...\n",
            "ğŸ’¾ Auto-saved 2550 combinations...\n",
            "ğŸ’¾ Auto-saved 2600 combinations...\n",
            "ğŸ’¾ Auto-saved 2650 combinations...\n",
            "ğŸ’¾ Auto-saved 2700 combinations...\n",
            "ğŸ’¾ Auto-saved 2750 combinations...\n",
            "ğŸ’¾ Auto-saved 2800 combinations...\n",
            "âœ… Generated 2819 instructed + reversed combinations!\n",
            "ğŸ’¾ Saved 50 rewritten docs...\n",
            "ğŸ’¾ Saved 100 rewritten docs...\n",
            "ğŸ’¾ Saved 150 rewritten docs...\n",
            "ğŸ’¾ Saved 200 rewritten docs...\n",
            "ğŸ’¾ Saved 250 rewritten docs...\n",
            "ğŸ’¾ Saved 300 rewritten docs...\n",
            "ğŸ’¾ Saved 350 rewritten docs...\n",
            "ğŸ’¾ Saved 400 rewritten docs...\n",
            "ğŸ’¾ Saved 450 rewritten docs...\n",
            "ğŸ’¾ Saved 500 rewritten docs...\n",
            "ğŸ’¾ Saved 550 rewritten docs...\n",
            "ğŸ’¾ Saved 600 rewritten docs...\n",
            "ğŸ’¾ Saved 650 rewritten docs...\n",
            "ğŸ’¾ Saved 700 rewritten docs...\n",
            "ğŸ’¾ Saved 750 rewritten docs...\n",
            "ğŸ’¾ Saved 800 rewritten docs...\n",
            "ğŸ’¾ Saved 850 rewritten docs...\n",
            "ğŸ’¾ Saved 900 rewritten docs...\n",
            "ğŸ’¾ Saved 950 rewritten docs...\n",
            "ğŸ’¾ Saved 1000 rewritten docs...\n",
            "ğŸ’¾ Saved 1050 rewritten docs...\n",
            "ğŸ’¾ Saved 1100 rewritten docs...\n",
            "ğŸ’¾ Saved 1150 rewritten docs...\n",
            "ğŸ’¾ Saved 1200 rewritten docs...\n",
            "ğŸ’¾ Saved 1250 rewritten docs...\n",
            "ğŸ’¾ Saved 1300 rewritten docs...\n",
            "ğŸ’¾ Saved 1350 rewritten docs...\n",
            "ğŸ’¾ Saved 1400 rewritten docs...\n",
            "ğŸ’¾ Saved 1450 rewritten docs...\n",
            "ğŸ’¾ Saved 1500 rewritten docs...\n",
            "ğŸ’¾ Saved 1550 rewritten docs...\n",
            "ğŸ’¾ Saved 1600 rewritten docs...\n",
            "ğŸ’¾ Saved 1650 rewritten docs...\n",
            "ğŸ’¾ Saved 1700 rewritten docs...\n",
            "ğŸ’¾ Saved 1750 rewritten docs...\n",
            "ğŸ’¾ Saved 1800 rewritten docs...\n",
            "ğŸ’¾ Saved 1850 rewritten docs...\n",
            "ğŸ’¾ Saved 1900 rewritten docs...\n",
            "ğŸ’¾ Saved 1950 rewritten docs...\n",
            "ğŸ’¾ Saved 2000 rewritten docs...\n",
            "ğŸ’¾ Saved 2050 rewritten docs...\n",
            "ğŸ’¾ Saved 2100 rewritten docs...\n",
            "ğŸ’¾ Saved 2150 rewritten docs...\n",
            "ğŸ’¾ Saved 2200 rewritten docs...\n",
            "ğŸ’¾ Saved 2250 rewritten docs...\n",
            "âŒ Error on query_id=362662: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
            "âŒ Error on query_id=362662: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
            "ğŸ’¾ Saved 2300 rewritten docs...\n",
            "ğŸ’¾ Saved 2350 rewritten docs...\n",
            "ğŸ’¾ Saved 2400 rewritten docs...\n",
            "ğŸ’¾ Saved 2450 rewritten docs...\n",
            "ğŸ’¾ Saved 2500 rewritten docs...\n",
            "ğŸ’¾ Saved 2550 rewritten docs...\n",
            "ğŸ’¾ Saved 2600 rewritten docs...\n",
            "ğŸ’¾ Saved 2650 rewritten docs...\n",
            "ğŸ’¾ Saved 2700 rewritten docs...\n",
            "ğŸ’¾ Saved 2750 rewritten docs...\n",
            "ğŸ’¾ Saved 2800 rewritten docs...\n",
            "ğŸ’¾ Saved 2850 rewritten docs...\n",
            "ğŸ’¾ Saved 2900 rewritten docs...\n",
            "ğŸ’¾ Saved 2950 rewritten docs...\n",
            "ğŸ’¾ Saved 3000 rewritten docs...\n",
            "ğŸ’¾ Saved 3050 rewritten docs...\n",
            "ğŸ’¾ Saved 3100 rewritten docs...\n",
            "ğŸ’¾ Saved 3150 rewritten docs...\n",
            "ğŸ’¾ Saved 3200 rewritten docs...\n",
            "ğŸ’¾ Saved 3250 rewritten docs...\n",
            "ğŸ’¾ Saved 3300 rewritten docs...\n",
            "ğŸ’¾ Saved 3350 rewritten docs...\n",
            "ğŸ’¾ Saved 3400 rewritten docs...\n",
            "ğŸ’¾ Saved 3450 rewritten docs...\n",
            "ğŸ’¾ Saved 3500 rewritten docs...\n",
            "ğŸ’¾ Saved 3550 rewritten docs...\n",
            "ğŸ’¾ Saved 3600 rewritten docs...\n",
            "ğŸ’¾ Saved 3650 rewritten docs...\n",
            "ğŸ’¾ Saved 3700 rewritten docs...\n",
            "ğŸ’¾ Saved 3750 rewritten docs...\n",
            "ğŸ’¾ Saved 3800 rewritten docs...\n",
            "ğŸ’¾ Saved 3850 rewritten docs...\n",
            "ğŸ’¾ Saved 3900 rewritten docs...\n",
            "ğŸ’¾ Saved 3950 rewritten docs...\n",
            "âŒ Error on query_id=362662: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
            "ğŸ’¾ Saved 4000 rewritten docs...\n",
            "ğŸ’¾ Saved 4050 rewritten docs...\n",
            "ğŸ’¾ Saved 4100 rewritten docs...\n",
            "ğŸ’¾ Saved 4150 rewritten docs...\n",
            "ğŸ’¾ Saved 4200 rewritten docs...\n",
            "ğŸ’¾ Saved 4250 rewritten docs...\n",
            "ğŸ’¾ Saved 4300 rewritten docs...\n",
            "ğŸ’¾ Saved 4350 rewritten docs...\n",
            "ğŸ’¾ Saved 4400 rewritten docs...\n",
            "ğŸ’¾ Saved 4450 rewritten docs...\n",
            "ğŸ’¾ Saved 4500 rewritten docs...\n",
            "ğŸ’¾ Saved 4550 rewritten docs...\n",
            "ğŸ’¾ Saved 4600 rewritten docs...\n",
            "ğŸ’¾ Saved 4650 rewritten docs...\n",
            "ğŸ’¾ Saved 4700 rewritten docs...\n",
            "ğŸ’¾ Saved 4750 rewritten docs...\n",
            "ğŸ’¾ Saved 4800 rewritten docs...\n",
            "ğŸ’¾ Saved 4850 rewritten docs...\n",
            "ğŸ’¾ Saved 4900 rewritten docs...\n",
            "ğŸ’¾ Saved 4950 rewritten docs...\n",
            "ğŸ’¾ Saved 5000 rewritten docs...\n",
            "ğŸ’¾ Saved 5050 rewritten docs...\n",
            "ğŸ’¾ Saved 5100 rewritten docs...\n",
            "ğŸ’¾ Saved 5150 rewritten docs...\n",
            "ğŸ’¾ Saved 5200 rewritten docs...\n",
            "ğŸ’¾ Saved 5250 rewritten docs...\n",
            "ğŸ’¾ Saved 5300 rewritten docs...\n",
            "ğŸ’¾ Saved 5350 rewritten docs...\n",
            "ğŸ’¾ Saved 5400 rewritten docs...\n",
            "ğŸ’¾ Saved 5450 rewritten docs...\n",
            "ğŸ’¾ Saved 5500 rewritten docs...\n",
            "ğŸ’¾ Saved 5550 rewritten docs...\n",
            "ğŸ’¾ Saved 5600 rewritten docs...\n",
            "âœ… Finished rewriting 5616 documents!\n"
          ]
        }
      ],
      "source": [
        "!pip install openai aiohttp nest_asyncio -q\n",
        "\n",
        "import json, re, time, asyncio, nest_asyncio, random, copy\n",
        "from openai import AsyncAzureOpenAI\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "API_KEY = \"#hidden\"\n",
        "ENDPOINT = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "API_VERSION = \"2024-12-01-preview\"\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "client = AsyncAzureOpenAI(\n",
        "    api_key=API_KEY,\n",
        "    azure_endpoint=ENDPOINT,\n",
        "    api_version=API_VERSION\n",
        ")\n",
        "\n",
        "\n",
        "# Rate Control for Token Usage\n",
        "MAX_CONCURRENT_REQUESTS = 6\n",
        "REQUESTS_PER_MINUTE = 1000\n",
        "SLEEP_BETWEEN_REQUESTS = 60 / REQUESTS_PER_MINUTE\n",
        "SAVE_INTERVAL = 50\n",
        "\n",
        "\n",
        "def safe_json_parse(text):\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception:\n",
        "        instructed = re.search(r'\"?instructed[_ ]?query\"?[:\\-]?\\s*[\"â€œ](.*?)[\"â€]', text, re.I | re.S)\n",
        "        reversed_q = re.search(r'\"?reversed[_ ]?query\"?[:\\-]?\\s*[\"â€œ](.*?)[\"â€]', text, re.I | re.S)\n",
        "        pos = re.search(r'\"?positive[_ ]?doc\"?[:\\-]?\\s*[\"â€œ](.*?)[\"â€]', text, re.I | re.S)\n",
        "        neg = re.search(r'\"?hard[_ ]?negative[_ ]?doc\"?[:\\-]?\\s*[\"â€œ](.*?)[\"â€]', text, re.I | re.S)\n",
        "        return {\n",
        "            \"instructed_query\": instructed.group(1).strip() if instructed else \"\",\n",
        "            \"reversed_query\": reversed_q.group(1).strip() if reversed_q else \"\",\n",
        "            \"positive_doc\": pos.group(1).strip() if pos else \"\",\n",
        "            \"hard_negative_doc\": neg.group(1).strip() if neg else \"\"\n",
        "        }\n",
        "\n",
        "def build_instruct_prompt(query, attributes):\n",
        "    attr_text = \", \".join([f\"{k}: {v}\" for k, v in attributes.items() if v])\n",
        "    return f\"\"\"\n",
        "You are generating search queries with multiple document-level attributes.\n",
        "\n",
        "Given a base query and its attributes, produce:\n",
        "1. An instructed version that naturally includes all given attributes.\n",
        "2. A reversed instructed version that logically negates them.\n",
        "\n",
        "Return a JSON object:\n",
        "{{\n",
        "  \"instructed_query\": \"...\",\n",
        "  \"reversed_query\": \"...\"\n",
        "}}\n",
        "\n",
        "Base query: \"{query}\"\n",
        "Attributes: {attr_text}\n",
        "\"\"\"\n",
        "\n",
        "def build_doc_prompt(document, attributes):\n",
        "    attr_text = \", \".join([f\"{k}: {v}\" for k, v in attributes.items() if v])\n",
        "    return f\"\"\"\n",
        "You are refining a document for an information-retrieval benchmark.\n",
        "\n",
        "Task 1 â€“ Rewrite the base document so that it **fully satisfies all given attributes**.\n",
        "Task 2 â€“ Create one **hard negative document** that is **topically similar** but violates one or two attributes.\n",
        "\n",
        "Return only JSON:\n",
        "{{\n",
        "  \"positive_doc\": \"... rewritten version ...\",\n",
        "  \"hard_negative_doc\": \"... violating version ...\"\n",
        "}}\n",
        "\n",
        "Base document:\n",
        "\\\"\\\"\\\"{document}\\\"\\\"\\\"\n",
        "Attributes: {attr_text}\n",
        "\"\"\"\n",
        "\n",
        "def sample_attr_combinations(entry, num_combinations=5):\n",
        "    all_attrs = {\n",
        "        \"audience\": entry.get(\"audience\", \"\"),\n",
        "        \"format\": entry.get(\"format\", \"\"),\n",
        "        \"language\": entry.get(\"language\", \"\"),\n",
        "        \"length\": entry.get(\"length\", \"\"),\n",
        "        \"source\": entry.get(\"source\", \"\")\n",
        "    }\n",
        "    non_empty = {k: v for k, v in all_attrs.items() if v}\n",
        "    num_combinations = random.randint(3, 5)\n",
        "\n",
        "    combos = []\n",
        "    for _ in range(num_combinations):\n",
        "        chosen = random.sample(list(non_empty.keys()), k=min(random.randint(2, 3), len(non_empty)))\n",
        "        combos.append({k: non_empty[k] for k in chosen})\n",
        "    return combos\n",
        "\n",
        "#Async Generation\n",
        "async def generate_instructed_queries():\n",
        "    input_file = \"2_base_with_multi_attri.jsonl\"\n",
        "    output_file = \"3_instructed_reverse_queries_with_attri.jsonl\"\n",
        "\n",
        "    with open(input_file, \"r\") as f:\n",
        "        base_data = [json.loads(line) for line in f]\n",
        "\n",
        "    print(f\"ğŸ“˜ Loaded {len(base_data)} core queries\")\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "    processed = []\n",
        "\n",
        "    async def process_combination(base_entry, combo, combo_id):\n",
        "        new_entry = copy.deepcopy(base_entry)\n",
        "        new_entry[\"attributes\"] = combo\n",
        "        new_entry[\"combo_id\"] = combo_id\n",
        "\n",
        "        prompt = build_instruct_prompt(base_entry[\"query\"], combo)\n",
        "        try:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                temperature=0.4,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "            parsed = safe_json_parse(response.choices[0].message.content)\n",
        "            new_entry[\"instructed_query\"] = parsed.get(\"instructed_query\", \"\")\n",
        "            new_entry[\"reversed_query\"] = parsed.get(\"reversed_query\", \"\")\n",
        "            new_entry[\"query_type\"] = \"expanded\"\n",
        "            return new_entry\n",
        "        except Exception as e:\n",
        "            print(f\" Error on {base_entry.get('query_id')}: {e}\")\n",
        "            return None\n",
        "\n",
        "    tasks = []\n",
        "    for entry in base_data:\n",
        "        combos = sample_attr_combinations(entry)\n",
        "        for i, combo in enumerate(combos):\n",
        "            tasks.append(process_combination(entry, combo, i+1))\n",
        "\n",
        "    async def limited_task(task):\n",
        "        async with semaphore:\n",
        "            res = await task\n",
        "            await asyncio.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "            return res\n",
        "\n",
        "    wrapped_tasks = [limited_task(t) for t in tasks]\n",
        "    for i, coro in enumerate(asyncio.as_completed(wrapped_tasks), 1):\n",
        "        res = await coro\n",
        "        if res:\n",
        "            processed.append(res)\n",
        "        if i % SAVE_INTERVAL == 0:\n",
        "            with open(output_file, \"a\") as f:\n",
        "                for p in processed[-SAVE_INTERVAL:]:\n",
        "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "            print(f\" Auto-saved {i} combinations...\")\n",
        "\n",
        "    with open(output_file, \"a\") as f:\n",
        "        for p in processed:\n",
        "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\" Generated {len(processed)} instructed + reversed combinations!\")\n",
        "\n",
        "# Document Rewriting\n",
        "async def rewrite_documents():\n",
        "    input_file = \"3_instructed_reverse_queries_with_attri.jsonl\"\n",
        "    output_file = \"4_rewritten_docs_hard_negatives_with_attri.jsonl\"\n",
        "\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "    processed = []\n",
        "\n",
        "    async def process_doc(entry):\n",
        "        attributes = entry.get(\"attributes\", {})\n",
        "        prompt = build_doc_prompt(entry.get(\"document\", \"\"), attributes)\n",
        "        try:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                temperature=0.5,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "            parsed = safe_json_parse(response.choices[0].message.content)\n",
        "            violated = random.sample(\n",
        "                list(attributes.keys()), k=min(random.randint(1, 2), len(attributes))\n",
        "            ) if attributes else []\n",
        "            entry[\"positive_doc\"] = parsed.get(\"positive_doc\", \"\")\n",
        "            entry[\"hard_negative_doc\"] = parsed.get(\"hard_negative_doc\", \"\")\n",
        "            entry[\"violated_attributes\"] = violated\n",
        "            return entry\n",
        "        except Exception as e:\n",
        "            print(f\" Error on query_id={entry.get('query_id')}: {e}\")\n",
        "            return None\n",
        "\n",
        "    tasks = [process_doc(e) for e in data]\n",
        "    async def limited(entry_task):\n",
        "        async with semaphore:\n",
        "            res = await entry_task\n",
        "            await asyncio.sleep(SLEEP_BETWEEN_REQUESTS)\n",
        "            return res\n",
        "\n",
        "    wrapped_tasks = [limited(t) for t in tasks]\n",
        "    for i, coro in enumerate(asyncio.as_completed(wrapped_tasks), 1):\n",
        "        res = await coro\n",
        "        if res:\n",
        "            processed.append(res)\n",
        "        if i % SAVE_INTERVAL == 0:\n",
        "            with open(output_file, \"a\") as f:\n",
        "                for p in processed[-SAVE_INTERVAL:]:\n",
        "                    f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "            print(f\"ğŸ’¾ Saved {i} rewritten docs...\")\n",
        "\n",
        "    with open(output_file, \"a\") as f:\n",
        "        for p in processed:\n",
        "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\" Finished rewriting {len(processed)} documents!\")\n",
        "\n",
        "await generate_instructed_queries()\n",
        "await rewrite_documents()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XltoyGbDqlGR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "input_file = \"4_rewritten_docs_hard_negatives_with_attri.jsonl\"\n",
        "output_file = \"final_sorted.jsonl\"\n",
        "\n",
        "with open(input_file, \"r\") as f:\n",
        "    data = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "def sort_key(entry):\n",
        "    qid = str(entry.get(\"query_id\", \"\")).strip()\n",
        "\n",
        "    return (0, int(qid)) if qid.isdigit() else (1, qid.lower())\n",
        "\n",
        "data.sort(key=sort_key)\n",
        "\n",
        "with open(output_file, \"w\") as f:\n",
        "    for entry in data:\n",
        "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\" Sorted {len(data)} records by query_id.\")\n",
        "print(f\" Output saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJF-HTsDyGwm"
      },
      "source": [
        " # **4. Evaluation Process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pViowtWMyOMr",
        "outputId": "3db82263-ad9a-4364-aa22-c528741f444e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.8/178.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m367.5/367.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyserini (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-auth 2.38.0 requires cachetools<6.0,>=2.0.0, but you have cachetools 6.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pyserini tqdm numpy pandas -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9T2Sj8_mLgT"
      },
      "source": [
        "#**4.1 Sparse Retrieval Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfMXdozXI9CS"
      },
      "source": [
        "#BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a5PotpR8zjQU"
      },
      "outputs": [],
      "source": [
        "!pip install rank_bm25 tqdm numpy pandas -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNdolQysmlqX",
        "outputId": "396cb911-63b5-4bf5-b4d8-f5ef76c33171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "import json, numpy as np, pandas as pd, os\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# CONFIG\n",
        "DATA_QDOC = \"query-doc.json\"\n",
        "DATA_QUERIES = \"final_sorted.jsonl\"\n",
        "RESULTS_DIR = \"results_generic\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# METRIC FUNCTIONS\n",
        "\n",
        "def compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev):\n",
        "    if (Rins_rank < Rori_rank and Sins > Sori) and (Rrev_rank > Rori_rank and Srev < Sori):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, sat, viol, K=10, N=1):\n",
        "    try:\n",
        "        if Rins_rank <= Rori_rank < Rrev_rank:\n",
        "            if Rori_rank <= N and Rins_rank == 1:\n",
        "                f = 1.0\n",
        "            elif Rori_rank <= K:\n",
        "                f = (1 - np.sqrt((Rori_rank - Rins_rank) / K)) * (1 / np.sqrt(max(Rins_rank, 1)))\n",
        "            else:\n",
        "                f = 0.01\n",
        "            return f * (sat / m)\n",
        "        else:\n",
        "            if Rrev_rank < Rori_rank < Rins_rank:\n",
        "                p = -1.0\n",
        "            elif Rori_rank <= Rins_rank:\n",
        "                p = (Rori_rank - Rins_rank) / max(Rins_rank, 1)\n",
        "            elif Rrev_rank <= Rori_rank:\n",
        "                p = (Rrev_rank - Rori_rank) / max(Rori_rank, 1)\n",
        "            else:\n",
        "                p = -0.5\n",
        "            return p * (viol / m)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def compute_MDCR_bm25(query, top_docs, attributes, top_k=10):\n",
        "    if not attributes:\n",
        "        return 0.0, 0\n",
        "\n",
        "    attr_keywords = [f\"{name} {value}\".lower().split() for name, value in attributes.items()]\n",
        "\n",
        "    soft_scores, strict_scores = [], []\n",
        "\n",
        "    for doc_text in top_docs[:top_k]:\n",
        "        doc_tokens = set(word_tokenize(doc_text.lower()))\n",
        "        sims = []\n",
        "        for attr_tokens in attr_keywords:\n",
        "            overlap = len(set(attr_tokens) & doc_tokens) / len(attr_tokens)\n",
        "            sims.append(overlap)\n",
        "\n",
        "        mean_sim = np.mean(sims)\n",
        "        threshold = max(0.4, mean_sim)\n",
        "\n",
        "        mdcr_soft = mean_sim\n",
        "        mdcr_strict = int(all(s >= threshold for s in sims))\n",
        "\n",
        "        soft_scores.append(mdcr_soft)\n",
        "        strict_scores.append(mdcr_strict)\n",
        "\n",
        "    return float(max(soft_scores)), int(max(strict_scores))\n",
        "\n",
        "# BM25 EVALUATION FUNCTION\n",
        "def evaluate_bm25(top_k=10):\n",
        "    print(\"\\n Evaluating BM25 baseline...\")\n",
        "\n",
        "    # Load Data\n",
        "    with open(DATA_QDOC, \"r\") as f:\n",
        "        qdoc = json.load(f)\n",
        "    with open(DATA_QUERIES, \"r\") as f:\n",
        "        queries = [json.loads(line) for line in f]\n",
        "\n",
        "    # Build corpus\n",
        "    corpus, doc_ids, qid_to_docs = [], [], {}\n",
        "    for entry in qdoc:\n",
        "        qid = entry[\"query_id\"]\n",
        "        qid_to_docs[qid] = {}\n",
        "        for doc in entry[\"documents\"]:\n",
        "            text = doc[\"text\"].strip().lower()\n",
        "            corpus.append(text)\n",
        "            doc_id = f\"{qid}_{doc['doc_id']}\"\n",
        "            doc_ids.append(doc_id)\n",
        "            qid_to_docs[qid][text] = doc_id\n",
        "\n",
        "    print(f\"Building BM25 index for {len(corpus)} documents ...\")\n",
        "    tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    def retrieve_topk(query):\n",
        "        scores = bm25.get_scores(word_tokenize(query.lower()))\n",
        "        top_idx = np.argsort(scores)[::-1][:top_k]\n",
        "        return [(doc_ids[i], corpus[i], float(scores[i])) for i in top_idx]\n",
        "\n",
        "    def find_rank(docid, ranking):\n",
        "        for i, (d, _, _) in enumerate(ranking):\n",
        "            if d == docid:\n",
        "                return i + 1\n",
        "        return len(ranking) + 1\n",
        "\n",
        "    results = []\n",
        "    for q in tqdm(queries, desc=\"Evaluating BM25\"):\n",
        "        try:\n",
        "            qid = q[\"query_id\"]\n",
        "            pos_text = q.get(\"positive_doc\", \"\").strip().lower()\n",
        "            attrs = q.get(\"attributes\", {})\n",
        "            m = len(attrs) or 1\n",
        "\n",
        "            pos_doc = qid_to_docs.get(qid, {}).get(pos_text)\n",
        "            if not pos_doc:\n",
        "                pos_doc = list(qid_to_docs.get(qid, {}).values())[0]\n",
        "\n",
        "            Rori = retrieve_topk(q[\"query\"])\n",
        "            Rins = retrieve_topk(q[\"instructed_query\"])\n",
        "            Rrev = retrieve_topk(q[\"reversed_query\"])\n",
        "\n",
        "            Rori_rank = find_rank(pos_doc, Rori)\n",
        "            Rins_rank = find_rank(pos_doc, Rins)\n",
        "            Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "            Sori = Rori[Rori_rank - 1][2] if Rori_rank <= len(Rori) else 0\n",
        "            Sins = Rins[Rins_rank - 1][2] if Rins_rank <= len(Rins) else 0\n",
        "            Srev = Rrev[Rrev_rank - 1][2] if Rrev_rank <= len(Rrev) else 0\n",
        "\n",
        "            # Compute metrics\n",
        "            mSICR_val = compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev)\n",
        "            mWISE_val = compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, m, 0)\n",
        "            top_docs_instructed = [text for _, text, _ in Rins]\n",
        "            mdcr_soft, mdcr_strict = compute_MDCR_bm25(q[\"instructed_query\"], top_docs_instructed, attrs)\n",
        "\n",
        "            results.append({\n",
        "                \"query_id\": qid,\n",
        "                \"mSICR\": mSICR_val,\n",
        "                \"mWISE\": mWISE_val,\n",
        "                \"MDCR_soft\": mdcr_soft,\n",
        "                \"MDCR_strict\": mdcr_strict\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš  Skipped query {q.get('query_id')} due to: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Aggregate results\n",
        "    df = pd.DataFrame(results)\n",
        "    metrics = {\n",
        "        \"model\": \"BM25\",\n",
        "        \"mSICR\": df[\"mSICR\"].mean(),\n",
        "        \"mWISE\": df[\"mWISE\"].mean(),\n",
        "        \"MDCR_soft\": df[\"MDCR_soft\"].mean(),\n",
        "        \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "        \"queries_evaluated\": len(df)\n",
        "    }\n",
        "\n",
        "    save_path = os.path.join(RESULTS_DIR, \"BM25_metrics.csv\")\n",
        "    df.to_csv(save_path, index=False)\n",
        "\n",
        "    print(f\"\\nâœ… Saved per-query results â†’ {save_path}\")\n",
        "    print(f\"âœ… Aggregate metrics for BM25:\")\n",
        "    print(json.dumps(metrics, indent=2))\n",
        "    return metrics\n",
        "\n",
        "# RUN\n",
        "evaluate_bm25()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6C0W-IS2J1H",
        "outputId": "13a6d0b9-4710-474f-b079-fc0e134bbd19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n",
            "Evaluating BM25 baseline... \n",
            "Building BM25 index for 9576 documents ... \n",
            "Evaluating BM25:  27%|â–ˆâ–ˆâ–‹       | 2616/9596 [10:51<16:53,  6.89it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BM25:  27%|â–ˆâ–ˆâ–‹       | 2620/9596 [10:51<14:55,  7.79it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BM25:  27%|â–ˆâ–ˆâ–‹       | 2624/9596 [10:52<17:09,  6.77it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BM25:  27%|â–ˆâ–ˆâ–‹       | 2628/9596 [10:53<17:40,  6.57it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BM25:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4746/9596 [19:48<11:06,  7.27it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BM25:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4751/9596 [19:48<07:36, 10.62it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BM25:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4757/9596 [19:49<07:25, 10.87it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BM25:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4761/9596 [19:49<07:30, 10.73it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BM25:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8351/9596 [32:50<04:14,  4.89it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating BM25:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8357/9596 [32:51<04:29,  4.60it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating BM25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9596/9596 [37:56<00:00,  4.21it/s]\n",
            "\n",
            "âœ… Saved per-query results â†’ results_generic/BM25_metrics.csv\n",
            "âœ… Aggregate metrics for BM25: \n",
            "{\n",
            "  \"model\": \"BM25\",\n",
            "  \"mSICR\": 0.019832985386221295,\n",
            "  \"mWISE\": 0.031414663453549256,\n",
            "  \"MDCR_soft\": 0.2892078404082579,\n",
            "  \"MDCR_strict\": 0.0,\n",
            "  \"queries_evaluated\": 9580\n",
            "}\n",
            "{'model': 'BM25',\n",
            " 'mSICR': np.float64(0.019832985386221295),\n",
            " 'mWISE': np.float64(0.031414663453549256),\n",
            " 'MDCR_soft': np.float64(0.2892078404082579),\n",
            " 'MDCR_strict': np.float64(0.0),\n",
            " 'queries_evaluated': 9580}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omEeO3VwQP96"
      },
      "source": [
        "#**4.2 Dense Retrieval Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqsyNB-FJFXk"
      },
      "source": [
        "#Bge-Large-v1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzkopKL75e-S"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch, json, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# CONFIG\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATA_QDOC = \"query-doc.json\"\n",
        "DATA_QUERIES = \"final_sorted.jsonl\"\n",
        "RESULTS_DIR = \"results_generic\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# Metric Functions\n",
        "\n",
        "def compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev):\n",
        "    if (Rins_rank < Rori_rank and Sins > Sori) and (Rrev_rank > Rori_rank and Srev < Sori):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, conditions_satisfied, conditions_violated, K=10, N=1):\n",
        "    try:\n",
        "        # Reward case\n",
        "        if Rins_rank <= Rori_rank < Rrev_rank:\n",
        "            if Rori_rank <= N and Rins_rank == 1:\n",
        "                freward = 1.0\n",
        "            elif Rori_rank <= K:\n",
        "                freward = (1 - np.sqrt((Rori_rank - Rins_rank) / K)) * (1 / np.sqrt(max(Rins_rank, 1)))\n",
        "            else:\n",
        "                freward = 0.01\n",
        "            return freward * (conditions_satisfied / m)\n",
        "\n",
        "        # Penalty case\n",
        "        else:\n",
        "            if Rrev_rank < Rori_rank < Rins_rank:\n",
        "                fpenalty = -1.0\n",
        "            elif Rori_rank <= Rins_rank:\n",
        "                fpenalty = (Rori_rank - Rins_rank) / max(Rins_rank, 1)\n",
        "            elif Rrev_rank <= Rori_rank:\n",
        "                fpenalty = (Rrev_rank - Rori_rank) / max(Rori_rank, 1)\n",
        "            else:\n",
        "                fpenalty = -0.5\n",
        "            return fpenalty * (conditions_violated / m)\n",
        "\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def compute_MDCR(model, top_docs, attributes, top_k=10):\n",
        "    if not attributes:\n",
        "        return 0.0, 0\n",
        "\n",
        "    attr_embs = []\n",
        "    for attr_name, attr_value in attributes.items():\n",
        "        desc = f\"The document should reflect {attr_name} = {attr_value}.\"\n",
        "        emb = model.encode(desc, convert_to_tensor=True)\n",
        "        attr_embs.append(emb)\n",
        "\n",
        "    soft_scores, strict_scores = [], []\n",
        "\n",
        "    # For each top-K document, evaluate attribute satisfaction\n",
        "    for doc_text in top_docs:\n",
        "        doc_emb = model.encode(doc_text, convert_to_tensor=True)\n",
        "        sims = [float(util.cos_sim(doc_emb, a_emb)) for a_emb in attr_embs]\n",
        "        sims = np.clip(sims, 0, 1)\n",
        "\n",
        "\n",
        "        mean_sim = np.mean(sims)\n",
        "        std_sim = np.std(sims)\n",
        "        threshold = np.clip(mean_sim + 0.1 * std_sim, 0.5, 0.9)\n",
        "\n",
        "        # Soft score\n",
        "        if np.max(sims) - np.min(sims) < 1e-6:\n",
        "            mdcr_soft = float(mean_sim)\n",
        "        else:\n",
        "            mdcr_soft = float((mean_sim - np.min(sims)) / (np.max(sims) - np.min(sims) + 1e-6))\n",
        "\n",
        "        # Strict score\n",
        "        mdcr_strict = int(all(s >= threshold for s in sims))\n",
        "\n",
        "        soft_scores.append(mdcr_soft)\n",
        "        strict_scores.append(mdcr_strict)\n",
        "\n",
        "    return float(max(soft_scores)), int(max(strict_scores))\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_embedding_model(model_name, batch_size=32, top_k=10):\n",
        "    print(f\"\\n Loading Embedding Model: {model_name}\")\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "    # Load Data\n",
        "    with open(DATA_QDOC, \"r\") as f:\n",
        "        qdoc = json.load(f)\n",
        "    with open(DATA_QUERIES, \"r\") as f:\n",
        "        queries = [json.loads(line) for line in f]\n",
        "\n",
        "    # Build Corpus\n",
        "    corpus, doc_ids, qid_to_docs = [], [], {}\n",
        "    for entry in qdoc:\n",
        "        qid = entry[\"query_id\"]\n",
        "        qid_to_docs[qid] = {}\n",
        "        for doc in entry[\"documents\"]:\n",
        "            text = doc[\"text\"].strip().lower()\n",
        "            corpus.append(text)\n",
        "            doc_id = f\"{qid}_{doc['doc_id']}\"\n",
        "            doc_ids.append(doc_id)\n",
        "            qid_to_docs[qid][text] = doc_id\n",
        "\n",
        "    print(f\"Encoding {len(corpus)} documents ...\")\n",
        "    corpus_embs = model.encode(corpus, convert_to_tensor=True, batch_size=batch_size, show_progress_bar=True)\n",
        "\n",
        "    def retrieve_topk(query):\n",
        "        query_emb = model.encode(query, convert_to_tensor=True)\n",
        "        cos_scores = util.cos_sim(query_emb, corpus_embs)[0]\n",
        "        top_results = torch.topk(cos_scores, top_k)\n",
        "        return [(doc_ids[idx], corpus[idx], float(cos_scores[idx])) for idx in top_results.indices]\n",
        "\n",
        "    def find_rank(docid, ranking):\n",
        "        for i, (d, _, _) in enumerate(ranking):\n",
        "            if d == docid:\n",
        "                return i + 1\n",
        "        return len(ranking) + 1\n",
        "\n",
        "    # Evaluation Loop\n",
        "    results = []\n",
        "\n",
        "    for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "        try:\n",
        "            qid = q[\"query_id\"]\n",
        "            pos_text = q.get(\"positive_doc\", \"\").strip().lower()\n",
        "            attrs = q.get(\"attributes\", {})\n",
        "            m = len(attrs) or 1\n",
        "\n",
        "            # Identify positive document\n",
        "            pos_doc = qid_to_docs.get(qid, {}).get(pos_text)\n",
        "            if not pos_doc:\n",
        "                pos_emb = model.encode(pos_text, convert_to_tensor=True)\n",
        "                cos_scores = util.cos_sim(pos_emb, corpus_embs)[0]\n",
        "                pos_doc = doc_ids[int(torch.argmax(cos_scores))]\n",
        "\n",
        "            # Retrieve top-k results\n",
        "            Rori = retrieve_topk(q[\"query\"])\n",
        "            Rins = retrieve_topk(q[\"instructed_query\"])\n",
        "            Rrev = retrieve_topk(q[\"reversed_query\"])\n",
        "\n",
        "            Rori_rank = find_rank(pos_doc, Rori)\n",
        "            Rins_rank = find_rank(pos_doc, Rins)\n",
        "            Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "            Sori = Rori[Rori_rank - 1][2] if Rori_rank <= len(Rori) else 0\n",
        "            Sins = Rins[Rins_rank - 1][2] if Rins_rank <= len(Rins) else 0\n",
        "            Srev = Rrev[Rrev_rank - 1][2] if Rrev_rank <= len(Rrev) else 0\n",
        "\n",
        "            # Metrics\n",
        "            conditions_satisfied = m\n",
        "            conditions_violated = 0\n",
        "\n",
        "            mSICR_val = compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev)\n",
        "            mWISE_val = compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, conditions_satisfied, conditions_violated)\n",
        "\n",
        "            # Adaptive MDCR\n",
        "            top_docs_instructed = [text for _, text, _ in Rins]\n",
        "            mdcr_soft, mdcr_strict = compute_MDCR(model, top_docs_instructed, attrs, top_k=top_k)\n",
        "\n",
        "            results.append({\n",
        "                \"query_id\": qid,\n",
        "                \"mSICR\": mSICR_val,\n",
        "                \"mWISE\": mWISE_val,\n",
        "                \"MDCR_soft\": mdcr_soft,\n",
        "                \"MDCR_strict\": mdcr_strict\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš  Skipped query {q.get('query_id')} due to: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Aggregate results\n",
        "    df = pd.DataFrame(results)\n",
        "    metrics = {\n",
        "        \"model\": model_name,\n",
        "        \"mSICR\": df[\"mSICR\"].mean(),\n",
        "        \"mWISE\": df[\"mWISE\"].mean(),\n",
        "        \"MDCR_soft\": df[\"MDCR_soft\"].mean(),\n",
        "        \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "        \"queries_evaluated\": len(df)\n",
        "    }\n",
        "\n",
        "    save_path = os.path.join(RESULTS_DIR, f\"{model_name.replace('/', '_')}_metrics.csv\")\n",
        "    df.to_csv(save_path, index=False)\n",
        "\n",
        "    print(f\"\\nâœ… Saved per-query results â†’ {save_path}\")\n",
        "    print(f\"âœ… Aggregate metrics for {model_name}:\")\n",
        "    print(json.dumps(metrics, indent=2))\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_embedding_model(\"BAAI/bge-large-en-v1.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGbtBzya5PLy",
        "outputId": "f764a746-7fd4-45fc-8db3-c7f9fbefc91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Embedding Model: BAAI/bge-large-en-v1.5\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "modules.json:â€‡100%â€‡349/349â€‡[00:00<00:00,â€‡32.0kB/s]config_sentence_transformers.json:â€‡100%â€‡124/124â€‡[00:00<00:00,â€‡16.0kB/s]README.md:â€‡â€‡94.6k/?â€‡[00:00<00:00,â€‡6.09MB/s]sentence_bert_config.json:â€‡100%â€‡52.0/52.0â€‡[00:00<00:00,â€‡5.01kB/s]config.json:â€‡100%â€‡779/779â€‡[00:00<00:00,â€‡73.9kB/s]model.safetensors:â€‡100%â€‡1.34G/1.34Gâ€‡[00:12<00:00,â€‡132MB/s]tokenizer_config.json:â€‡100%â€‡366/366â€‡[00:00<00:00,â€‡36.0kB/s]vocab.txt:â€‡â€‡232k/?â€‡[00:00<00:00,â€‡12.7MB/s]tokenizer.json:â€‡â€‡711k/?â€‡[00:00<00:00,â€‡32.8MB/s]special_tokens_map.json:â€‡100%â€‡125/125â€‡[00:00<00:00,â€‡13.7kB/s]config.json:â€‡100%â€‡191/191â€‡[00:00<00:00,â€‡20.6kB/s]Encoding 9576 documents ...\n",
            "Batches:â€‡100%â€‡300/300â€‡[03:56<00:00,â€‡â€‡3.28it/s]Evaluating BAAI/bge-large-en-v1.5:  27%|â–ˆâ–ˆâ–‹       | 2614/9596 [20:06<58:31,  1.99it/s]  âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  27%|â–ˆâ–ˆâ–‹       | 2618/9596 [20:07<47:36,  2.44it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  27%|â–ˆâ–ˆâ–‹       | 2622/9596 [20:09<44:32,  2.61it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  27%|â–ˆâ–ˆâ–‹       | 2626/9596 [20:10<39:53,  2.91it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4744/9596 [36:55<32:59,  2.45it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4746/9596 [36:56<26:19,  3.07it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4748/9596 [36:56<22:54,  3.53it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4751/9596 [36:56<16:11,  4.99it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4754/9596 [36:57<16:37,  4.85it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4756/9596 [36:57<15:11,  5.31it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4758/9596 [36:57<14:09,  5.70it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4761/9596 [36:58<11:53,  6.77it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8351/9596 [1:00:44<07:21,  2.82it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8357/9596 [1:00:46<08:09,  2.53it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating BAAI/bge-large-en-v1.5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9596/9596 [1:11:17<00:00,  2.24it/s]\n",
            "âœ… Saved per-query results â†’ results_generic/BAAI_bge-large-en-v1.5_metrics.csv\n",
            "âœ… Aggregate metrics for BAAI/bge-large-en-v1.5:\n",
            "{\n",
            "  \"model\": \"BAAI/bge-large-en-v1.5\",\n",
            "  \"mSICR\": 0.008559498956158663,\n",
            "  \"mWISE\": 0.05453979198037159,\n",
            "  \"MDCR_soft\": 0.5312920170601979,\n",
            "  \"MDCR_strict\": 0.0101523645839205,\n",
            "  \"queries_evaluated\": 9580\n",
            "}\n",
            "\n",
            "{'model': 'BAAI/bge-large-en-v1.5',\n",
            " 'mSICR': np.float64(0.008559498956158663),\n",
            " 'mWISE': np.float64(0.05453979198037159),\n",
            " 'MDCR_soft': np.float64(0.5312920170601979),\n",
            " 'MDCR_strict': np.float64(0.010152364583920),\n",
            " 'queries_evaluated': 9580}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8XSbMmiM7tg"
      },
      "source": [
        "#E5-Large-v2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"intfloat/e5-large-v2\"\n",
        "evaluate_embedding_model(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yzl8Hwen5uUJ",
        "outputId": "b5641fac-ec75-4c19-b62c-2810f90fdc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Embedding Model: intfloat/e5-large-v2 \n",
            "Encoding 9576 documents ...\n",
            "Batches:â€‡100%â€‡300/300â€‡[03:55<00:00,â€‡â€‡3.57it/s]Evaluating intfloat/e5-large-v2:  27%|â–ˆâ–ˆâ–‹       | 2614/9596 [19:58<53:36,  2.17it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  27%|â–ˆâ–ˆâ–‹       | 2618/9596 [19:59<42:23,  2.74it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  27%|â–ˆâ–ˆâ–‹       | 2622/9596 [20:00<39:28,  2.94it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  27%|â–ˆâ–ˆâ–‹       | 2626/9596 [20:01<38:06,  3.05it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4744/9596 [36:36<28:08,  2.87it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4746/9596 [36:36<22:43,  3.56it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4748/9596 [36:37<20:18,  3.98it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4751/9596 [36:37<16:19,  4.94it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4754/9596 [36:38<20:23,  3.96it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4756/9596 [36:38<18:29,  4.36it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4758/9596 [36:39<16:22,  4.92it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4761/9596 [36:39<12:59,  6.20it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8351/9596 [1:00:15<07:14,  2.87it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8357/9596 [1:00:17<08:55,  2.31it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating intfloat/e5-large-v2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9596/9596 [1:09:44<00:00,  2.29it/s]\n",
            "âœ… Saved per-query results â†’ results_generic/intfloat_e5-large-v2_metrics.csv\n",
            "âœ…Aggregate metrics for intfloat/e5-large-v2: \n",
            "{\n",
            "  \"model\": \"intfloat/e5-large-v2\",\n",
            "  \"mSICR\": 0.015448851774530271,\n",
            "  \"mWISE\": 0.05378945332822605,\n",
            "  \"MDCR_soft\": 0.5345863448810532,\n",
            "  \"MDCR_strict\": 0.0106702859319631, \n",
            "  \"queries_evaluated\": 9580\n",
            "}\n",
            "\n",
            "{'model': 'intfloat/e5-large-v2',\n",
            " 'mSICR': np.float64(0.015448851774530271),\n",
            " 'mWISE': np.float64(0.05378945332822605),\n",
            " 'MDCR_soft': np.float64(0.5345863448810532),\n",
            " 'MDCR_strict': np.float64(0.0106702859319631),\n",
            " 'queries_evaluated': 9580}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftNHNxpvNEiH"
      },
      "source": [
        "#Instructor-XL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_embedding_model(\"hkunlp/instructor-large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYqp8YvF8bLT",
        "outputId": "4d48668a-98d5-4f38-c3db-6f59df83e2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Embedding Model: hkunlp/instructor-large\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
            "The secret HF_TOKEN does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "modules.json:â€‡100%\n",
            "â€‡461/461â€‡[00:00<00:00,â€‡49.8kB/s]\n",
            "config_sentence_transformers.json:â€‡100%\n",
            "â€‡122/122â€‡[00:00<00:00,â€‡8.99kB/s]\n",
            "README.md:\n",
            "â€‡66.3k/?â€‡[00:00<00:00,â€‡6.75MB/s]\n",
            "sentence_bert_config.json:â€‡100%\n",
            "â€‡53.0/53.0â€‡[00:00<00:00,â€‡5.49kB/s]\n",
            "config.json:\n",
            "â€‡1.53k/?â€‡[00:00<00:00,â€‡160kB/s]\n",
            "pytorch_model.bin:â€‡100%\n",
            "â€‡1.34G/1.34Gâ€‡[00:13<00:00,â€‡79.8MB/s]\n",
            "tokenizer_config.json:\n",
            "â€‡2.41k/?â€‡[00:00<00:00,â€‡241kB/s]\n",
            "model.safetensors:â€‡100%\n",
            "â€‡1.34G/1.34Gâ€‡[00:13<00:00,â€‡125MB/s]\n",
            "spiece.model:â€‡100%\n",
            "â€‡792k/792kâ€‡[00:01<00:00,â€‡757kB/s]\n",
            "tokenizer.json:\n",
            "â€‡2.42M/?â€‡[00:00<00:00,â€‡25.5MB/s]\n",
            "special_tokens_map.json:\n",
            "â€‡2.20k/?â€‡[00:00<00:00,â€‡59.3kB/s]\n",
            "config.json:â€‡100%\n",
            "â€‡270/270â€‡[00:00<00:00,â€‡3.97kB/s]\n",
            "config.json:â€‡100%\n",
            "â€‡116/116â€‡[00:00<00:00,â€‡3.21kB/s]\n",
            "2_Dense/pytorch_model.bin:â€‡100%\n",
            "â€‡3.15M/3.15Mâ€‡[00:00<00:00,â€‡5.29MB/s]\n",
            "Encoding 9576 documents ...\n",
            "Batches:â€‡100%\n",
            "â€‡300/300â€‡[05:18<00:00,â€‡â€‡2.86it/s]\n",
            "Evaluating hkunlp/instructor-large:  27%|â–ˆâ–ˆâ–‹       | 2614/9596 [23:14<1:16:29,  1.52it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  27%|â–ˆâ–ˆâ–‹       | 2618/9596 [23:16<53:49,  2.16it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  27%|â–ˆâ–ˆâ–‹       | 2622/9596 [23:17<52:00,  2.23it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  27%|â–ˆâ–ˆâ–‹       | 2626/9596 [23:19<46:59,  2.47it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4744/9596 [43:08<32:44,  2.47it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4746/9596 [43:08<24:24,  3.31it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4748/9596 [43:08<20:29,  3.94it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4751/9596 [43:09<15:37,  5.17it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4754/9596 [43:09<17:34,  4.59it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4756/9596 [43:10<16:44,  4.82it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4758/9596 [43:10<16:06,  5.00it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4761/9596 [43:11<13:27,  5.98it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8351/9596 [1:09:57<08:47,  2.36it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8357/9596 [1:10:00<09:42,  2.13it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating hkunlp/instructor-large: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9596/9596 [1:22:34<00:00,  1.94it/s]\n",
            "âœ… Saved per-query results â†’ results_generic/hkunlp_instructor-large_metrics.csv\n",
            "âœ… Aggregate metrics for hkunlp/instructor-large:\n",
            "{\n",
            "  \"model\": \"hkunlp/instructor-large\",\n",
            "  \"mSICR\": 0.02139874739039666,\n",
            "  \"mWISE\": 0.06053711643105075,\n",
            "  \"MDCR_soft\": 0.5271962203459227,\n",
            "  \"MDCR_strict\": 0.0195678230456725,\n",
            "  \"queries_evaluated\": 9580\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO3G7JTUNv1q"
      },
      "source": [
        "#GTE-Qwen2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_embedding_model(\"thenlper/gte-small\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQxa0EHa7W2K",
        "outputId": "d7dd8b6a-b649-4867-cbce-25e7421e99ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Embedding Model: thenlper/gte-small \n",
            "modules.json:â€‡100%\n",
            "â€‡385/385â€‡[00:00<00:00,â€‡46.6kB/s]\n",
            "README.md:\n",
            "â€‡68.1k/?â€‡[00:00<00:00,â€‡5.96MB/s]\n",
            "sentence_bert_config.json:â€‡100%\n",
            "â€‡57.0/57.0â€‡[00:00<00:00,â€‡6.67kB/s]\n",
            "config.json:â€‡100%\n",
            "â€‡583/583â€‡[00:00<00:00,â€‡68.9kB/s]\n",
            "model.safetensors:â€‡100%\n",
            "â€‡66.7M/66.7Mâ€‡[00:00<00:00,â€‡93.1MB/s]\n",
            "tokenizer_config.json:â€‡100%\n",
            "â€‡394/394â€‡[00:00<00:00,â€‡40.0kB/s]\n",
            "vocab.txt:\n",
            "â€‡232k/?â€‡[00:00<00:00,â€‡11.2MB/s]\n",
            "tokenizer.json:\n",
            "â€‡712k/?â€‡[00:00<00:00,â€‡37.9MB/s]\n",
            "special_tokens_map.json:â€‡100%\n",
            "â€‡125/125â€‡[00:00<00:00,â€‡16.0kB/s]\n",
            "config.json:â€‡100%\n",
            "â€‡190/190â€‡[00:00<00:00,â€‡20.8kB/s]\n",
            "Encoding 9576 documents ...\n",
            "Batches:â€‡100%\n",
            "â€‡300/300â€‡[00:23<00:00,â€‡30.16it/s]\n",
            "Evaluating thenlper/gte-small:  27%|â–ˆâ–ˆâ–‹       | 2616/9596 [07:32<14:58,  7.77it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating thenlper/gte-small:  27%|â–ˆâ–ˆâ–‹       | 2618/9596 [07:33<19:51,  5.86it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating thenlper/gte-small:  27%|â–ˆâ–ˆâ–‹       | 2624/9596 [07:33<17:20,  6.70it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating thenlper/gte-small:  27%|â–ˆâ–ˆâ–‹       | 2626/9596 [07:34<22:07,  5.25it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating thenlper/gte-small:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4746/9596 [13:42<10:02,  8.06it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating thenlper/gte-small:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4751/9596 [13:43<06:38, 12.17it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating thenlper/gte-small:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4757/9596 [13:43<06:53, 11.69it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating thenlper/gte-small:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4761/9596 [13:43<06:41, 12.04it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating thenlper/gte-small:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8353/9596 [23:59<02:35,  7.98it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating thenlper/gte-small:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8359/9596 [24:00<02:30,  8.23it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating thenlper/gte-small: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9596/9596 [27:36<00:00,  5.79it/s]\n",
            "\n",
            "âœ… Saved per-query results â†’ results_generic/thenlper_gte-small_metrics.csv\n",
            "âœ… Aggregate metrics for thenlper/gte-small: \n",
            "{\n",
            "  \"model\": \"thenlper/gte-small\",\n",
            "  \"mSICR\": 0.0081419624217119,\n",
            "  \"mWISE\": 0.04888272938551353,\n",
            "  \"MDCR_soft\": 0.5307835959217594,\n",
            "  \"MDCR_strict\": 0.0217845089345181,\n",
            "  \"queries_evaluated\": 9580\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21OBFtv0OK4D"
      },
      "source": [
        "# GritLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_embedding_model(\"Alibaba-NLP/gte-base-en-v1.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42r0K2nd7apx",
        "outputId": "0a416c78-45d7-4317-8602-136d0c4ece38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Embedding Model: Alibaba-NLP/gte-base-en-v1.5\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret HF_TOKEN does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "modules.json:â€‡100%\n",
            "â€‡229/229â€‡[00:00<00:00,â€‡25.3kB/s]\n",
            "README.md:â€‡\n",
            "â€‡72.3k/?â€‡[00:00<00:00,â€‡6.28MB/s]\n",
            "sentence_bert_config.json:â€‡100%\n",
            "â€‡54.0/54.0â€‡[00:00<00:00,â€‡6.11kB/s]\n",
            "config.json:â€‡\n",
            "â€‡1.35k/?â€‡[00:00<00:00,â€‡145kB/s]\n",
            "configuration.py:â€‡\n",
            "â€‡7.13k/?â€‡[00:00<00:00,â€‡705kB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
            "- configuration.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "modeling.py:â€‡\n",
            "â€‡59.0k/?â€‡[00:00<00:00,â€‡6.07MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
            "- modeling.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "model.safetensors:â€‡100%\n",
            "â€‡547M/547Mâ€‡[00:03<00:00,â€‡267MB/s]\n",
            "tokenizer_config.json:â€‡\n",
            "â€‡1.38k/?â€‡[00:00<00:00,â€‡129kB/s]\n",
            "vocab.txt:â€‡\n",
            "â€‡232k/?â€‡[00:00<00:00,â€‡11.2MB/s]\n",
            "tokenizer.json:â€‡\n",
            "â€‡712k/?â€‡[00:00<00:00,â€‡39.9MB/s]\n",
            "special_tokens_map.json:â€‡100%\n",
            "â€‡695/695â€‡[00:00<00:00,â€‡72.4kB/s]\n",
            "config.json:â€‡100%\n",
            "â€‡297/297â€‡[00:00<00:00,â€‡31.4kB/s]\n",
            "Encoding 9576 documents ...\n",
            "Batches:â€‡100%\n",
            "â€‡300/300â€‡[01:33<00:00,â€‡â€‡8.66it/s]\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5:  27%|â–ˆâ–ˆâ–‹       | 2614/9596 [09:47<33:58,  3.43it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5:  27%|â–ˆâ–ˆâ–‹       | 2618/9596 [09:48<26:37,  4.37it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5:  27%|â–ˆâ–ˆâ–‹       | 2624/9596 [09:49<18:54,  6.14it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5:  27%|â–ˆâ–ˆâ–‹       | 2628/9596 [09:50<16:35,  7.00it/s]âš  Skipped query 192005 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4746/9596 [17:55<12:00,  6.73it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4751/9596 [17:55<07:39, 10.54it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4757/9596 [17:56<07:34, 10.65it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4761/9596 [17:56<07:27, 10.80it/s]âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "âš  Skipped query 447321 due to: 'dict' object has no attribute 'strip'\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8351/9596 [29:57<03:49,  5.42it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8357/9596 [29:58<03:54,  5.29it/s]âš  Skipped query 1042639 due to: 'list' object has no attribute 'strip'\n",
            "Evaluating Alibaba-NLP/gte-base-en-v1.5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9596/9596 [35:01<00:00,  4.57it/s]\n",
            "âœ… Saved per-query results â†’ results_generic/Alibaba-NLP_gte-base-en-v1.5_metrics.csv\n",
            "âœ… Aggregate metrics for Alibaba-NLP/gte-base-en-v1.5:\n",
            "{\n",
            "  \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n",
            "  \"mSICR\": 0.015135699373695199,\n",
            "  \"mWISE\": 0.05208533839120343,\n",
            "  \"MDCR_soft\": 0.5408017348057576,\n",
            "  \"MDCR_strict\": 0.0223564728946294,\n",
            "  \"queries_evaluated\": 9580\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EI7_XSdN9Yl"
      },
      "source": [
        "# E5-Mistral-ins"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub tqdm pandas numpy --quiet\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# CONFIG\n",
        "HF_ENDPOINT = \"#hidden\"\n",
        "HF_TOKEN = \"#hidden\"\n",
        "\n",
        "MODEL_NAME = \"intfloat/e5-mistral-7b-instruct\"\n",
        "QUERY_DOC_FILE = \"query-doc.json\"\n",
        "FINAL_SORTED_FILE = \"final_sorted.jsonl\"\n",
        "\n",
        "EMBED_DIR = Path(\"embeddings_remote\")\n",
        "RESULTS_DIR = Path(\"results_remote\")\n",
        "EMBED_DIR.mkdir(exist_ok=True)\n",
        "RESULTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "TOP_K = 100\n",
        "DEVICE = \"remote_endpoint\"\n",
        "\n",
        "print(f\"âœ… Using remote inference endpoint: {HF_ENDPOINT}\")\n",
        "print(f\"Running evaluation for model: {MODEL_NAME}\")\n",
        "\n",
        "\n",
        "def load_data(query_doc_file, final_sorted_file):\n",
        "    with open(query_doc_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        qdoc = json.load(f)\n",
        "    queries = []\n",
        "    with open(final_sorted_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                queries.append(json.loads(line))\n",
        "    return qdoc, queries\n",
        "\n",
        "\n",
        "def build_corpus_from_qdoc(qdoc):\n",
        "    corpus, doc_ids, qid_to_docs = [], [], {}\n",
        "    for entry in qdoc:\n",
        "        qid = entry[\"query_id\"]\n",
        "        qid_to_docs[qid] = {}\n",
        "        for doc in entry[\"documents\"]:\n",
        "            text = doc[\"text\"].strip()\n",
        "            key = text.lower()\n",
        "            doc_id = f\"{qid}_{doc['doc_id']}\"\n",
        "            corpus.append(text)\n",
        "            doc_ids.append(doc_id)\n",
        "            qid_to_docs[qid][key] = doc_id\n",
        "    return corpus, doc_ids, qid_to_docs\n",
        "\n",
        "\n",
        "def remote_embed(sentences, client, batch_size=2, retries=3, sleep_time=10):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Embedding via Endpoint\"):\n",
        "        batch = [s.strip() for s in sentences[i:i + batch_size] if s and s.strip()]\n",
        "        if not batch:\n",
        "            continue\n",
        "\n",
        "        success = False\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                resp = client.feature_extraction(batch)\n",
        "\n",
        "                if isinstance(resp, np.ndarray):\n",
        "                    arr = resp.astype(np.float32)\n",
        "                elif isinstance(resp, list):\n",
        "                    if isinstance(resp[0], list) and isinstance(resp[0][0], list):\n",
        "                        resp = resp[0]\n",
        "                    arr = np.array(resp, dtype=np.float32)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected response type: {type(resp)}\")\n",
        "\n",
        "                if arr.ndim != 2:\n",
        "                    raise ValueError(f\"Unexpected embedding shape: {arr.shape}\")\n",
        "\n",
        "                embeddings.append(arr)\n",
        "                success = True\n",
        "                break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Failed batch {i}, attempt {attempt+1}/{retries}: {e}\")\n",
        "                time.sleep(sleep_time * (2 ** attempt))\n",
        "\n",
        "        if not success:\n",
        "            print(f\"âŒ Skipping batch {i} after {retries} retries.\")\n",
        "\n",
        "    if not embeddings:\n",
        "        raise RuntimeError(\"No successful embeddings retrieved.\")\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "\n",
        "def encode_corpus_cached(model_name, corpus, client):\n",
        "    safe_name = model_name.replace(\"/\", \"_\")\n",
        "    out_dir = EMBED_DIR / safe_name\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "    emb_file = out_dir / \"corpus_emb.npy\"\n",
        "    if emb_file.exists():\n",
        "        print(f\"âœ… Loading cached corpus embeddings for {model_name}\")\n",
        "        return np.load(emb_file)\n",
        "    print(f\" Encoding corpus for {model_name} via endpoint ...\")\n",
        "    embs = remote_embed(corpus, client, batch_size=BATCH_SIZE)\n",
        "    np.save(emb_file, embs)\n",
        "    return embs\n",
        "\n",
        "\n",
        "def cosine_sim(a, b):\n",
        "    a = a / np.linalg.norm(a)\n",
        "    b = b / np.linalg.norm(b)\n",
        "    return float(np.dot(a, b))\n",
        "\n",
        "\n",
        "def find_rank(target_docid, ranked_list):\n",
        "    try:\n",
        "        return ranked_list.index(target_docid) + 1\n",
        "    except ValueError:\n",
        "        return len(ranked_list) + 1\n",
        "\n",
        "\n",
        "# Metric Functions\n",
        "def compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev):\n",
        "    if (Rins_rank < Rori_rank and Sins > Sori) and (Rrev_rank > Rori_rank and Srev < Sori):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, conditions_satisfied, conditions_violated, K=10, N=1):\n",
        "    try:\n",
        "        if Rins_rank <= Rori_rank < Rrev_rank:\n",
        "            if Rori_rank <= N and Rins_rank == 1:\n",
        "                freward = 1.0\n",
        "            elif Rori_rank <= K:\n",
        "                freward = (1 - np.sqrt((Rori_rank - Rins_rank) / K)) * (1 / np.sqrt(max(Rins_rank, 1)))\n",
        "            else:\n",
        "                freward = 0.01\n",
        "            return freward * (conditions_satisfied / m)\n",
        "        else:\n",
        "            if Rrev_rank < Rori_rank < Rins_rank:\n",
        "                fpenalty = -1.0\n",
        "            elif Rori_rank <= Rins_rank:\n",
        "                fpenalty = (Rori_rank - Rins_rank) / max(Rins_rank, 1)\n",
        "            elif Rrev_rank <= Rori_rank:\n",
        "                fpenalty = (Rrev_rank - Rori_rank) / max(Rori_rank, 1)\n",
        "            else:\n",
        "                fpenalty = -0.5\n",
        "            return fpenalty * (conditions_violated / m)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def compute_MDCR_remote(client, top_docs, attributes):\n",
        "    if not attributes:\n",
        "        return 0.0, 0\n",
        "\n",
        "    attr_embs = []\n",
        "    for k, v in attributes.items():\n",
        "        desc = f\"The document should reflect {k} = {v}.\"\n",
        "        attr_emb = remote_embed([desc], client, batch_size=1)[0]\n",
        "        attr_embs.append(attr_emb)\n",
        "\n",
        "    soft_scores, strict_scores = [], []\n",
        "    for doc_text in top_docs:\n",
        "        doc_emb = remote_embed([doc_text], client, batch_size=1)[0]\n",
        "        sims = [cosine_sim(doc_emb, a_emb) for a_emb in attr_embs]\n",
        "        sims = np.clip(sims, 0, 1)\n",
        "\n",
        "        mean_sim = np.mean(sims)\n",
        "        std_sim = np.std(sims)\n",
        "        threshold = np.clip(mean_sim + 0.1 * std_sim, 0.5, 0.9)\n",
        "\n",
        "        mdcr_soft = float((mean_sim - np.min(sims)) / (np.max(sims) - np.min(sims) + 1e-6))\n",
        "        mdcr_strict = int(all(s >= threshold for s in sims))\n",
        "\n",
        "        soft_scores.append(mdcr_soft)\n",
        "        strict_scores.append(mdcr_strict)\n",
        "\n",
        "    return float(max(soft_scores)), int(max(strict_scores))\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model_remote(model_name, corpus, doc_ids, qid_to_docs, queries):\n",
        "    client = InferenceClient(base_url=HF_ENDPOINT, token=HF_TOKEN)\n",
        "    results = []\n",
        "\n",
        "    corpus_embs = encode_corpus_cached(model_name, corpus, client)\n",
        "    corpus_embs = corpus_embs / np.linalg.norm(corpus_embs, axis=1, keepdims=True).clip(min=1e-9)\n",
        "\n",
        "    def retrieve_topk(text):\n",
        "        q_emb = remote_embed([text], client, batch_size=1)[0]\n",
        "        q_emb = q_emb / np.linalg.norm(q_emb)\n",
        "        sims = corpus_embs @ q_emb\n",
        "        idxs = np.argsort(-sims)[:TOP_K]\n",
        "        return [(doc_ids[i], corpus[i], float(sims[i])) for i in idxs]\n",
        "\n",
        "    for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "        try:\n",
        "            qid = q[\"query_id\"]\n",
        "            pos_text = q.get(\"positive_doc\", \"\")\n",
        "            if isinstance(pos_text, dict):\n",
        "                pos_text = pos_text.get(\"text\", \"\")\n",
        "            pos_text = str(pos_text).strip().lower()\n",
        "            if not pos_text:\n",
        "                continue\n",
        "\n",
        "            pos_doc = qid_to_docs.get(qid, {}).get(pos_text)\n",
        "            if not pos_doc:\n",
        "                continue\n",
        "\n",
        "            attrs = q.get(\"attributes\", {})\n",
        "            m = len(attrs) or 1\n",
        "            conditions_satisfied, conditions_violated = m, 0\n",
        "\n",
        "            Rori = retrieve_topk(q[\"query\"])\n",
        "            Rins = retrieve_topk(q[\"instructed_query\"])\n",
        "            Rrev = retrieve_topk(q[\"reversed_query\"])\n",
        "\n",
        "            Rori_rank = find_rank(pos_doc, [d for d, _, _ in Rori])\n",
        "            Rins_rank = find_rank(pos_doc, [d for d, _, _ in Rins])\n",
        "            Rrev_rank = find_rank(pos_doc, [d for d, _, _ in Rrev])\n",
        "\n",
        "            Sori = Rori[Rori_rank - 1][2] if Rori_rank <= len(Rori) else 0\n",
        "            Sins = Rins[Rins_rank - 1][2] if Rins_rank <= len(Rins) else 0\n",
        "            Srev = Rrev[Rrev_rank - 1][2] if Rrev_rank <= len(Rrev) else 0\n",
        "\n",
        "            mSICR_val = compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev)\n",
        "            mWISE_val = compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, conditions_satisfied, conditions_violated)\n",
        "            mdcr_soft, mdcr_strict = compute_MDCR_remote(client, [t for _, t, _ in Rins], attrs)\n",
        "\n",
        "            results.append({\n",
        "                \"query_id\": qid,\n",
        "                \"mSICR\": mSICR_val,\n",
        "                \"mWISE\": mWISE_val,\n",
        "                \"MDCR_soft\": mdcr_soft,\n",
        "                \"MDCR_strict\": mdcr_strict\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Skipped query {q.get('query_id')} due to: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not results:\n",
        "        print(f\"âš ï¸ No results for {model_name}\")\n",
        "        return None\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    metrics = {\n",
        "        \"model\": model_name,\n",
        "        \"mSICR\": df[\"mSICR\"].mean(),\n",
        "        \"mWISE\": df[\"mWISE\"].mean(),\n",
        "        \"MDCR_soft\": df[\"MDCR_soft\"].mean(),\n",
        "        \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "        \"queries_evaluated\": len(df)\n",
        "    }\n",
        "\n",
        "    df.to_csv(RESULTS_DIR / f\"{model_name.replace('/', '_')}_per_query.csv\", index=False)\n",
        "    print(f\"\\nâœ… Saved per-query results â†’ {RESULTS_DIR}/{model_name.replace('/', '_')}_per_query.csv\")\n",
        "    print(f\"âœ… Aggregate metrics for {model_name}:\")\n",
        "    print(json.dumps(metrics, indent=2))\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Main\n",
        "def main():\n",
        "    qdoc, queries = load_data(QUERY_DOC_FILE, FINAL_SORTED_FILE)\n",
        "    corpus, doc_ids, qid_to_docs = build_corpus_from_qdoc(qdoc)\n",
        "    print(f\"Corpus: {len(corpus)} docs | Queries: {len(queries)}\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    metrics = evaluate_model_remote(MODEL_NAME, corpus, doc_ids, qid_to_docs, queries)\n",
        "    if metrics:\n",
        "        pd.DataFrame([metrics]).to_csv(RESULTS_DIR / \"mistral_remote_metrics.csv\", index=False)\n",
        "    print(f\"â± Total time: {time.time() - t0:.1f}s\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fheXJCVyZkzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2106999-c991-4318-9fb5-708aafb9bcd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Using remote inference endpoint: https://qiyh6rp25py7sppf.us-east-1.aws.endpoints.huggingface.cloud\n",
            "Running evaluation for model: intfloat/e5-mistral-7b-instruct\n",
            "Corpus: 9576 docs | Queries: 9596\n",
            "Encoding corpus for intfloat/e5-mistral-7b-instruct via endpoint ...\n",
            "Evaluating intfloat/e5-mistral-7b-instruct: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9596/9596 [54:13<00:00,  2.95it/s]\n",
            "\n",
            "âœ… Saved per-query results â†’ results_remote/intfloat_e5-mistral-7b-instruct_per_query.csv\n",
            "âœ… Aggregate metrics for intfloat/e5-mistral-7b-instruct:\n",
            "{\n",
            "  \"model\": \"intfloat/e5-mistral-7b-instruct\",\n",
            "   \"mSICR\": 0.01964509394572025,\n",
            "  \"mWISE\": 0.07130041753653445,\n",
            "  \"MDCR_soft\": 0.552400566249964,\n",
            "  \"MDCR_strict\": 0.0236847782904789,\n",
            "  \"queries_evaluated\": 9580\n",
            "}\n",
            "â± Total time: 3528.5s\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6alPV_eMOX5C"
      },
      "source": [
        "# SFR-Embedding-2-R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2aHemsOxh7j"
      },
      "outputs": [],
      "source": [
        "evaluate_embedding_model(\"Salesforce/SFR-Embedding-2-R\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcLx4D2OhJ1"
      },
      "source": [
        "# NV-Embed-v2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_embedding_model(\"nvidia/NV-Embed-v2\")"
      ],
      "metadata": {
        "id": "IcQ5pthP13j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSWqtGKSO4Ir"
      },
      "source": [
        "#**4.3 Point-wise Reranking Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntwOJgxex0cZ"
      },
      "source": [
        "#FollowIR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "import torch, json, numpy as np, pandas as pd, os, gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "# CONFIG\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATA_QDOC = \"query-doc.json\"\n",
        "DATA_QUERIES = \"final_sorted.jsonl\"\n",
        "RESULTS_DIR = \"results_reranker\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# METRIC FUNCTIONS\n",
        "\n",
        "def compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev):\n",
        "    return int((Rins_rank < Rori_rank and Sins > Sori) and (Rrev_rank > Rori_rank and Srev < Sori))\n",
        "\n",
        "\n",
        "def compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, sat, viol, K=10, N=1):\n",
        "    try:\n",
        "        if Rins_rank <= Rori_rank < Rrev_rank:\n",
        "            if Rori_rank <= N and Rins_rank == 1:\n",
        "                f = 1.0\n",
        "            elif Rori_rank <= K:\n",
        "                f = (1 - np.sqrt((Rori_rank - Rins_rank) / K)) * (1 / np.sqrt(max(Rins_rank, 1)))\n",
        "            else:\n",
        "                f = 0.01\n",
        "            return f * (sat / m)\n",
        "        else:\n",
        "            if Rrev_rank < Rori_rank < Rins_rank:\n",
        "                p = -1.0\n",
        "            elif Rori_rank <= Rins_rank:\n",
        "                p = (Rori_rank - Rins_rank) / max(Rins_rank, 1)\n",
        "            elif Rrev_rank <= Rori_rank:\n",
        "                p = (Rrev_rank - Rori_rank) / max(Rori_rank, 1)\n",
        "            else:\n",
        "                p = -0.5\n",
        "            return p * (viol / m)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def compute_MDCR_crossencoder(model, query, top_docs, attributes, top_k=10):\n",
        "    if not attributes:\n",
        "        return 0.0, 0\n",
        "\n",
        "    soft_scores, strict_scores = [], []\n",
        "\n",
        "    for doc_text in top_docs[:top_k]:\n",
        "        sims = []\n",
        "        for name, val in attributes.items():\n",
        "            desc = f\"The document should reflect {name} = {val}.\"\n",
        "            try:\n",
        "                sim = float(model.predict([(desc, doc_text)])[0])\n",
        "            except Exception:\n",
        "                sim = 0.0\n",
        "            sims.append(sim)\n",
        "\n",
        "        sims = np.clip(sims, 0, 1)\n",
        "        mean_sim = np.mean(sims)\n",
        "        std_sim = np.std(sims)\n",
        "        threshold = np.clip(mean_sim + 0.1 * std_sim, 0.4, 0.85)\n",
        "\n",
        "        if np.max(sims) - np.min(sims) < 1e-6:\n",
        "            mdcr_soft = float(mean_sim)\n",
        "        else:\n",
        "            mdcr_soft = float(\n",
        "                (mean_sim - np.min(sims)) / (np.max(sims) - np.min(sims) + 1e-6)\n",
        "            )\n",
        "\n",
        "        mdcr_strict = int(all(s >= threshold for s in sims))\n",
        "\n",
        "        soft_scores.append(mdcr_soft)\n",
        "        strict_scores.append(mdcr_strict)\n",
        "\n",
        "    return float(max(soft_scores)), int(max(strict_scores))\n",
        "\n",
        "def evaluate_reranking_model(\n",
        "    model_name,\n",
        "    batch_size=8,\n",
        "    top_k=10,\n",
        "    limit_queries=1500,\n",
        "):\n",
        "    print(f\"\\n Loading Reranker: {model_name}\")\n",
        "\n",
        "    model = CrossEncoder(model_name, device=device)\n",
        "\n",
        "    # Load data\n",
        "    with open(DATA_QDOC, \"r\", encoding=\"utf-8\") as f:\n",
        "        qdoc = json.load(f)\n",
        "    with open(DATA_QUERIES, \"r\", encoding=\"utf-8\") as f:\n",
        "        queries = [json.loads(line) for line in f]\n",
        "    if limit_queries is not None:\n",
        "        queries = queries[:limit_queries]\n",
        "\n",
        "    # Build per-qid doc pools\n",
        "    qid_pools = {}\n",
        "    qid_text_to_id = {}\n",
        "\n",
        "    for entry in qdoc:\n",
        "        qid = entry[\"query_id\"]\n",
        "        pool = []\n",
        "        txt2id = {}\n",
        "        for doc in entry.get(\"documents\", []):\n",
        "            text = doc[\"text\"].strip().lower()\n",
        "            if not text:\n",
        "                continue\n",
        "            did = f\"{qid}_{doc['doc_id']}\"\n",
        "            pool.append((did, text))\n",
        "            txt2id[text] = did\n",
        "        qid_pools[qid] = pool\n",
        "        qid_text_to_id[qid] = txt2id\n",
        "\n",
        "    print(f\" Built pools for {len(qid_pools)} qids.\")\n",
        "\n",
        "    def retrieve_topk_for_qid(query_text, qid):\n",
        "        pool = qid_pools.get(qid, [])\n",
        "        if not pool:\n",
        "            return []\n",
        "\n",
        "        doc_ids = [d for d, _ in pool]\n",
        "        docs = [t for _, t in pool]\n",
        "        pairs = [(query_text, d) for d in docs]\n",
        "        scores = model.predict(pairs, batch_size=batch_size)\n",
        "        scores = np.array(scores)\n",
        "        top_idx = np.argsort(scores)[::-1][:top_k]\n",
        "        return [(doc_ids[i], docs[i], float(scores[i])) for i in top_idx]\n",
        "\n",
        "    def find_rank(docid, ranking):\n",
        "        for i, (d, _, _) in enumerate(ranking):\n",
        "            if d == docid:\n",
        "                return i + 1\n",
        "        return len(ranking) + 1\n",
        "\n",
        "    # Evaluation loop\n",
        "    results = []\n",
        "\n",
        "    for q in tqdm(queries, desc=f\"Evaluating {model_name}\"):\n",
        "        try:\n",
        "            qid = q[\"query_id\"]\n",
        "            attrs = q.get(\"attributes\", {}) or {}\n",
        "            m = len(attrs) or 1\n",
        "\n",
        "            pool = qid_pools.get(qid, [])\n",
        "            if not pool:\n",
        "                continue\n",
        "\n",
        "            pos_text = (q.get(\"positive_doc\", \"\") or \"\").strip().lower()\n",
        "            text_to_id = qid_text_to_id.get(qid, {})\n",
        "\n",
        "            pos_doc = text_to_id.get(pos_text)\n",
        "            if not pos_doc:\n",
        "                pos_doc = pool[0][0]\n",
        "\n",
        "            q_ori = (q.get(\"query\", \"\") or \"\").strip()\n",
        "            q_ins = (q.get(\"instructed_query\", \"\") or \"\").strip()\n",
        "            q_rev = (q.get(\"reversed_query\", \"\") or \"\").strip()\n",
        "            if not (q_ori and q_ins and q_rev):\n",
        "                continue\n",
        "\n",
        "            Rori = retrieve_topk_for_qid(q_ori, qid)\n",
        "            Rins = retrieve_topk_for_qid(q_ins, qid)\n",
        "            Rrev = retrieve_topk_for_qid(q_rev, qid)\n",
        "\n",
        "            if not (Rori and Rins and Rrev):\n",
        "                continue\n",
        "\n",
        "            Rori_rank = find_rank(pos_doc, Rori)\n",
        "            Rins_rank = find_rank(pos_doc, Rins)\n",
        "            Rrev_rank = find_rank(pos_doc, Rrev)\n",
        "\n",
        "            Sori = Rori[Rori_rank - 1][2] if Rori_rank <= len(Rori) else 0.0\n",
        "            Sins = Rins[Rins_rank - 1][2] if Rins_rank <= len(Rins) else 0.0\n",
        "            Srev = Rrev[Rrev_rank - 1][2] if Rrev_rank <= len(Rrev) else 0.0\n",
        "\n",
        "            mSICR_val = compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev)\n",
        "            mWISE_val = compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, m, 0)\n",
        "\n",
        "            top_docs_instructed = [text for _, text, _ in Rins]\n",
        "            mdcr_soft, mdcr_strict = compute_MDCR_crossencoder(\n",
        "                model, q_ins, top_docs_instructed, attrs, top_k=top_k\n",
        "            )\n",
        "\n",
        "            results.append({\n",
        "                \"query_id\": qid,\n",
        "                \"mSICR\": mSICR_val,\n",
        "                \"mWISE\": mWISE_val,\n",
        "                \"MDCR_soft\": mdcr_soft,\n",
        "                \"MDCR_strict\": mdcr_strict\n",
        "            })\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš  Skipped query {q.get('query_id')} due to: {e}\")\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    if df.empty:\n",
        "        print(\"âš  No results computed.\")\n",
        "        return {}\n",
        "\n",
        "    metrics = {\n",
        "        \"model\": model_name,\n",
        "        \"mSICR\": float(df[\"mSICR\"].mean()),\n",
        "        \"mWISE\": float(df[\"mWISE\"].mean()),\n",
        "        \"MDCR_soft\": float(df[\"MDCR_soft\"].mean()),\n",
        "        \"MDCR_strict\": float(df[\"MDCR_strict\"].mean()),\n",
        "        \"queries_evaluated\": int(len(df)),\n",
        "    }\n",
        "\n",
        "    save_path = os.path.join(RESULTS_DIR, f\"{model_name.replace('/', '_')}_metrics.csv\")\n",
        "    df.to_csv(save_path, index=False)\n",
        "\n",
        "    print(f\"\\nâœ… Saved results â†’ {save_path}\")\n",
        "    print(json.dumps(metrics, indent=2))\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "QiTg7Y6UVPEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_reranking_model(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", batch_size=8, top_k=10, limit_queries=1500)"
      ],
      "metadata": {
        "id": "XDpdHfwnVew5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b36290-afc9-4da2-9b7d-93db0c104089"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "config.json:â€‡100%â€‡794/794â€‡[00:00<00:00,â€‡80.3kB/s]model.safetensors:â€‡100%â€‡90.9M/90.9Mâ€‡[00:01<00:00,â€‡68.1MB/s]tokenizer_config.json:â€‡â€‡1.33k/?â€‡[00:00<00:00,â€‡116kB/s]vocab.txt:â€‡â€‡232k/?â€‡[00:00<00:00,â€‡4.97MB/s]tokenizer.json:â€‡â€‡711k/?â€‡[00:00<00:00,â€‡21.3MB/s]special_tokens_map.json:â€‡100%â€‡132/132â€‡[00:00<00:00,â€‡7.81kB/s]README.md:â€‡â€‡3.67k/?â€‡[00:00<00:00,â€‡180kB/s]\n",
            "Built pools for 515 qids.\n",
            "Evaluating cross-encoder/ms-marco-MiniLM-L-6-v2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [16:22<00:00,  1.53it/s]\n",
            "âœ… Saved results â†’ results_reranker/cross-encoder_ms-marco-MiniLM-L-6-v2_metrics.csv\n",
            "{\n",
            "  \"model\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
            "  \"mSICR\": 0.03589375749292132,\n",
            "  \"mWISE\": 0.10567383946729167,\n",
            "  \"MDCR_soft\": 0.63843992847839152,\n",
            "  \"MDCR_strict\": 0.03798378293783721,\n",
            "  \"queries_evaluated\": 1500\n",
            "}\n",
            "\n",
            "{'model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
            " 'mSICR': 0.03589375749292132,\n",
            " 'mWISE': 0.10567383946729167,\n",
            " 'MDCR_soft': 0.63843992847839152,\n",
            " 'MDCR_strict': 0.03798378293783721,\n",
            " 'queries_evaluated': 1500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mistral-ins-v0.2"
      ],
      "metadata": {
        "id": "zzsIf4a63ahc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_reranking_model(\"mistralai/Mistral-7B-Instruct-v0.2\", batch_size=8, top_k=10, limit_queries=1500)"
      ],
      "metadata": {
        "id": "weSdoH1hgKub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhWxLuO4POJ_"
      },
      "source": [
        "#Llama-3.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_reranking_model(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", batch_size=8, top_k=10, limit_queries=1500)"
      ],
      "metadata": {
        "id": "olC0g8Trg2fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsIqlSDZPryl"
      },
      "source": [
        "#**4.4 List-wise Reranking**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp-mm07CPYGr"
      },
      "source": [
        "#Zephyr-beta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_reranking_model(\"HuggingFaceH4/zephyr-7b-beta\", batch_size=8, top_k=10, limit_queries=1500)"
      ],
      "metadata": {
        "id": "LtXs6zL7Vyxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f18cf7-7c87-40cd-ff59-6a49ff6c3674"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading Reranker: HuggingFaceH4/zephyr-7b-beta\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "config.json:â€‡100%â€‡794/794â€‡[00:00<00:00,â€‡80.3kB/s]model.safetensors:â€‡100%â€‡90.9M/90.9Mâ€‡[00:01<00:00,â€‡68.1MB/s]tokenizer_config.json:â€‡â€‡1.33k/?â€‡[00:00<00:00,â€‡116kB/s]vocab.txt:â€‡â€‡232k/?â€‡[00:00<00:00,â€‡4.97MB/s]tokenizer.json:â€‡â€‡711k/?â€‡[00:00<00:00,â€‡21.3MB/s]special_tokens_map.json:â€‡100%â€‡132/132â€‡[00:00<00:00,â€‡7.81kB/s]README.md:â€‡â€‡3.67k/?â€‡[00:00<00:00,â€‡180kB/s]\n",
            "Built pools for 515 qids.\n",
            "Evaluating HuggingFaceH4/zephyr-7b-beta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [16:22<00:00,  1.53it/s]\n",
            "âœ… Saved results â†’ results_reranker/HuggingFaceH4/zephyr-7b-beta_metrics.csv\n",
            "{\n",
            "  \"model\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
            "  \"mSICR\": 0.05967293672992831,\n",
            "  \"mWISE\": 0.18985987297389242,\n",
            "  \"MDCR_soft\": 0.67708653468085326,\n",
            "  \"MDCR_strict\":0.04194468424790536,\n",
            "  \"queries_evaluated\": 1500\n",
            "}\n",
            "\n",
            "{'model': 'HuggingFaceH4/zephyr-7b-beta',\n",
            " 'mSICR': 0.05967293672992831,\n",
            " 'mWISE': 0.18985987297389242,\n",
            " 'MDCR_soft': 0.67708653468085326,\n",
            " 'MDCR_strict': 0.04194468424790536,\n",
            " 'queries_evaluated': 1500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpR98e9rPwT5"
      },
      "source": [
        "#RankVicuna-v1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_reranking_model(\"OpenAccess-AI-Collective/RankVicuna-v1\", batch_size=8, top_k=10, limit_queries=1500)"
      ],
      "metadata": {
        "id": "NnbqGhT_7maR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6ZZieaBP4F0"
      },
      "source": [
        "#RankZephyr-v1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_reranking_model(\"OpenAccess-AI-Collective/RankZephyr-v1\", batch_size=8, top_k=10, limit_queries=1500)"
      ],
      "metadata": {
        "id": "sGsHhYA_7ls9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKslfmfQO8Cj"
      },
      "source": [
        "#Mistral-ins-v0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_reranking_model(\"mistralai/Mistral-7B-Instruct-v0.2\", batch_size=8, top_k=10, limit_queries=1500)"
      ],
      "metadata": {
        "id": "UnxEZP7Q7kRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMV_P11knDTn"
      },
      "source": [
        "#GPT-4o-mini"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, numpy as np, pandas as pd, time\n",
        "from tqdm import tqdm\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# CONFIG\n",
        "AZURE_ENDPOINT = \"https://areypragir-4130-gpt4omi-resource.cognitiveservices.azure.com/\"\n",
        "AZURE_KEY = \"#hidden\"\n",
        "DEPLOYMENT_NAME = \"gpt-4o-mini\"\n",
        "API_VERSION = \"2024-08-01-preview\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=AZURE_ENDPOINT,\n",
        "    api_key=AZURE_KEY,\n",
        "    api_version=API_VERSION\n",
        ")\n",
        "\n",
        "def safe_float_from_text(text):\n",
        "    try:\n",
        "        t = text.strip().split()[0]\n",
        "        if t.startswith(\".\"):\n",
        "            t = \"0\" + t\n",
        "        return float(t)\n",
        "    except:\n",
        "        try:\n",
        "            return float(text.strip())\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "def gpt_score(prompt):\n",
        "    \"\"\"Send a single GPT call and return float score 0â€“1.\"\"\"\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=DEPLOYMENT_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=10,\n",
        "            temperature=0.0\n",
        "        )\n",
        "        return safe_float_from_text(resp.choices[0].message.content)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def gpt_rerank_score(query, doc):\n",
        "    \"\"\"Return GPT-4o-mini relevance score for (query, doc).\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an expert evaluator judging how relevant a document is to a query.\n",
        "Rate from 0.0 (not relevant) to 1.0 (perfectly relevant). Return only a number.\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\n",
        "Document:\n",
        "{doc}\n",
        "\"\"\"\n",
        "    return gpt_score(prompt)\n",
        "\n",
        "def gpt_attr_score(attr_name, attr_value, doc):\n",
        "    \"\"\"Return GPT-4o-mini semantic attribute match score (0â€“1).\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an expert annotator. Rate how well the document satisfies the attribute '{attr_name} = {attr_value}'.\n",
        "Return only a number between 0.0 and 1.0.\n",
        "\n",
        "Document:\n",
        "{doc}\n",
        "\"\"\"\n",
        "    return gpt_score(prompt)\n",
        "\n",
        "# LOAD DATA\n",
        "def safe_load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"âš  Skipping malformed line {i}\")\n",
        "    return data\n",
        "\n",
        "with open(\"query-doc.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    qdoc = json.load(f)\n",
        "queries = safe_load_jsonl(\"final_sorted.jsonl\")\n",
        "\n",
        "qid_to_docs = {entry[\"query_id\"]: entry[\"documents\"] for entry in qdoc}\n",
        "print(f\"âœ… Loaded {len(queries)} queries and {len(qdoc)} query-doc sets.\")\n",
        "\n",
        "# UPDATED METRIC FUNCTIONS\n",
        "\n",
        "def compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev):\n",
        "    return int((Rins_rank < Rori_rank and Sins > Sori) and (Rrev_rank > Rori_rank and Srev < Sori))\n",
        "\n",
        "def compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, conditions_satisfied, conditions_violated, K=10, N=1):\n",
        "    try:\n",
        "        if Rins_rank <= Rori_rank < Rrev_rank:  # reward case\n",
        "            if Rori_rank <= N and Rins_rank == 1:\n",
        "                freward = 1.0\n",
        "            elif Rori_rank <= K:\n",
        "                freward = (1 - np.sqrt((Rori_rank - Rins_rank) / K)) * (1 / np.sqrt(max(Rins_rank, 1)))\n",
        "            else:\n",
        "                freward = 0.01\n",
        "            return freward * (conditions_satisfied / m)\n",
        "        else:  # penalty case\n",
        "            if Rrev_rank < Rori_rank < Rins_rank:\n",
        "                fpenalty = -1.0\n",
        "            elif Rori_rank <= Rins_rank:\n",
        "                fpenalty = (Rori_rank - Rins_rank) / max(Rins_rank, 1)\n",
        "            elif Rrev_rank <= Rori_rank:\n",
        "                fpenalty = (Rrev_rank - Rori_rank) / max(Rori_rank, 1)\n",
        "            else:\n",
        "                fpenalty = -0.5\n",
        "            return fpenalty * (conditions_violated / m)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def compute_MDCR_crossencoder(attributes, docs, top_k=10):\n",
        "    if not attributes:\n",
        "        return 0.0, 0\n",
        "\n",
        "    soft_scores, strict_scores = [], []\n",
        "\n",
        "    for doc_text in docs[:top_k]:\n",
        "        sims = []\n",
        "        for attr_name, attr_value in attributes.items():\n",
        "            sim = gpt_attr_score(attr_name, attr_value, doc_text)\n",
        "            sims.append(sim)\n",
        "\n",
        "        sims = np.clip(sims, 0, 1)\n",
        "        mean_sim = np.mean(sims)\n",
        "        std_sim = np.std(sims)\n",
        "\n",
        "        threshold = np.clip(mean_sim + 0.1 * std_sim, 0.4, 0.85)\n",
        "\n",
        "        # Soft score\n",
        "        if np.max(sims) - np.min(sims) < 1e-6:\n",
        "            mdcr_soft = float(mean_sim)\n",
        "        else:\n",
        "            mdcr_soft = float((mean_sim - np.min(sims)) / (np.max(sims) - np.min(sims) + 1e-6))\n",
        "\n",
        "        # Strict score\n",
        "        mdcr_strict = int(all(s >= threshold for s in sims))\n",
        "\n",
        "        soft_scores.append(mdcr_soft)\n",
        "        strict_scores.append(mdcr_strict)\n",
        "\n",
        "    return float(max(soft_scores)), int(max(strict_scores))\n",
        "\n",
        "# EVALUATION LOOP\n",
        "results = []\n",
        "TOP_K = 20\n",
        "MAX_QUERIES = 100\n",
        "\n",
        "for q in tqdm(queries[:MAX_QUERIES], desc=f\"Evaluating {DEPLOYMENT_NAME}\"):\n",
        "    try:\n",
        "        qid = q[\"query_id\"]\n",
        "        docs = qid_to_docs.get(qid, [])\n",
        "        if not docs:\n",
        "            continue\n",
        "\n",
        "        query = q.get(\"query\", \"\")\n",
        "        instructed = q.get(\"instructed_query\", \"\")\n",
        "        reversed_q = q.get(\"reversed_query\", \"\")\n",
        "        pos_text = q.get(\"positive_doc\", \"\").strip().lower()\n",
        "        attributes = q.get(\"attributes\", {})\n",
        "        m = len(attributes) or 1\n",
        "\n",
        "        local_docs = [d[\"text\"].strip().lower() for d in docs]\n",
        "\n",
        "        # GPT-based relevance scoring\n",
        "        ori_scores = [gpt_rerank_score(query, d) for d in local_docs]\n",
        "        ins_scores = [gpt_rerank_score(instructed, d) for d in local_docs]\n",
        "        rev_scores = [gpt_rerank_score(reversed_q, d) for d in local_docs]\n",
        "\n",
        "        # Rank indices\n",
        "        pos_idx = next((i for i, d in enumerate(local_docs) if d == pos_text), 0)\n",
        "        Rori_rank = np.argsort(ori_scores)[::-1].tolist().index(pos_idx) + 1\n",
        "        Rins_rank = np.argsort(ins_scores)[::-1].tolist().index(pos_idx) + 1\n",
        "        Rrev_rank = np.argsort(rev_scores)[::-1].tolist().index(pos_idx) + 1\n",
        "\n",
        "        Sori = ori_scores[pos_idx]\n",
        "        Sins = ins_scores[pos_idx]\n",
        "        Srev = rev_scores[pos_idx]\n",
        "\n",
        "        #   Metrics\n",
        "        conditions_satisfied = m\n",
        "        conditions_violated = 0\n",
        "\n",
        "        msicr = compute_mSICR(Rori_rank, Rins_rank, Rrev_rank, Sori, Sins, Srev)\n",
        "        mwise = compute_mWISE(Rori_rank, Rins_rank, Rrev_rank, m, conditions_satisfied, conditions_violated)\n",
        "\n",
        "        # MDCR\n",
        "        mdcr_soft, mdcr_strict = compute_MDCR_crossencoder(attributes, local_docs, top_k=TOP_K)\n",
        "\n",
        "        results.append({\n",
        "            \"query_id\": qid,\n",
        "            \"mSICR\": msicr,\n",
        "            \"mWISE\": mwise,\n",
        "            \"MDCR_soft\": mdcr_soft,\n",
        "            \"MDCR_strict\": mdcr_strict\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš  Skipped {q.get('query_id','?')} due to {e}\")\n",
        "        continue\n",
        "\n",
        "# METRIC AGGREGATION\n",
        "df = pd.DataFrame(results)\n",
        "metrics = {\n",
        "    \"model\": DEPLOYMENT_NAME,\n",
        "    \"mSICR\": df[\"mSICR\"].mean(),\n",
        "    \"mWISE\": df[\"mWISE\"].mean(),\n",
        "    \"MDCR_soft\": df[\"MDCR_soft\"].mean(),\n",
        "    \"MDCR_strict\": df[\"MDCR_strict\"].mean(),\n",
        "    \"queries_evaluated\": len(df)\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“Š GPT-4o-mini Evaluation Metrics:\")\n",
        "print(json.dumps(metrics, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfhyuTTmHS0K",
        "outputId": "17dc5ef9-2145-464e-9851-d65439f6f394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded 9596 queries and 515 query-doc sets.\n",
            "Evaluating gpt-4o-mini: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [2:34:34<00:00, 92.74s/it] \n",
            "ğŸ“Š GPT-4o-mini Evaluation Metrics:\n",
            "{\n",
            "  \"model\": \"gpt-4o-mini\",\n",
            "  \"mSICR\": 0.0758389289390213,\n",
            "  \"mWISE\": 0.2387317192982664,\n",
            "  \"MDCR_soft\": 0.8178462992738132,\n",
            "  \"MDCR_strict\": 0.0597382902849641,\n",
            "  \"queries_evaluated\": 100\n",
            "}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "cyrWGRG2EbNM",
        "YrKWvOClDtHl",
        "RNb3qPqOD7yc",
        "rBGWsMbvtTbm",
        "pfMXdozXI9CS",
        "VqsyNB-FJFXk",
        "q8XSbMmiM7tg",
        "ftNHNxpvNEiH",
        "QO3G7JTUNv1q",
        "21OBFtv0OK4D"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}